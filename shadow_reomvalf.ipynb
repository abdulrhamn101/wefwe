{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iyo1v8YvoqO1"
   },
   "source": [
    "## FFAT + RAMiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H8f_8E37e9uA"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\kornia\\feature\\lightglue.py:44: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  @torch.cuda.amp.custom_fwd(cast_inputs=torch.float32)\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\timm\\models\\fx_features.py:4: FutureWarning: Importing from timm.models.fx_features is deprecated, please import via timm.models\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\timm\\models\\layers\\__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# # RAMiT (Reciprocal Attention Mixing Transformer)\n",
    "\n",
    "# from pdb import set_trace as stx\n",
    "# import numbers\n",
    "# from saicinpainting.training.modules.ffc0 import FFC_BN_ACT\n",
    "# from skimage import io\n",
    "# from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from torch import nn, Tensor\n",
    "# from einops import rearrange\n",
    "# import torch.nn.functional as F\n",
    "# import torch\n",
    "# from torchvision.transforms import functional as TF\n",
    "# import math\n",
    "# from RAMiT.my_model.common.mean_std import mean_std\n",
    "# from RAMiT.utils.etc_utils import denormalize\n",
    "# from timm.models.fx_features import register_notrace_function\n",
    "# from timm.models.layers import trunc_normal_, to_2tuple\n",
    "# from torch.nn.modules.batchnorm import _BatchNorm\n",
    "# def make_model(args, opts, pe):\n",
    "#     model = RAMiT(target_mode=args.target_mode,\n",
    "#                   img_norm=args.img_norm,\n",
    "#                   in_chans=opts['in_chans'],\n",
    "#                   dim = opts['dim'],\n",
    "#                   depths = opts['depths'],\n",
    "#                   num_heads = opts['num_heads'],\n",
    "#                   head_dim = opts['head_dim'],\n",
    "#                   chsa_head_ratio = opts['chsa_head_ratio'],\n",
    "#                   window_size = opts['window_size'],\n",
    "#                   hidden_ratio = opts['hidden_ratio'],\n",
    "#                   qkv_bias = opts['qkv_bias'],\n",
    "#                   mv_ver = opts['mv_ver'],\n",
    "#                   exp_factor=opts['exp_factor'],\n",
    "#                   expand_groups=opts['expand_groups'],\n",
    "#                   act_layer = opts['act_layer'],\n",
    "#                   norm_layer = opts['norm_layer'],\n",
    "#                   tail_mv = opts['tail_mv'],\n",
    "#                   attn_drop = opts['attn_drop'],\n",
    "#                   proj_drop = opts['proj_drop'],\n",
    "#                   drop_path = opts['drop_path'],\n",
    "#                   helper = opts['helper'],\n",
    "#                   mv_act = opts['mv_act']\n",
    "#                  )\n",
    "#     num_params = sum([p.numel() for n,p in model.named_parameters()])\n",
    "#     print(format(num_params, ','))\n",
    "#     return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class ShallowModule(nn.Module):\n",
    "#     def __init__(self, in_chans, out_chans, kernel_size=3, stride=1):\n",
    "#         super(ShallowModule, self).__init__()\n",
    "\n",
    "#         self.in_chans = in_chans\n",
    "#         self.out_chans = out_chans\n",
    "#         self.kernel_size = kernel_size\n",
    "#         self.stride = stride\n",
    "\n",
    "#         self.conv = nn.Conv2d(in_chans, out_chans, kernel_size, stride, kernel_size//2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.conv(x)\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         return resolutions[0]*resolutions[1] * self.kernel_size*self.kernel_size * self.in_chans * self.out_chans\n",
    "\n",
    "# class QKVProjection(nn.Module):\n",
    "#     def __init__(self, dim, num_head, qkv_bias=True):\n",
    "#         super(QKVProjection, self).__init__()\n",
    "#         self.dim = dim\n",
    "#         self.num_head = num_head\n",
    "\n",
    "#         self.qkv = nn.Conv2d(dim, 3*dim, 1, bias=qkv_bias)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, C, H, W = x.size()\n",
    "#         qkv = self.qkv(x)\n",
    "#         qkv = rearrange(qkv, 'b (l c) h w -> b l c h w', l=self.num_head)\n",
    "#         return qkv\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         return resolutions[0]*resolutions[1] * 1*1 * self.dim * 3*self.dim\n",
    "\n",
    "# def get_relative_position_index(win_h, win_w):\n",
    "#     # get pair-wise relative position index for each token inside the window\n",
    "#     coords = torch.stack(torch.meshgrid([torch.arange(win_h), torch.arange(win_w)]))  # 2, Wh, Ww\n",
    "#     coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "#     relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww (xaxis matrix & yaxis matrix)\n",
    "#     relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "#     relative_coords[:, :, 0] += win_h - 1  # shift to start from 0\n",
    "#     relative_coords[:, :, 1] += win_w - 1\n",
    "#     relative_coords[:, :, 0] *= 2 * win_w - 1\n",
    "#     return relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "\n",
    "# class SpatialSelfAttention(nn.Module):\n",
    "#     def __init__(self, dim, num_head, total_head, window_size=8, shift=0, attn_drop=0.0, proj_drop=0.0, helper=True):\n",
    "#         super(SpatialSelfAttention, self).__init__()\n",
    "#         self.dim = dim\n",
    "#         self.num_head = num_head\n",
    "#         self.total_head = total_head\n",
    "#         self.window_size = window_size\n",
    "#         self.window_area = window_size**2\n",
    "#         self.shift = shift\n",
    "#         self.helper = helper\n",
    "\n",
    "#         self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_head, 1, 1))), requires_grad=True)\n",
    "\n",
    "#         # define a parameter table of relative position bias, shape: 2*Wh-1 * 2*Ww-1, nH\n",
    "#         self.relative_position_bias_table = nn.Parameter(torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_head))\n",
    "\n",
    "#         # get pair-wise relative position index for each token inside the window\n",
    "#         self.register_buffer(\"relative_position_index\", get_relative_position_index(window_size, window_size))\n",
    "\n",
    "#         self.attn_drop = nn.Dropout(attn_drop)\n",
    "\n",
    "#         self.proj = nn.Conv2d(dim*num_head, dim*num_head, 1)\n",
    "#         self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "#     def _get_rel_pos_bias(self) -> torch.Tensor:\n",
    "#         relative_position_bias = self.relative_position_bias_table[\n",
    "#             self.relative_position_index.view(-1)].view(self.window_area, self.window_area, -1)  # Wh*Ww,Wh*Ww,nH\n",
    "#         relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "#         return relative_position_bias.unsqueeze(0)\n",
    "\n",
    "#     def forward(self, qkv, ch=None):\n",
    "#         B, L, C, H, W = qkv.size()\n",
    "#         # window shift\n",
    "#         if self.shift > 0:\n",
    "#             qkv = torch.roll(qkv, shifts=(-self.shift, -self.shift), dims=(-2,-1))\n",
    "\n",
    "#         # window partition\n",
    "#         q,k,v = rearrange(qkv, 'b l c (h wh) (w ww) -> (b h w) l (wh ww) c',\n",
    "#                           wh=self.window_size, ww=self.window_size).chunk(3, dim=-1) # [B_, L1, hw, C/L] respectively\n",
    "#         if ch is not None and self.helper: # [B, C, H, W]\n",
    "#             if self.shift > 0:\n",
    "#                 ch = torch.roll(ch, shifts=(-self.shift, -self.shift), dims=(-2,-1))\n",
    "#             ch = rearrange(ch, 'b (l c) (h wh) (w ww) -> (b h w) l (wh ww) c',\n",
    "#                            l=self.total_head-self.num_head, wh=self.window_size, ww=self.window_size) # [B_, L1, hw, C/L]\n",
    "#             ch = torch.mean(ch, dim=1, keepdim=True) # head squeeze [B_, 1, hw, C/L]\n",
    "#             v = v*ch # [B_, L1, hw, C/L]\n",
    "\n",
    "#         attn = F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(2,-1) # [B_, L1, hw, hw]\n",
    "#         logit_scale = torch.clamp(self.logit_scale, max=math.log(1. / 0.01)).exp()\n",
    "#         attn = attn * logit_scale\n",
    "\n",
    "#         attn = attn + self._get_rel_pos_bias()\n",
    "#         attn = self.attn_drop(F.softmax(attn, dim=-1))\n",
    "\n",
    "#         x = attn @ v # [B_, L1, hw, C/L]\n",
    "\n",
    "#         # window unpartition + head merge\n",
    "#         x = window_unpartition(x, (H,W), self.window_size) # [B, L1*C/L, H, W]\n",
    "#         x = self.proj_drop(self.proj(x))\n",
    "\n",
    "#         # window reverse shift\n",
    "#         if self.shift > 0:\n",
    "#             x = torch.roll(x, shifts=(self.shift, self.shift), dims=(-2,-1))\n",
    "\n",
    "#         return x\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         H,W = resolutions\n",
    "#         num_wins = H//self.window_size * W//self.window_size\n",
    "#         flops = self.num_head * H*W * self.dim if self.helper else 0 # v = v*ch\n",
    "#         flops += num_wins * self.num_head * self.window_area * self.dim * self.window_area # attn = Q@K^T\n",
    "#         flops += num_wins * self.num_head * self.window_area * self.window_area * self.dim # attn@V\n",
    "#         flops += H*W * 1*1 * self.num_head*self.dim * self.num_head*self.dim # self.proj\n",
    "#         return flops\n",
    "\n",
    "# @register_notrace_function\n",
    "# def window_unpartition(x, resolutions, window_size):\n",
    "#     return rearrange(x, '(b h w) l (wh ww) c -> b (l c) (h wh) (w ww)',\n",
    "#                      h=resolutions[0]//window_size, w=resolutions[1]//window_size, wh=window_size)\n",
    "\n",
    "# class ChannelSelfAttention(nn.Module):\n",
    "#     def __init__(self, dim, num_head, total_head, attn_drop=0.0, proj_drop=0.0, helper=True):\n",
    "#         super(ChannelSelfAttention, self).__init__()\n",
    "#         self.dim = dim\n",
    "#         self.num_head = num_head\n",
    "#         self.total_head = total_head\n",
    "#         self.helper = helper\n",
    "\n",
    "#         self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_head, 1, 1))), requires_grad=True)\n",
    "\n",
    "#         self.attn_drop = nn.Dropout(attn_drop)\n",
    "\n",
    "#         self.proj = nn.Conv2d(dim*num_head, dim*num_head, 1)\n",
    "#         self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "#     def forward(self, qkv, sp=None):\n",
    "#         B, L, C, H, W = qkv.size()\n",
    "\n",
    "#         q,k,v = rearrange(qkv, 'b l c h w -> b l c (h w)').chunk(3, dim=-2) # [B, L2, C/L, HW]\n",
    "#         if sp is not None and self.helper:\n",
    "#             sp = torch.mean(sp, dim=1, keepdim=True) # channel squeeze # [B, 1, H, W]\n",
    "#             sp = rearrange(sp, 'b (l c) h w -> b l c (h w)', l=1) # [B, 1, 1, HW]\n",
    "#             v = v*sp # [B, L2, C/L, HW]\n",
    "\n",
    "#         attn = F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(2,-1) # [B, L2, C/L, C/L]\n",
    "#         logit_scale = torch.clamp(self.logit_scale, max=math.log(1. / 0.01)).exp()\n",
    "#         attn = attn * logit_scale\n",
    "\n",
    "#         attn = F.softmax(attn, dim=-1)\n",
    "#         attn = self.attn_drop(attn)\n",
    "\n",
    "#         x = attn @ v # [B, L2, C/L, HW]\n",
    "\n",
    "#         # head merge\n",
    "#         x = rearrange(x, 'b l c (h w) -> b (l c) h w', h=H) # [B, L2*C/L, H, W]\n",
    "#         x = self.proj_drop(self.proj(x)) # [B, L2*C/L, H, W]\n",
    "\n",
    "#         return x\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         H,W = resolutions\n",
    "#         flops = self.num_head * self.dim * H*W if self.helper else 0 # v = v*sp\n",
    "#         flops += self.num_head * self.dim * H*W * self.dim # attn = Q@K^T\n",
    "#         flops += self.num_head * self.dim * self.dim * H*W # attn@V\n",
    "#         flops += H*W * 1*1 * self.num_head*self.dim * self.num_head*self.dim # self.proj\n",
    "#         return flops\n",
    "\n",
    "# class ReshapeLayerNorm(nn.Module):\n",
    "#     def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "#         super(ReshapeLayerNorm, self).__init__()\n",
    "\n",
    "#         self.dim = dim\n",
    "#         self.norm = norm_layer(dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, C, H, W = x.size()\n",
    "#         x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "#         x = self.norm(x)\n",
    "#         x = rearrange(x, 'b (h w) c -> b c h w', h=H)\n",
    "#         return x\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         H,W = resolutions\n",
    "#         flops = 0\n",
    "#         flops += H*W * self.dim\n",
    "#         return flops\n",
    "\n",
    "# class MobiVari1(nn.Module): # MobileNet v1 Variants\n",
    "#     def __init__(self, dim, kernel_size, stride, act=nn.LeakyReLU, out_dim=None):\n",
    "#         super(MobiVari1, self).__init__()\n",
    "#         self.dim = dim\n",
    "#         self.kernel_size = kernel_size\n",
    "#         self.out_dim = out_dim or dim\n",
    "\n",
    "#         self.dw_conv = nn.Conv2d(dim, dim, kernel_size, stride, kernel_size//2, groups=dim)\n",
    "#         self.pw_conv = nn.Conv2d(dim, self.out_dim, 1, 1, 0)\n",
    "#         self.act = act()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = self.act(self.pw_conv(self.act(self.dw_conv(x))+x))\n",
    "#         return out + x if self.dim==self.out_dim else out\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         H,W = resolutions\n",
    "#         flops = H*W * self.kernel_size*self.kernel_size * self.dim  +  H*W * 1*1 * self.dim * self.out_dim # self.dw_conv + self.pw_conv\n",
    "#         return flops\n",
    "\n",
    "# class MobiVari2(MobiVari1): # MobileNet v2 Variants\n",
    "#     def __init__(self, dim, kernel_size, stride, act=nn.LeakyReLU, out_dim=None, exp_factor=1.2, expand_groups=4):\n",
    "#         super(MobiVari2, self).__init__(dim, kernel_size, stride, act, out_dim)\n",
    "#         self.expand_groups = expand_groups\n",
    "#         expand_dim = int(dim*exp_factor)\n",
    "#         expand_dim = expand_dim+(expand_groups-expand_dim%expand_groups)\n",
    "#         self.expand_dim = expand_dim\n",
    "\n",
    "#         self.exp_conv = nn.Conv2d(dim, self.expand_dim, 1, 1, 0, groups=expand_groups)\n",
    "#         self.dw_conv = nn.Conv2d(expand_dim, expand_dim, kernel_size, stride, kernel_size//2, groups=expand_dim)\n",
    "#         self.pw_conv = nn.Conv2d(expand_dim, self.out_dim, 1, 1, 0)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x1 = self.act(self.exp_conv(x))\n",
    "#         out = self.pw_conv(self.act(self.dw_conv(x1)+x1))\n",
    "#         return out + x if self.dim==self.out_dim else out\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         H,W = resolutions\n",
    "#         flops = H*W * 1*1 * (self.dim//self.expand_groups) * self.expand_dim # self.exp_conv\n",
    "#         flops += H*W * self.kernel_size*self.kernel_size * self.expand_dim # self.dw_conv\n",
    "#         flops += H*W * 1*1 * self.expand_dim * self.out_dim # self.pw_conv\n",
    "#         return flops\n",
    "\n",
    "# class FeedForward(nn.Module):\n",
    "#     def __init__(self, dim, hidden_ratio, act_layer=nn.GELU, bias=True, drop=0.0):\n",
    "#         super(FeedForward, self).__init__()\n",
    "\n",
    "#         self.dim = dim\n",
    "#         self.hidden_ratio = hidden_ratio\n",
    "\n",
    "#         self.hidden = nn.Conv2d(dim, int(dim*hidden_ratio), 1, bias=bias)\n",
    "#         self.drop1 = nn.Dropout(drop)\n",
    "#         self.out = nn.Conv2d(int(dim*hidden_ratio), dim, 1, bias=bias)\n",
    "#         self.drop2 = nn.Dropout(drop)\n",
    "#         self.act = act_layer()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.drop2(self.out(self.drop1(self.act(self.hidden(x)))))\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         H,W = resolutions\n",
    "#         flops = 2 * H*W * 1*1 * self.dim * self.dim*self.hidden_ratio # self.hidden + self.out\n",
    "#         return flops\n",
    "\n",
    "# class NoLayer(nn.Identity):\n",
    "#     def __init__(self):\n",
    "#         super(NoLayer, self).__init__()\n",
    "#     def flops(self, resolutions):\n",
    "#         return 0\n",
    "#     def forward(self, x, **kwargs):\n",
    "#         return x.flatten(1,2)\n",
    "\n",
    "# class DRAMiTransformer(nn.Module): # Reciprocal Attention Transformer Block\n",
    "#     def __init__(self, dim, num_head, chsa_head_ratio, FFAT, window_size=8, shift=0, head_dim=None, qkv_bias=True, mv_ver=1,\n",
    "#                  hidden_ratio=2.0, act_layer=nn.GELU, norm_layer=ReshapeLayerNorm, attn_drop=0.0, proj_drop=0.0, drop_path=0.0, helper=True,\n",
    "#                  mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4,\n",
    "#                  num_blocks = [4,6,6,8], heads = [1,2,4,8], ffn_expansion_factor = 2.66, bias = False, LayerNorm_type = 'WithBias'):\n",
    "#         super(DRAMiTransformer, self).__init__()\n",
    "\n",
    "#         self.dim = dim\n",
    "#         self.num_head = num_head\n",
    "#         self.window_size = window_size\n",
    "#         self.chsa_head = int(num_head*chsa_head_ratio)\n",
    "#         self.shift = shift\n",
    "#         self.helper = helper\n",
    "#         self.FFAT = FFAT\n",
    "\n",
    "#         self.qkv_proj = QKVProjection(dim, num_head, qkv_bias=qkv_bias)\n",
    "#         self.sp_attn = SpatialSelfAttention(dim//num_head, num_head-self.chsa_head, num_head,\n",
    "#                                             window_size, shift, attn_drop, proj_drop, helper) if num_head-self.chsa_head != 0 else NoLayer()\n",
    "#         self.ch_attn = ChannelSelfAttention(dim//num_head, self.chsa_head, num_head, attn_drop, proj_drop, helper) if self.chsa_head != 0 else NoLayer()\n",
    "\n",
    "\n",
    "#         #FFAT\n",
    "#         if self.FFAT:\n",
    "#           self.encoder_level1 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "#         #FFAT\n",
    "\n",
    "\n",
    "#         if mv_ver==3:\n",
    "#             self.mobivari = MobiVari1(dim, 3, 1, act=mv_act)\n",
    "#         elif mv_ver==2:\n",
    "#             self.mobivari = MobiVari2(dim, 3, 1, act=mv_act, out_dim=None, exp_factor=exp_factor, expand_groups=expand_groups)\n",
    "#             # NAFBlock---------------------------------------------\n",
    "#         elif mv_ver==1:\n",
    "#             self.mobivari = NAFBlock(c=self.dim)\n",
    "#             # -------------------------------------------------------\n",
    "\n",
    "#         self.norm1 = norm_layer(dim)\n",
    "\n",
    "#         self.ffn = FeedForward(dim, hidden_ratio, act_layer=act_layer)\n",
    "#         self.norm2 = norm_layer(dim)\n",
    "\n",
    "#         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "#     def forward(self, x, sp_=None, ch_=None):\n",
    "#         B, C, H, W = x.size()\n",
    "\n",
    "#         # QKV projection + head split\n",
    "#         qkv = self.qkv_proj(x) # [B, L, C, H, W]\n",
    "\n",
    "#         # SP-SA / CH-SA\n",
    "#         sp = self.sp_attn(qkv[:,:self.num_head-self.chsa_head], ch=ch_) # [B, L1*C/L, H, W]\n",
    "#         ch = self.ch_attn(qkv[:,self.num_head-self.chsa_head:], sp=sp_) # [B, L2*C/L, H, W]\n",
    "\n",
    "\n",
    "#         #FFAT\n",
    "#         if self.FFAT:\n",
    "#           FFAT_block = self.encoder_level1(torch.cat([sp, ch], dim=1))\n",
    "#           attn0 = self.mobivari(FFAT_block)\n",
    "#         #FFAT\n",
    "\n",
    "\n",
    "#         else:\n",
    "#           attn0 = self.mobivari(torch.cat([sp, ch], dim=1)) # merge [B, C, H, W]\n",
    "#         attn = self.drop_path(self.norm1(attn0)) + x # LN, skip connection [B, C, H, W]\n",
    "\n",
    "#         # FFN\n",
    "#         out = self.drop_path(self.norm2(self.ffn(attn))) + attn # FFN, LN, skip connection [B, C, H, W]\n",
    "\n",
    "#         return out, sp, ch, attn0\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         flops = self.qkv_proj.flops(resolutions)\n",
    "#         flops += self.sp_attn.flops(resolutions)\n",
    "#         flops += self.ch_attn.flops(resolutions)\n",
    "#         flops += self.mobivari.flops(resolutions)\n",
    "#         flops += self.norm1.flops(resolutions)\n",
    "#         flops += self.ffn.flops(resolutions)\n",
    "#         flops += self.norm2.flops(resolutions)\n",
    "#         params = sum([p.numel() for n,p in self.named_parameters()])\n",
    "#         return flops\n",
    "\n",
    "# class EncoderStage(nn.Module):\n",
    "#     def __init__(self, depth, dim, num_head, chsa_head_ratio, FFAT, window_size=8, head_dim=None,\n",
    "#                  qkv_bias=True, mv_ver=1, hidden_ratio=2.0, act_layer=nn.GELU, norm_layer=ReshapeLayerNorm,\n",
    "#                  attn_drop=0.0, proj_drop=0.0, drop_path=0.0, helper=True, mv_act=nn.LeakyReLU,):\n",
    "#         super(EncoderStage, self).__init__()\n",
    "\n",
    "#         self.depth = depth\n",
    "#         self.dim = dim\n",
    "#         self.num_head = num_head\n",
    "#         self.window_size = window_size\n",
    "#         shift = window_size//2\n",
    "\n",
    "#         self.blocks = nn.ModuleList()\n",
    "#         for d in range(depth):\n",
    "#             self.blocks.add_module(f'block{d}', DRAMiTransformer(dim, num_head, chsa_head_ratio, FFAT,\n",
    "#                                                                  window_size, 0 if d%2==0 else shift, head_dim, qkv_bias,\n",
    "#                                                                  mv_ver, hidden_ratio, act_layer, norm_layer,\n",
    "#                                                                  attn_drop, proj_drop, drop_path, helper, mv_act,\n",
    "#                                                                  ))\n",
    "#     def forward(self, x):\n",
    "#         sp, ch = None, None\n",
    "#         for i, blk in enumerate(self.blocks):\n",
    "#             x, sp, ch, attn = blk(x, sp, ch)\n",
    "#             sp, ch = (None, None) if (sp.size(1)==0 or ch.size(1)==0) else (sp, ch) # pure SPSA or CHSA\n",
    "#         return x, attn\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         flops = 0\n",
    "#         for blk in self.blocks:\n",
    "#             flops += blk.flops(resolutions)\n",
    "#         return flops\n",
    "\n",
    "# class Downsizing(nn.Module):\n",
    "#     \"\"\" Patch Merging Layer.\n",
    "\n",
    "#     Args:\n",
    "#         dim (int): Number of input dimension (channels).\n",
    "#         downsample_dim (int, optional): Number of output dimension (channels) (dim if not set).  Default: None\n",
    "#         norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, dim, downsample_dim=None, norm_layer=ReshapeLayerNorm, mv_ver=1, mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4):\n",
    "#         super(Downsizing, self).__init__()\n",
    "#         self.dim = dim\n",
    "#         self.downsample_dim = downsample_dim or dim\n",
    "#         self.norm = norm_layer(4*dim)\n",
    "#         if mv_ver==3:\n",
    "#             self.reduction = MobiVari1(4*dim, 3, 1, act=mv_act, out_dim=self.downsample_dim)\n",
    "#         elif mv_ver==2:\n",
    "#             self.reduction = MobiVari2(4*dim, 3, 1, act=mv_act, out_dim=self.downsample_dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
    "\n",
    "#             # NAFBlock in not accepted\n",
    "#             # NAFBlock---------------------------------------------\n",
    "#         elif mv_ver==1:\n",
    "#             self.mobivari = NAFBlock(c=self.dim)\n",
    "#             # -------------------------------------------------------\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, C, H, W = x.size()\n",
    "\n",
    "#         # Concat 2x2\n",
    "#         x0 = x[:, :, 0::2, 0::2]  # [B, C, H/2, W/2], top-left\n",
    "#         x1 = x[:, :, 0::2, 1::2]  # [B, C, H/2, W/2], top-right\n",
    "#         x2 = x[:, :, 1::2, 0::2]  # [B, C, H/2, W/2], bottom-left\n",
    "#         x3 = x[:, :, 1::2, 1::2]  # [B, C, H/2, W/2], bottom-right\n",
    "#         x = torch.cat([x0, x1, x2, x3], dim=1)  # [B, 4C, H/2, W/2]\n",
    "#         return self.reduction(self.norm(x)) # [B, C, H/2, W/2]\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         H,W = resolutions\n",
    "#         flops = self.norm.flops((H//2,W//2)) + self.reduction.flops((H//2,W//2))\n",
    "#         return flops\n",
    "\n",
    "# class Bottleneck(nn.Module):\n",
    "#     def __init__(self, dim, num_stages, act_layer=nn.GELU, norm_layer=ReshapeLayerNorm, mv_ver=1, mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4):\n",
    "#         super(Bottleneck, self).__init__()\n",
    "\n",
    "#         self.dim = dim\n",
    "#         self.cat_dim = sum([dim//(4**x) for x in range(num_stages)])\n",
    "#         if mv_ver==1:\n",
    "#             self.mobivari = MobiVari1(self.cat_dim, 3, 1, act=mv_act, out_dim=dim)\n",
    "#         elif mv_ver==2:\n",
    "#             self.mobivari = MobiVari2(self.cat_dim, 3, 1, act=mv_act, out_dim=dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
    "\n",
    "#             # NAFBlock---------------------------------------------\n",
    "#         elif mv_ver==3:\n",
    "#             self.mobivari = NAFBlock(c=self.dim)\n",
    "#             # -------------------------------------------------------\n",
    "#         self.act = act_layer()\n",
    "#         self.norm = norm_layer(dim)\n",
    "\n",
    "#     def shallow_down(self, x, r):\n",
    "#         for _ in range(r):\n",
    "#             x = F.max_pool2d(x, 2)\n",
    "#         return F.leaky_relu(x)\n",
    "\n",
    "#     def forward(self, x_list):\n",
    "#         xs = x_list[0]\n",
    "#         new_x = []\n",
    "#         for i in range(len(x_list[1:])):\n",
    "#             x_ = x_list[i+1]+self.shallow_down(xs, i)\n",
    "#             new_x.append(F.pixel_shuffle(x_, 2**i))\n",
    "#         new_x = self.norm(self.mobivari(torch.cat(new_x, dim=1)))\n",
    "#         return new_x\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         H,W = resolutions\n",
    "#         flops = 0\n",
    "#         # self.shallow_down (iterative max-pool)\n",
    "#         flops += (H//2)*(W//2) * 2*2 * self.dim # shallow-down into stage2 output (iter1)\n",
    "#         flops += (H//2)*(W//2) * 2*2 * self.dim # shallow-down into stage3 output (iter1)\n",
    "#         flops += (H//4)*(W//4) * 2*2 * self.dim # shallow-down into stage3 output (iter2)\n",
    "#         flops += self.mobivari.flops((H,W))\n",
    "#         flops += self.norm.flops((H,W))\n",
    "#         return flops\n",
    "\n",
    "# class HRAMi(nn.Module):\n",
    "#     def __init__(self, dim, kernel_size=3, stride=1, mv_ver=1, mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4):\n",
    "#         super(HRAMi, self).__init__()\n",
    "\n",
    "#         self.dim = dim\n",
    "#         self.kernel_size = kernel_size\n",
    "\n",
    "#         if mv_ver==1:\n",
    "#             self.mobivari = MobiVari1(dim+dim//4+dim//16+dim, kernel_size, stride, act=mv_act, out_dim=dim)\n",
    "#         elif mv_ver==2:\n",
    "#             self.mobivari = MobiVari2(dim+dim//4+dim//16+dim, kernel_size, stride, act=mv_act, out_dim=dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
    "#             # NAFBlock---------------------------------------------\n",
    "#         elif mv_ver==3:\n",
    "#             self.mobivari = NAFBlock(c=self.dim)\n",
    "#             # -------------------------------------------------------\n",
    "\n",
    "#     def forward(self, attn_list):\n",
    "#         for i, attn in enumerate(attn_list[:-1]):\n",
    "#             attn = F.pixel_shuffle(attn, 2**i)\n",
    "#             x = attn if i==0 else torch.cat([x, attn], dim=1)\n",
    "#         x = torch.cat([x, attn_list[-1]], dim=1)\n",
    "#         x = self.mobivari(x)\n",
    "#         return x\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         return self.mobivari.flops(resolutions)\n",
    "\n",
    "# class Reconstruction(nn.Module):\n",
    "#     def __init__(self, target_mode, dim, kernel_size=3, stride=1, num_mv=2, mv_ver=1, mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4):\n",
    "#         super(Reconstruction, self).__init__()\n",
    "\n",
    "#         self.upscale = upscale = int(target_mode[-1]) if target_mode in ['light_x2', 'light_x3', 'light_x4'] else 1\n",
    "#         self.target_mode = target_mode\n",
    "#         self.dim = dim\n",
    "#         self.out_chans = 3 if 'gray' not in target_mode else 1\n",
    "#         self.kernel_size = kernel_size\n",
    "#         self.stride = stride\n",
    "#         self.num_mv = num_mv\n",
    "\n",
    "#         self.mobivari = nn.ModuleList()\n",
    "#         for i in range(num_mv):\n",
    "#             if mv_ver==3:\n",
    "#                 self.mobivari.add_module(f'mobivari{i}', MobiVari1(dim, kernel_size, stride, mv_act))\n",
    "#             elif mv_ver==2:\n",
    "#                 self.mobivari.add_module(f'mobivari{i}', MobiVari2(dim, kernel_size, stride, mv_act, None, exp_factor, expand_groups))\n",
    "#                 # NAFBlock---------------------------------------------\n",
    "#             elif mv_ver==1:\n",
    "#                  self.mobivari.add_module(f'mobivari{i}',NAFBlock(c=self.dim))\n",
    "#                 # -------------------------------------------------------\n",
    "\n",
    "\n",
    "#         self.conv = nn.Conv2d(dim, self.out_chans*(upscale**2), kernel_size, stride, kernel_size//2)\n",
    "#         self.shuffle = nn.PixelShuffle(upscale) if upscale!=1 else nn.Identity()\n",
    "#         self.final_conv = nn.Conv2d(self.out_chans, self.out_chans, kernel_size, stride, kernel_size//2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         for mobivari in self.mobivari:\n",
    "#             x = mobivari(x)\n",
    "#         return self.final_conv(self.shuffle(self.conv(x)))\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         H,W = resolutions\n",
    "#         flops = 0\n",
    "#         for mobivari in self.mobivari: # self.mobivari\n",
    "#             flops += mobivari.flops((H,W))\n",
    "#         flops += H*W * self.kernel_size*self.kernel_size * self.dim * self.out_chans*(self.upscale**2) # self.conv\n",
    "#         flops += H*W * self.kernel_size*self.kernel_size * self.out_chans * self.out_chans # self.final_conv\n",
    "#         return flops\n",
    "\n",
    "# class RAMiT_FFAT(nn.Module):\n",
    "#     def __init__(self, in_chans=3, dim=64, depths=(1,4,4,6), num_heads=(4,4,4,4), head_dim=None, chsa_head_ratio=0.25, #depths = (6,4,4,6)\n",
    "#                  window_size=8, hidden_ratio=2.0, qkv_bias=True, mv_ver=1, exp_factor=1.2, expand_groups=4,\n",
    "#                  act_layer=nn.GELU, norm_layer=ReshapeLayerNorm, tail_mv=2, weight_init='', target_mode='light_x2', img_norm=True,\n",
    "#                  attn_drop=0.0, proj_drop=0.0, drop_path=0.0, helper=True, mv_act=nn.LeakyReLU,\n",
    "#                  num_blocks = [1,4,6,6,8],\n",
    "#                  heads = [1,2,4,8],\n",
    "#                  ffn_expansion_factor = 2.66,\n",
    "#                  bias = False,\n",
    "#                  LayerNorm_type = 'WithBias',   ## Other option 'BiasFree'\n",
    "#                  FFAT = False, **kwargs):\n",
    "#         super(RAMiT_FFAT, self).__init__()\n",
    "#         self.cat_dim = sum([dim//(4**x) for x in range(len(depths)-1)]) # added to use the torchinfor.summary\n",
    "#         self.unit = 2**(len(depths)-2)*window_size\n",
    "\n",
    "#         self.in_chans = in_chans\n",
    "#         self.dim = dim\n",
    "#         self.depths = depths\n",
    "#         self.num_heads = num_heads\n",
    "#         self.head_dim = head_dim\n",
    "#         self.window_size = window_size\n",
    "#         self.hidden_ratio = hidden_ratio\n",
    "#         self.qkv_bias = qkv_bias\n",
    "#         self.act_layer = act_layer\n",
    "#         norm_layer = ReshapeLayerNorm if norm_layer == 'ReshapeLayerNorm' else norm_layer\n",
    "#         self.norm_layer = norm_layer = ReshapeLayerNorm if norm_layer == 'ReshapeLayerNorm' else norm_layer\n",
    "#         self.tail_mv = tail_mv\n",
    "\n",
    "#         self.scale = int(target_mode[-1]) if target_mode in ['light_x2', 'light_x3', 'light_x4'] else 1\n",
    "#         self.mean, self.std = mean_std(self.scale, target_mode)\n",
    "#         self.target_mode = target_mode\n",
    "#         self.img_norm = img_norm\n",
    "\n",
    "#         dpr = [x.item() for x in torch.linspace(0, drop_path, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "#         self.shallow = ShallowModule(in_chans, dim, 3, 1)\n",
    "#         self.stage1 = EncoderStage(depths[0], dim, num_heads[0], chsa_head_ratio, FFAT, window_size, head_dim, qkv_bias, mv_ver,\n",
    "#                                    hidden_ratio, act_layer, norm_layer, attn_drop, proj_drop, drop_path, helper, mv_act)\n",
    "\n",
    "\n",
    "#         # FFAT\n",
    "#         self.encoder_level1 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "\n",
    "#         self.encoder_level2 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "\n",
    "#         self.encoder_level3 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "#         # FFAT\n",
    "\n",
    "\n",
    "#         self.down1 = Downsizing(dim, dim, norm_layer, mv_ver, mv_act)\n",
    "#         self.stage2 = EncoderStage(depths[0], dim, num_heads[1], chsa_head_ratio, FFAT, window_size, head_dim, qkv_bias, mv_ver,\n",
    "#                                    hidden_ratio, act_layer, norm_layer, attn_drop, proj_drop, drop_path, helper, mv_act)\n",
    "#         self.down2 = Downsizing(dim, dim, norm_layer, mv_ver, mv_act)\n",
    "#         self.stage3 = EncoderStage(depths[0], dim, num_heads[2], chsa_head_ratio, FFAT, window_size, head_dim, qkv_bias, mv_ver,\n",
    "#                                    hidden_ratio, act_layer, norm_layer, attn_drop, proj_drop, drop_path, helper, mv_act)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         if mv_ver==3:\n",
    "#             self.mobivari = MobiVari1(self.cat_dim, 3, 1, act=mv_act, out_dim=dim)\n",
    "#         elif mv_ver==2:\n",
    "#             self.mobivari = MobiVari2(self.cat_dim, 3, 1, act=mv_act, out_dim=dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
    "#             # NAFBlock not Accepted\n",
    "#             # NAFBlock---------------------------------------------\n",
    "#         elif mv_ver==1:\n",
    "#             self.mobivari = NAFBlock(c=self.dim)\n",
    "#             # -------------------------------------------------------\n",
    "#         self.act = act_layer()\n",
    "#         self.norm = norm_layer(dim)\n",
    "#         # self.bottleneck = Bottleneck(dim, len(depths)-1, act_layer, norm_layer, mv_ver, mv_act)\n",
    "\n",
    "#         self.stage4 = EncoderStage(depths[0], dim, num_heads[3], chsa_head_ratio, FFAT, window_size, head_dim, qkv_bias, mv_ver,\n",
    "#                                    hidden_ratio, act_layer, norm_layer, attn_drop, proj_drop, drop_path, helper, mv_act)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         if mv_ver==3:\n",
    "#             self.mobivari_mix = MobiVari1(dim+dim//4+dim//16+dim, 3, 1, act=mv_act, out_dim=dim)\n",
    "#         elif mv_ver==2:\n",
    "#             self.mobivari_mix = MobiVari2(dim+dim//4+dim//16+dim, 3, 1, act=mv_act, out_dim=dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
    "#             # NAFBlock not accepted\n",
    "#              # NAFBlock---------------------------------------------\n",
    "#         elif mv_ver==1:\n",
    "#             self.mobivari = NAFBlock(c=self.dim)\n",
    "#             # -------------------------------------------------------\n",
    "#         # self.attn_mix = HRAMi(dim, 3, 1, mv_ver, mv_act)\n",
    "\n",
    "\n",
    "#         self.to_target = Reconstruction(target_mode, dim, 3, 1, tail_mv, mv_ver, mv_act, exp_factor, expand_groups)\n",
    "\n",
    "#         self.apply(self._init_weights)\n",
    "\n",
    "#     def forward_size_norm(self, x):\n",
    "#         _,_,h,w = x.size()\n",
    "#         padh = self.unit-(h%self.unit) if h%self.unit!=0 else 0\n",
    "#         padw = self.unit-(w%self.unit) if w%self.unit!=0 else 0\n",
    "#         x = TF.pad(x, (0,0,padw,padh))\n",
    "#         return TF.normalize(x, self.mean, self.std) if self.img_norm else x\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         _,_,H_ori,W_ori = x.size()\n",
    "#         x = self.forward_size_norm(x)\n",
    "#         B, C, H, W = x.size()\n",
    "#         x_lq = x if self.scale == 1 else None # due to residual image (except for only SR)\n",
    "\n",
    "#         shallow = self.shallow(x)\n",
    "#         o1_, attn1 = self.stage1(shallow) # [B, C, H, W]\n",
    "\n",
    "\n",
    "#         # FFAT\n",
    "#         o1_ = self.encoder_level1(o1_)\n",
    "#         # FFAT\n",
    "\n",
    "\n",
    "#         o1 = self.down1(o1_) # [B, C, H//2, W//2]\n",
    "\n",
    "#         o2_, attn2 = self.stage2(o1) # [B, C, H//2, W//2]\n",
    "\n",
    "\n",
    "#         # FFAT\n",
    "#         o2_ = self.encoder_level2(o2_)\n",
    "#         # FFAT\n",
    "\n",
    "\n",
    "#         o2 = self.down2(o2_) # [B, C, H//4, W//4]\n",
    "\n",
    "#         o3_, attn3 = self.stage3(o2) # [B, C, H//4, W//4]\n",
    "\n",
    "\n",
    "#         # FFAT\n",
    "#         o3_ = self.encoder_level3(o3_)\n",
    "#         # FFAT\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Bottleneck\n",
    "#         def shallow_down( x, r):\n",
    "#           for _ in range(r):\n",
    "#               x = F.max_pool2d(x, 2)\n",
    "#           return F.leaky_relu(x)\n",
    "#         x_list = [shallow, o1_, o2_, o3_]\n",
    "#         xs = x_list[0]\n",
    "#         new_x = []\n",
    "#         for i in range(len(x_list[1:])):\n",
    "#             x_ = x_list[i+1]+shallow_down(xs, i)\n",
    "#             new_x.append(F.pixel_shuffle(x_, 2**i))\n",
    "#         ob = self.norm(self.mobivari(torch.cat(new_x, dim=1)))\n",
    "#         # ob = self.bottleneck([shallow, o1_, o2_, o3_]) # [B, C, H, W]\n",
    "\n",
    "\n",
    "\n",
    "#         o4, attn4 = self.stage4(ob) # [B, C, H, W]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # HRAMiT\n",
    "#         attn_list = [attn1, attn2, attn3, attn4]\n",
    "#         for i, attn in enumerate(attn_list[:-1]):\n",
    "#             attn = F.pixel_shuffle(attn, 2**i)\n",
    "#             x = attn if i==0 else torch.cat([x, attn], dim=1)\n",
    "#         x = torch.cat([x, attn_list[-1]], dim=1)\n",
    "#         mix = self.mobivari_mix(x)\n",
    "#         # mix = self.attn_mix([attn1, attn2, attn3, attn4]) # [B, C, H, W]\n",
    "\n",
    "\n",
    "\n",
    "#         o4 = o4 * mix # [B, C, H, W]\n",
    "\n",
    "#         out = self.to_target(o4+shallow) # global skip connection\n",
    "#         out = out + x_lq if self.scale == 1 else out # residual output (except for only SR)\n",
    "\n",
    "#         out = denormalize(out, self.mean, self.std) if self.img_norm else out\n",
    "#         out = out[:, :, :H_ori*self.scale, :W_ori*self.scale]\n",
    "#         return out\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         H_ori,W_ori = resolutions\n",
    "#         padh = self.unit-(H_ori%self.unit) if H_ori%self.unit!=0 else 0\n",
    "#         padw = self.unit-(W_ori%self.unit) if W_ori%self.unit!=0 else 0\n",
    "#         H,W = H_ori+padh,W_ori+padw\n",
    "#         flops = 0\n",
    "#         flops += self.shallow.flops((H,W))\n",
    "#         flops += self.stage1.flops((H,W))\n",
    "#         flops += self.down1.flops((H,W))\n",
    "#         flops += self.stage2.flops((H//2,W//2))\n",
    "#         flops += self.down2.flops((H//2,W//2))\n",
    "#         flops += self.stage3.flops((H//4,W//4))\n",
    "#         flops += self.bottleneck.flops((H,W))\n",
    "#         flops += self.stage4.flops((H,W))\n",
    "#         flops += self.attn_mix.flops((H,W))\n",
    "#         flops += self.dim * H*W # o4 = o4*mix\n",
    "#         flops += self.to_target.flops((H,W))\n",
    "#         return flops\n",
    "\n",
    "#     def _init_weights(self, m):\n",
    "#         # Swin V2 manner\n",
    "#         if isinstance(m, nn.Linear):\n",
    "#             trunc_normal_(m.weight, std=.02)\n",
    "#             if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "#                 nn.init.constant_(m.bias, 0)\n",
    "#         elif isinstance(m, nn.LayerNorm):\n",
    "#             nn.init.constant_(m.bias, 0)\n",
    "#             nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "#     @torch.jit.ignore\n",
    "#     def no_weight_decay(self):\n",
    "#         nwd = set()\n",
    "#         for n, _ in self.named_parameters():\n",
    "#             if 'relative_position_bias_table' in n:\n",
    "#                 nwd.add(n)\n",
    "#         return nwd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # ----------------------------Shadow_R-------------------------\n",
    "#     # @torch.no_grad()\n",
    "#     # def _init_weights(module_list, scale=1, bias_fill=0, **kwargs):\n",
    "#     #   \"\"\"Initialize network weights.\n",
    "\n",
    "#     #   Args:\n",
    "#     #       module_list (list[nn.Module] | nn.Module): Modules to be initialized.\n",
    "#     #       scale (float): Scale initialized weights, especially for residual\n",
    "#     #           blocks. Default: 1.\n",
    "#     #       bias_fill (float): The value to fill bias. Default: 0\n",
    "#     #       kwargs (dict): Other arguments for initialization function.\n",
    "#     #   \"\"\"\n",
    "#     #   if not isinstance(module_list, list):\n",
    "#     #       module_list = [module_list]\n",
    "#     #   for module in module_list:\n",
    "#     #       for m in module.modules():\n",
    "#     #           if isinstance(m, nn.Conv2d):\n",
    "#     #               nn.init.kaiming_normal_(m.weight, **kwargs)\n",
    "#     #               m.weight.data *= scale\n",
    "#     #               if m.bias is not None:\n",
    "#     #                   m.bias.data.fill_(bias_fill)\n",
    "#     #           elif isinstance(m, nn.Linear):\n",
    "#     #               nn.init.kaiming_normal_(m.weight, **kwargs)\n",
    "#     #               m.weight.data *= scale\n",
    "#     #               if m.bias is not None:\n",
    "#     #                   m.bias.data.fill_(bias_fill)\n",
    "#     #           elif isinstance(m, _BatchNorm):\n",
    "#     #               nn.init.constant_(m.weight, 1)\n",
    "#     #               if m.bias is not None:\n",
    "#     #                   m.bias.data.fill_(bias_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'RAMiT'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtransforms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m functional \u001b[38;5;28;01mas\u001b[39;00m TF\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmath\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mRAMiT\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmy_model\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmean_std\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mean_std\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mRAMiT\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01metc_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m denormalize\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtimm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfx_features\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m register_notrace_function\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'RAMiT'"
     ]
    }
   ],
   "source": [
    "# RAMiT (Reciprocal Attention Mixing Transformer)\n",
    "\n",
    "from pdb import set_trace as stx\n",
    "import numbers\n",
    "from skimage import io\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "from torch import nn, Tensor\n",
    "from einops import rearrange\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torchvision.transforms import functional as TF\n",
    "import math\n",
    "from RAMiT.my_model.common.mean_std import mean_std\n",
    "from RAMiT.utils.etc_utils import denormalize\n",
    "from timm.models.fx_features import register_notrace_function\n",
    "from timm.models.layers import trunc_normal_, to_2tuple\n",
    "from torch.nn.modules.batchnorm import _BatchNorm\n",
    "def make_model(args, opts, pe):\n",
    "    model = RAMiT(target_mode=args.target_mode,\n",
    "                  img_norm=args.img_norm,\n",
    "                  in_chans=opts['in_chans'],\n",
    "                  dim = opts['dim'],\n",
    "                  depths = opts['depths'],\n",
    "                  num_heads = opts['num_heads'],\n",
    "                  head_dim = opts['head_dim'],\n",
    "                  chsa_head_ratio = opts['chsa_head_ratio'],\n",
    "                  window_size = opts['window_size'],\n",
    "                  hidden_ratio = opts['hidden_ratio'],\n",
    "                  qkv_bias = opts['qkv_bias'],\n",
    "                  mv_ver = opts['mv_ver'],\n",
    "                  exp_factor=opts['exp_factor'],\n",
    "                  expand_groups=opts['expand_groups'],\n",
    "                  act_layer = opts['act_layer'],\n",
    "                  norm_layer = opts['norm_layer'],\n",
    "                  tail_mv = opts['tail_mv'],\n",
    "                  attn_drop = opts['attn_drop'],\n",
    "                  proj_drop = opts['proj_drop'],\n",
    "                  drop_path = opts['drop_path'],\n",
    "                  helper = opts['helper'],\n",
    "                  mv_act = opts['mv_act']\n",
    "                 )\n",
    "    num_params = sum([p.numel() for n,p in model.named_parameters()])\n",
    "    print(format(num_params, ','))\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# FFSANet------------------------------------------------\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.fft\n",
    "\n",
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size, stride, bias=True, norm=False, relu=True, transpose=False):\n",
    "        super(BasicConv, self).__init__()\n",
    "        if bias and norm:\n",
    "            bias = False\n",
    "\n",
    "        padding = kernel_size // 2\n",
    "        layers = list()\n",
    "        if transpose:\n",
    "            padding = kernel_size // 2 -1\n",
    "            layers.append(nn.ConvTranspose2d(in_channel, out_channel, kernel_size, padding=padding, stride=stride, bias=bias))\n",
    "        else:\n",
    "            layers.append(\n",
    "                nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding, stride=stride, bias=bias))\n",
    "        if norm:\n",
    "            layers.append(nn.BatchNorm2d(out_channel))\n",
    "        if relu:\n",
    "            layers.append(nn.GELU())\n",
    "        self.main = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, filter=False):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = BasicConv(in_channel, out_channel, kernel_size=3, stride=1, relu=True)\n",
    "        self.conv2 = BasicConv(out_channel, out_channel, kernel_size=3, stride=1, relu=False)\n",
    "        if filter:\n",
    "            self.cubic_11 = cubic_attention(in_channel//2, group=1, kernel=11)\n",
    "            self.cubic_7 = cubic_attention(in_channel//2, group=1, kernel=7)\n",
    "        self.filter = filter\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        if self.filter:\n",
    "            out = torch.chunk(out, 2, dim=1)\n",
    "            out_11 = self.cubic_11(out[0])\n",
    "            out_7 = self.cubic_7(out[1])\n",
    "            out = torch.cat((out_11, out_7), dim=1)\n",
    "        out = self.conv2(out)\n",
    "        return out + x\n",
    "\n",
    "# class cubic_attention(nn.Module):\n",
    "#     def __init__(self, dim, group, kernel) -> None:\n",
    "#         super().__init__()\n",
    "\n",
    "#         self.H_spatial_att = spatial_strip_att(dim, group=group, kernel=kernel)\n",
    "#         self.W_spatial_att = spatial_strip_att(dim, group=group, kernel=kernel, H=False)\n",
    "#         self.gamma = nn.Parameter(torch.zeros(dim,1,1))\n",
    "#         self.beta = nn.Parameter(torch.ones(dim,1,1))\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         # Apply Fourier Transform to the input\n",
    "#         x_fft = torch.fft.fft2(x)\n",
    "\n",
    "#         # Apply attention mechanisms in the frequency domain\n",
    "#         out = self.H_spatial_att(x_fft)\n",
    "#         out = self.W_spatial_att(out)\n",
    "\n",
    "#         # Apply Inverse Fourier Transform to get back to the spatial domain\n",
    "#         out_ifft = torch.fft.ifft2(out).real\n",
    "\n",
    "#         return self.gamma * out_ifft + x * self.beta\n",
    "\n",
    "class cubic_attention(nn.Module):\n",
    "    def __init__(self, dim, group, kernel) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.H_spatial_att = spatial_strip_att(dim, group=group, kernel=kernel)\n",
    "        self.W_spatial_att = spatial_strip_att(dim, group=group, kernel=kernel, H=False)\n",
    "        self.gamma = nn.Parameter(torch.zeros(dim,1,1))\n",
    "        self.beta = nn.Parameter(torch.ones(dim,1,1))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply Fourier Transform to the input\n",
    "        x_fft = torch.fft.fft2(x)\n",
    "\n",
    "        # Separate real and imaginary parts\n",
    "        real_part = x_fft.real\n",
    "        imag_part = x_fft.imag\n",
    "\n",
    "        # Apply attention mechanisms to real and imaginary parts separately\n",
    "        out_real = self.H_spatial_att(real_part)\n",
    "        out_real = self.W_spatial_att(out_real)\n",
    "        out_imag = self.H_spatial_att(imag_part)\n",
    "        out_imag = self.W_spatial_att(out_imag)\n",
    "\n",
    "        # Combine real and imaginary parts back into a complex tensor\n",
    "        out = torch.complex(out_real, out_imag)\n",
    "\n",
    "        # Apply Inverse Fourier Transform to get back to the spatial domain\n",
    "        out_ifft = torch.fft.ifft2(out).real\n",
    "\n",
    "        return self.gamma * out_ifft + x * self.beta\n",
    "\n",
    "\n",
    "class spatial_strip_att(nn.Module):\n",
    "    def __init__(self, dim, kernel=5, group=2, H=True) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.k = kernel\n",
    "        pad = kernel // 2\n",
    "        self.kernel = (1, kernel) if H else (kernel, 1)\n",
    "        self.padding = (kernel//2, 1) if H else (1, kernel//2)\n",
    "\n",
    "        self.group = group\n",
    "        self.pad = nn.ReflectionPad2d((pad, pad, 0, 0)) if H else nn.ReflectionPad2d((0, 0, pad, pad))\n",
    "        self.conv = nn.Conv2d(dim, group*kernel, kernel_size=1, stride=1, bias=False)\n",
    "        self.ap = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.filter_act = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        filter = self.ap(x)\n",
    "        filter = self.conv(filter)\n",
    "        n, c, h, w = x.shape\n",
    "\n",
    "        # Perform operations in the frequency domain\n",
    "        x = F.unfold(self.pad(x), kernel_size=self.kernel).reshape(n, self.group, c//self.group, self.k, h*w)\n",
    "        n, c1, p, q = filter.shape\n",
    "        filter = filter.reshape(n, c1//self.k, self.k, p*q).unsqueeze(2)\n",
    "        filter = self.filter_act(filter)\n",
    "        out = torch.sum(x * filter, dim=3).reshape(n, c, h, w)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "# FFSANet----------------------------------------------------------\n",
    "\n",
    "class ShallowModule(nn.Module):\n",
    "    def __init__(self, in_chans, out_chans, kernel_size=3, stride=1):\n",
    "        super(ShallowModule, self).__init__()\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.out_chans = out_chans\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "\n",
    "        self.conv = nn.Conv2d(in_chans, out_chans, kernel_size, stride, kernel_size//2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "    def flops(self, resolutions):\n",
    "        return resolutions[0]*resolutions[1] * self.kernel_size*self.kernel_size * self.in_chans * self.out_chans\n",
    "\n",
    "class QKVProjection(nn.Module):\n",
    "    def __init__(self, dim, num_head, qkv_bias=True):\n",
    "        super(QKVProjection, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.num_head = num_head\n",
    "\n",
    "        self.qkv = nn.Conv2d(dim, 3*dim, 1, bias=qkv_bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "        qkv = self.qkv(x)\n",
    "        qkv = rearrange(qkv, 'b (l c) h w -> b l c h w', l=self.num_head)\n",
    "        return qkv\n",
    "\n",
    "    def flops(self, resolutions):\n",
    "        return resolutions[0]*resolutions[1] * 1*1 * self.dim * 3*self.dim\n",
    "\n",
    "def get_relative_position_index(win_h, win_w):\n",
    "    # get pair-wise relative position index for each token inside the window\n",
    "    coords = torch.stack(torch.meshgrid([torch.arange(win_h), torch.arange(win_w)]))  # 2, Wh, Ww\n",
    "    coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "    relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww (xaxis matrix & yaxis matrix)\n",
    "    relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "    relative_coords[:, :, 0] += win_h - 1  # shift to start from 0\n",
    "    relative_coords[:, :, 1] += win_w - 1\n",
    "    relative_coords[:, :, 0] *= 2 * win_w - 1\n",
    "    return relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "\n",
    "class SpatialSelfAttention(nn.Module):\n",
    "    def __init__(self, dim, num_head, total_head, window_size=8, shift=0, attn_drop=0.0, proj_drop=0.0, helper=True):\n",
    "        super(SpatialSelfAttention, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.num_head = num_head\n",
    "        self.total_head = total_head\n",
    "        self.window_size = window_size\n",
    "        self.window_area = window_size**2\n",
    "        self.shift = shift\n",
    "        self.helper = helper\n",
    "\n",
    "        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_head, 1, 1))), requires_grad=True)\n",
    "\n",
    "        # define a parameter table of relative position bias, shape: 2*Wh-1 * 2*Ww-1, nH\n",
    "        self.relative_position_bias_table = nn.Parameter(torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_head))\n",
    "\n",
    "        # get pair-wise relative position index for each token inside the window\n",
    "        self.register_buffer(\"relative_position_index\", get_relative_position_index(window_size, window_size))\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "\n",
    "        self.proj = nn.Conv2d(dim*num_head, dim*num_head, 1)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def _get_rel_pos_bias(self) -> torch.Tensor:\n",
    "        relative_position_bias = self.relative_position_bias_table[\n",
    "            self.relative_position_index.view(-1)].view(self.window_area, self.window_area, -1)  # Wh*Ww,Wh*Ww,nH\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "        return relative_position_bias.unsqueeze(0)\n",
    "\n",
    "    def forward(self, qkv, ch=None):\n",
    "        B, L, C, H, W = qkv.size()\n",
    "        # window shift\n",
    "        if self.shift > 0:\n",
    "            qkv = torch.roll(qkv, shifts=(-self.shift, -self.shift), dims=(-2,-1))\n",
    "\n",
    "        # window partition\n",
    "        q,k,v = rearrange(qkv, 'b l c (h wh) (w ww) -> (b h w) l (wh ww) c',\n",
    "                          wh=self.window_size, ww=self.window_size).chunk(3, dim=-1) # [B_, L1, hw, C/L] respectively\n",
    "        if ch is not None and self.helper: # [B, C, H, W]\n",
    "            if self.shift > 0:\n",
    "                ch = torch.roll(ch, shifts=(-self.shift, -self.shift), dims=(-2,-1))\n",
    "            ch = rearrange(ch, 'b (l c) (h wh) (w ww) -> (b h w) l (wh ww) c',\n",
    "                           l=self.total_head-self.num_head, wh=self.window_size, ww=self.window_size) # [B_, L1, hw, C/L]\n",
    "            ch = torch.mean(ch, dim=1, keepdim=True) # head squeeze [B_, 1, hw, C/L]\n",
    "            v = v*ch # [B_, L1, hw, C/L]\n",
    "\n",
    "        attn = F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(2,-1) # [B_, L1, hw, hw]\n",
    "        logit_scale = torch.clamp(self.logit_scale, max=math.log(1. / 0.01)).exp()\n",
    "        attn = attn * logit_scale\n",
    "\n",
    "        attn = attn + self._get_rel_pos_bias()\n",
    "        attn = self.attn_drop(F.softmax(attn, dim=-1))\n",
    "\n",
    "        x = attn @ v # [B_, L1, hw, C/L]\n",
    "\n",
    "        # window unpartition + head merge\n",
    "        x = window_unpartition(x, (H,W), self.window_size) # [B, L1*C/L, H, W]\n",
    "        x = self.proj_drop(self.proj(x))\n",
    "\n",
    "        # window reverse shift\n",
    "        if self.shift > 0:\n",
    "            x = torch.roll(x, shifts=(self.shift, self.shift), dims=(-2,-1))\n",
    "\n",
    "        return x\n",
    "\n",
    "    def flops(self, resolutions):\n",
    "        H,W = resolutions\n",
    "        num_wins = H//self.window_size * W//self.window_size\n",
    "        flops = self.num_head * H*W * self.dim if self.helper else 0 # v = v*ch\n",
    "        flops += num_wins * self.num_head * self.window_area * self.dim * self.window_area # attn = Q@K^T\n",
    "        flops += num_wins * self.num_head * self.window_area * self.window_area * self.dim # attn@V\n",
    "        flops += H*W * 1*1 * self.num_head*self.dim * self.num_head*self.dim # self.proj\n",
    "        return flops\n",
    "\n",
    "@register_notrace_function\n",
    "def window_unpartition(x, resolutions, window_size):\n",
    "    return rearrange(x, '(b h w) l (wh ww) c -> b (l c) (h wh) (w ww)',\n",
    "                     h=resolutions[0]//window_size, w=resolutions[1]//window_size, wh=window_size)\n",
    "\n",
    "class ChannelSelfAttention(nn.Module):\n",
    "    def __init__(self, dim, num_head, total_head, attn_drop=0.0, proj_drop=0.0, helper=True):\n",
    "        super(ChannelSelfAttention, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.num_head = num_head\n",
    "        self.total_head = total_head\n",
    "        self.helper = helper\n",
    "\n",
    "        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_head, 1, 1))), requires_grad=True)\n",
    "\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "\n",
    "        self.proj = nn.Conv2d(dim*num_head, dim*num_head, 1)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, qkv, sp=None):\n",
    "        B, L, C, H, W = qkv.size()\n",
    "\n",
    "        q,k,v = rearrange(qkv, 'b l c h w -> b l c (h w)').chunk(3, dim=-2) # [B, L2, C/L, HW]\n",
    "        if sp is not None and self.helper:\n",
    "            sp = torch.mean(sp, dim=1, keepdim=True) # channel squeeze # [B, 1, H, W]\n",
    "            sp = rearrange(sp, 'b (l c) h w -> b l c (h w)', l=1) # [B, 1, 1, HW]\n",
    "            v = v*sp # [B, L2, C/L, HW]\n",
    "\n",
    "        attn = F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(2,-1) # [B, L2, C/L, C/L]\n",
    "        logit_scale = torch.clamp(self.logit_scale, max=math.log(1. / 0.01)).exp()\n",
    "        attn = attn * logit_scale\n",
    "\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = attn @ v # [B, L2, C/L, HW]\n",
    "\n",
    "        # head merge\n",
    "        x = rearrange(x, 'b l c (h w) -> b (l c) h w', h=H) # [B, L2*C/L, H, W]\n",
    "        x = self.proj_drop(self.proj(x)) # [B, L2*C/L, H, W]\n",
    "\n",
    "        return x\n",
    "\n",
    "    def flops(self, resolutions):\n",
    "        H,W = resolutions\n",
    "        flops = self.num_head * self.dim * H*W if self.helper else 0 # v = v*sp\n",
    "        flops += self.num_head * self.dim * H*W * self.dim # attn = Q@K^T\n",
    "        flops += self.num_head * self.dim * self.dim * H*W # attn@V\n",
    "        flops += H*W * 1*1 * self.num_head*self.dim * self.num_head*self.dim # self.proj\n",
    "        return flops\n",
    "\n",
    "class ReshapeLayerNorm(nn.Module):\n",
    "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "        super(ReshapeLayerNorm, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.norm = norm_layer(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "        x = self.norm(x)\n",
    "        x = rearrange(x, 'b (h w) c -> b c h w', h=H)\n",
    "        return x\n",
    "\n",
    "    def flops(self, resolutions):\n",
    "        H,W = resolutions\n",
    "        flops = 0\n",
    "        flops += H*W * self.dim\n",
    "        return flops\n",
    "\n",
    "class MobiVari1(nn.Module): # MobileNet v1 Variants\n",
    "    def __init__(self, dim, kernel_size, stride, act=nn.LeakyReLU, out_dim=None):\n",
    "        super(MobiVari1, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.kernel_size = kernel_size\n",
    "        self.out_dim = out_dim or dim\n",
    "\n",
    "        self.dw_conv = nn.Conv2d(dim, dim, kernel_size, stride, kernel_size//2, groups=dim)\n",
    "        self.pw_conv = nn.Conv2d(dim, self.out_dim, 1, 1, 0)\n",
    "        self.act = act()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.act(self.pw_conv(self.act(self.dw_conv(x))+x))\n",
    "        return out + x if self.dim==self.out_dim else out\n",
    "\n",
    "    def flops(self, resolutions):\n",
    "        H,W = resolutions\n",
    "        flops = H*W * self.kernel_size*self.kernel_size * self.dim  +  H*W * 1*1 * self.dim * self.out_dim # self.dw_conv + self.pw_conv\n",
    "        return flops\n",
    "\n",
    "class MobiVari2(MobiVari1): # MobileNet v2 Variants\n",
    "    def __init__(self, dim, kernel_size, stride, act=nn.LeakyReLU, out_dim=None, exp_factor=1.2, expand_groups=4):\n",
    "        super(MobiVari2, self).__init__(dim, kernel_size, stride, act, out_dim)\n",
    "        self.expand_groups = expand_groups\n",
    "        expand_dim = int(dim*exp_factor)\n",
    "        expand_dim = expand_dim+(expand_groups-expand_dim%expand_groups)\n",
    "        self.expand_dim = expand_dim\n",
    "\n",
    "        self.exp_conv = nn.Conv2d(dim, self.expand_dim, 1, 1, 0, groups=expand_groups)\n",
    "        self.dw_conv = nn.Conv2d(expand_dim, expand_dim, kernel_size, stride, kernel_size//2, groups=expand_dim)\n",
    "        self.pw_conv = nn.Conv2d(expand_dim, self.out_dim, 1, 1, 0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x1 = self.act(self.exp_conv(x))\n",
    "        out = self.pw_conv(self.act(self.dw_conv(x1)+x1))\n",
    "        return out + x if self.dim==self.out_dim else out\n",
    "\n",
    "    def flops(self, resolutions):\n",
    "        H,W = resolutions\n",
    "        flops = H*W * 1*1 * (self.dim//self.expand_groups) * self.expand_dim # self.exp_conv\n",
    "        flops += H*W * self.kernel_size*self.kernel_size * self.expand_dim # self.dw_conv\n",
    "        flops += H*W * 1*1 * self.expand_dim * self.out_dim # self.pw_conv\n",
    "        return flops\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_ratio, act_layer=nn.GELU, bias=True, drop=0.0):\n",
    "        super(FeedForward, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.hidden_ratio = hidden_ratio\n",
    "\n",
    "        self.hidden = nn.Conv2d(dim, int(dim*hidden_ratio), 1, bias=bias)\n",
    "        self.drop1 = nn.Dropout(drop)\n",
    "        self.out = nn.Conv2d(int(dim*hidden_ratio), dim, 1, bias=bias)\n",
    "        self.drop2 = nn.Dropout(drop)\n",
    "        self.act = act_layer()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.drop2(self.out(self.drop1(self.act(self.hidden(x)))))\n",
    "\n",
    "    def flops(self, resolutions):\n",
    "        H,W = resolutions\n",
    "        flops = 2 * H*W * 1*1 * self.dim * self.dim*self.hidden_ratio # self.hidden + self.out\n",
    "        return flops\n",
    "\n",
    "class NoLayer(nn.Identity):\n",
    "    def __init__(self):\n",
    "        super(NoLayer, self).__init__()\n",
    "    def flops(self, resolutions):\n",
    "        return 0\n",
    "    def forward(self, x, **kwargs):\n",
    "        return x.flatten(1,2)\n",
    "\n",
    "class DRAMiTransformer(nn.Module): # Reciprocal Attention Transformer Block\n",
    "    def __init__(self, dim, num_head, chsa_head_ratio, FFAT, window_size=8, shift=0, head_dim=None, qkv_bias=True, mv_ver=1,\n",
    "                 hidden_ratio=2.0, act_layer=nn.GELU, norm_layer=ReshapeLayerNorm, attn_drop=0.0, proj_drop=0.0, drop_path=0.0, helper=True,\n",
    "                 mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4,\n",
    "                 num_blocks = [4,6,6,8], heads = [1,2,4,8], ffn_expansion_factor = 2.66, bias = False, LayerNorm_type = 'WithBias'):\n",
    "        super(DRAMiTransformer, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.num_head = num_head\n",
    "        self.window_size = window_size\n",
    "        self.chsa_head = int(num_head*chsa_head_ratio)\n",
    "        self.shift = shift\n",
    "        self.helper = helper\n",
    "        self.FFAT = FFAT\n",
    "\n",
    "        self.qkv_proj = QKVProjection(dim, num_head, qkv_bias=qkv_bias)\n",
    "        self.sp_attn = SpatialSelfAttention(dim//num_head, num_head-self.chsa_head, num_head,\n",
    "                                            window_size, shift, attn_drop, proj_drop, helper) if num_head-self.chsa_head != 0 else NoLayer()\n",
    "        self.ch_attn = ChannelSelfAttention(dim//num_head, self.chsa_head, num_head, attn_drop, proj_drop, helper) if self.chsa_head != 0 else NoLayer()\n",
    "\n",
    "\n",
    "        #FFAT\n",
    "        if self.FFAT:\n",
    "          self.encoder_level1 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "        #FFAT\n",
    "\n",
    "\n",
    "        if mv_ver==3:\n",
    "            self.mobivari = MobiVari1(dim, 3, 1, act=mv_act)\n",
    "        elif mv_ver==2:\n",
    "            self.mobivari = MobiVari2(dim, 3, 1, act=mv_act, out_dim=None, exp_factor=exp_factor, expand_groups=expand_groups)\n",
    "            # NAFBlock---------------------------------------------\n",
    "        elif mv_ver==1:\n",
    "            self.mobivari = NAFBlock(c=self.dim)\n",
    "            # -------------------------------------------------------\n",
    "\n",
    "        self.norm1 = norm_layer(dim)\n",
    "\n",
    "        self.ffn = FeedForward(dim, hidden_ratio, act_layer=act_layer)\n",
    "        self.norm2 = norm_layer(dim)\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "    def forward(self, x, sp_=None, ch_=None):\n",
    "        B, C, H, W = x.size()\n",
    "\n",
    "        # QKV projection + head split\n",
    "        qkv = self.qkv_proj(x) # [B, L, C, H, W]\n",
    "\n",
    "        # SP-SA / CH-SA\n",
    "        sp = self.sp_attn(qkv[:,:self.num_head-self.chsa_head], ch=ch_) # [B, L1*C/L, H, W]\n",
    "        ch = self.ch_attn(qkv[:,self.num_head-self.chsa_head:], sp=sp_) # [B, L2*C/L, H, W]\n",
    "\n",
    "\n",
    "        #FFAT\n",
    "        if self.FFAT:\n",
    "          FFAT_block = self.encoder_level1(torch.cat([sp, ch], dim=1))\n",
    "          attn0 = self.mobivari(FFAT_block)\n",
    "        #FFAT\n",
    "\n",
    "\n",
    "        else:\n",
    "          attn0 = self.mobivari(torch.cat([sp, ch], dim=1)) # merge [B, C, H, W]\n",
    "        attn = self.drop_path(self.norm1(attn0)) + x # LN, skip connection [B, C, H, W]\n",
    "\n",
    "        # FFN\n",
    "        out = self.drop_path(self.norm2(self.ffn(attn))) + attn # FFN, LN, skip connection [B, C, H, W]\n",
    "\n",
    "        return out, sp, ch, attn0\n",
    "\n",
    "    def flops(self, resolutions):\n",
    "        flops = self.qkv_proj.flops(resolutions)\n",
    "        flops += self.sp_attn.flops(resolutions)\n",
    "        flops += self.ch_attn.flops(resolutions)\n",
    "        flops += self.mobivari.flops(resolutions)\n",
    "        flops += self.norm1.flops(resolutions)\n",
    "        flops += self.ffn.flops(resolutions)\n",
    "        flops += self.norm2.flops(resolutions)\n",
    "        params = sum([p.numel() for n,p in self.named_parameters()])\n",
    "        return flops\n",
    "\n",
    "class EncoderStage(nn.Module):\n",
    "    def __init__(self, depth, dim, num_head, chsa_head_ratio, FFAT, window_size=8, head_dim=None,\n",
    "                 qkv_bias=True, mv_ver=1, hidden_ratio=2.0, act_layer=nn.GELU, norm_layer=ReshapeLayerNorm,\n",
    "                 attn_drop=0.0, proj_drop=0.0, drop_path=0.0, helper=True, mv_act=nn.LeakyReLU,):\n",
    "        super(EncoderStage, self).__init__()\n",
    "\n",
    "        self.depth = depth\n",
    "        self.dim = dim\n",
    "        self.num_head = num_head\n",
    "        self.window_size = window_size\n",
    "        shift = window_size//2\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for d in range(depth):\n",
    "            self.blocks.add_module(f'block{d}', DRAMiTransformer(dim, num_head, chsa_head_ratio, FFAT,\n",
    "                                                                 window_size, 0 if d%2==0 else shift, head_dim, qkv_bias,\n",
    "                                                                 mv_ver, hidden_ratio, act_layer, norm_layer,\n",
    "                                                                 attn_drop, proj_drop, drop_path, helper, mv_act,\n",
    "                                                                 ))\n",
    "    def forward(self, x):\n",
    "        sp, ch = None, None\n",
    "        for i, blk in enumerate(self.blocks):\n",
    "            x, sp, ch, attn = blk(x, sp, ch)\n",
    "            sp, ch = (None, None) if (sp.size(1)==0 or ch.size(1)==0) else (sp, ch) # pure SPSA or CHSA\n",
    "        return x, attn\n",
    "\n",
    "    def flops(self, resolutions):\n",
    "        flops = 0\n",
    "        for blk in self.blocks:\n",
    "            flops += blk.flops(resolutions)\n",
    "        return flops\n",
    "\n",
    "class Downsizing(nn.Module):\n",
    "    \"\"\" Patch Merging Layer.\n",
    "\n",
    "    Args:\n",
    "        dim (int): Number of input dimension (channels).\n",
    "        downsample_dim (int, optional): Number of output dimension (channels) (dim if not set).  Default: None\n",
    "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, downsample_dim=None, norm_layer=ReshapeLayerNorm, mv_ver=1, mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4):\n",
    "        super(Downsizing, self).__init__()\n",
    "        self.dim = dim\n",
    "        self.downsample_dim = downsample_dim or dim\n",
    "        self.norm = norm_layer(4*dim)\n",
    "        if mv_ver==3:\n",
    "            self.reduction = MobiVari1(4*dim, 3, 1, act=mv_act, out_dim=self.downsample_dim)\n",
    "        elif mv_ver==2:\n",
    "            self.reduction = MobiVari2(4*dim, 3, 1, act=mv_act, out_dim=self.downsample_dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
    "\n",
    "            # NAFBlock in not accepted\n",
    "            # NAFBlock---------------------------------------------\n",
    "        elif mv_ver==1:\n",
    "            self.mobivari = NAFBlock(c=self.dim)\n",
    "            # -------------------------------------------------------\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.size()\n",
    "\n",
    "        # Concat 2x2\n",
    "        x0 = x[:, :, 0::2, 0::2]  # [B, C, H/2, W/2], top-left\n",
    "        x1 = x[:, :, 0::2, 1::2]  # [B, C, H/2, W/2], top-right\n",
    "        x2 = x[:, :, 1::2, 0::2]  # [B, C, H/2, W/2], bottom-left\n",
    "        x3 = x[:, :, 1::2, 1::2]  # [B, C, H/2, W/2], bottom-right\n",
    "        x = torch.cat([x0, x1, x2, x3], dim=1)  # [B, 4C, H/2, W/2]\n",
    "        return self.reduction(self.norm(x)) # [B, C, H/2, W/2]\n",
    "\n",
    "    def flops(self, resolutions):\n",
    "        H,W = resolutions\n",
    "        flops = self.norm.flops((H//2,W//2)) + self.reduction.flops((H//2,W//2))\n",
    "        return flops\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    def __init__(self, dim, num_stages, act_layer=nn.GELU, norm_layer=ReshapeLayerNorm, mv_ver=1, mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4):\n",
    "        super(Bottleneck, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.cat_dim = sum([dim//(4**x) for x in range(num_stages)])\n",
    "        if mv_ver==1:\n",
    "            self.mobivari = MobiVari1(self.cat_dim, 3, 1, act=mv_act, out_dim=dim)\n",
    "        elif mv_ver==2:\n",
    "            self.mobivari = MobiVari2(self.cat_dim, 3, 1, act=mv_act, out_dim=dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
    "\n",
    "            # NAFBlock---------------------------------------------\n",
    "        elif mv_ver==3:\n",
    "            self.mobivari = NAFBlock(c=self.dim)\n",
    "            # -------------------------------------------------------\n",
    "        self.act = act_layer()\n",
    "        self.norm = norm_layer(dim)\n",
    "\n",
    "    def shallow_down(self, x, r):\n",
    "        for _ in range(r):\n",
    "            x = F.max_pool2d(x, 2)\n",
    "        return F.leaky_relu(x)\n",
    "\n",
    "    def forward(self, x_list):\n",
    "        xs = x_list[0]\n",
    "        new_x = []\n",
    "        for i in range(len(x_list[1:])):\n",
    "            x_ = x_list[i+1]+self.shallow_down(xs, i)\n",
    "            new_x.append(F.pixel_shuffle(x_, 2**i))\n",
    "        new_x = self.norm(self.mobivari(torch.cat(new_x, dim=1)))\n",
    "        return new_x\n",
    "\n",
    "    def flops(self, resolutions):\n",
    "        H,W = resolutions\n",
    "        flops = 0\n",
    "        # self.shallow_down (iterative max-pool)\n",
    "        flops += (H//2)*(W//2) * 2*2 * self.dim # shallow-down into stage2 output (iter1)\n",
    "        flops += (H//2)*(W//2) * 2*2 * self.dim # shallow-down into stage3 output (iter1)\n",
    "        flops += (H//4)*(W//4) * 2*2 * self.dim # shallow-down into stage3 output (iter2)\n",
    "        flops += self.mobivari.flops((H,W))\n",
    "        flops += self.norm.flops((H,W))\n",
    "        return flops\n",
    "\n",
    "class HRAMi(nn.Module):\n",
    "    def __init__(self, dim, kernel_size=3, stride=1, mv_ver=1, mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4):\n",
    "        super(HRAMi, self).__init__()\n",
    "\n",
    "        self.dim = dim\n",
    "        self.kernel_size = kernel_size\n",
    "\n",
    "        if mv_ver==1:\n",
    "            self.mobivari = MobiVari1(dim+dim//4+dim//16+dim, kernel_size, stride, act=mv_act, out_dim=dim)\n",
    "        elif mv_ver==2:\n",
    "            self.mobivari = MobiVari2(dim+dim//4+dim//16+dim, kernel_size, stride, act=mv_act, out_dim=dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
    "            # NAFBlock---------------------------------------------\n",
    "        elif mv_ver==3:\n",
    "            self.mobivari = NAFBlock(c=self.dim)\n",
    "            # -------------------------------------------------------\n",
    "\n",
    "    def forward(self, attn_list):\n",
    "        for i, attn in enumerate(attn_list[:-1]):\n",
    "            attn = F.pixel_shuffle(attn, 2**i)\n",
    "            x = attn if i==0 else torch.cat([x, attn], dim=1)\n",
    "        x = torch.cat([x, attn_list[-1]], dim=1)\n",
    "        x = self.mobivari(x)\n",
    "        return x\n",
    "\n",
    "    def flops(self, resolutions):\n",
    "        return self.mobivari.flops(resolutions)\n",
    "\n",
    "class Reconstruction(nn.Module):\n",
    "    def __init__(self, target_mode, dim, kernel_size=3, stride=1, num_mv=2, mv_ver=1, mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4):\n",
    "        super(Reconstruction, self).__init__()\n",
    "\n",
    "        self.upscale = upscale = int(target_mode[-1]) if target_mode in ['light_x2', 'light_x3', 'light_x4'] else 1\n",
    "        self.target_mode = target_mode\n",
    "        self.dim = dim\n",
    "        self.out_chans = 3 if 'gray' not in target_mode else 1\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.num_mv = num_mv\n",
    "\n",
    "        self.mobivari = nn.ModuleList()\n",
    "        for i in range(num_mv):\n",
    "            if mv_ver==3:\n",
    "                self.mobivari.add_module(f'mobivari{i}', MobiVari1(dim, kernel_size, stride, mv_act))\n",
    "            elif mv_ver==2:\n",
    "                self.mobivari.add_module(f'mobivari{i}', MobiVari2(dim, kernel_size, stride, mv_act, None, exp_factor, expand_groups))\n",
    "                # NAFBlock---------------------------------------------\n",
    "            elif mv_ver==1:\n",
    "                 self.mobivari.add_module(f'mobivari{i}',NAFBlock(c=self.dim))\n",
    "                # -------------------------------------------------------\n",
    "\n",
    "\n",
    "        self.conv = nn.Conv2d(dim, self.out_chans*(upscale**2), kernel_size, stride, kernel_size//2)\n",
    "        self.shuffle = nn.PixelShuffle(upscale) if upscale!=1 else nn.Identity()\n",
    "        self.final_conv = nn.Conv2d(self.out_chans, self.out_chans, kernel_size, stride, kernel_size//2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        for mobivari in self.mobivari:\n",
    "            x = mobivari(x)\n",
    "        return self.final_conv(self.shuffle(self.conv(x)))\n",
    "\n",
    "    def flops(self, resolutions):\n",
    "        H,W = resolutions\n",
    "        flops = 0\n",
    "        for mobivari in self.mobivari: # self.mobivari\n",
    "            flops += mobivari.flops((H,W))\n",
    "        flops += H*W * self.kernel_size*self.kernel_size * self.dim * self.out_chans*(self.upscale**2) # self.conv\n",
    "        flops += H*W * self.kernel_size*self.kernel_size * self.out_chans * self.out_chans # self.final_conv\n",
    "        return flops\n",
    "\n",
    "class RAMiT_FFAT(nn.Module):\n",
    "    def __init__(self, in_chans=3, dim=64, depths=(6,4,4,6), num_heads=(4,4,4,4), head_dim=None, chsa_head_ratio=0.25, #depths = (6,4,4,6)\n",
    "                 window_size=8, hidden_ratio=2.0, qkv_bias=True, mv_ver=1, exp_factor=1.2, expand_groups=4,\n",
    "                 act_layer=nn.GELU, norm_layer=ReshapeLayerNorm, tail_mv=2, weight_init='', target_mode='light_x2', img_norm=True,\n",
    "                 attn_drop=0.0, proj_drop=0.0, drop_path=0.0, helper=True, mv_act=nn.LeakyReLU,\n",
    "                 num_blocks = [1,4,6,6,8],\n",
    "                 heads = [1,2,4,8],\n",
    "                 ffn_expansion_factor = 2.66,\n",
    "                 bias = False,\n",
    "                 LayerNorm_type = 'WithBias',   ## Other option 'BiasFree'\n",
    "                 FFAT = False, **kwargs):\n",
    "        super(RAMiT_FFAT, self).__init__()\n",
    "        self.cat_dim = sum([dim//(4**x) for x in range(len(depths)-1)]) # added to use the torchinfor.summary\n",
    "        self.unit = 2**(len(depths)-2)*window_size\n",
    "\n",
    "        self.in_chans = in_chans\n",
    "        self.dim = dim\n",
    "        self.depths = depths\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = head_dim\n",
    "        self.window_size = window_size\n",
    "        self.hidden_ratio = hidden_ratio\n",
    "        self.qkv_bias = qkv_bias\n",
    "        self.act_layer = act_layer\n",
    "        norm_layer = ReshapeLayerNorm if norm_layer == 'ReshapeLayerNorm' else norm_layer\n",
    "        self.norm_layer = norm_layer = ReshapeLayerNorm if norm_layer == 'ReshapeLayerNorm' else norm_layer\n",
    "        self.tail_mv = tail_mv\n",
    "\n",
    "        self.scale = int(target_mode[-1]) if target_mode in ['light_x2', 'light_x3', 'light_x4'] else 1\n",
    "        self.mean, self.std = mean_std(self.scale, target_mode)\n",
    "        self.target_mode = target_mode\n",
    "        self.img_norm = img_norm\n",
    "\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "        self.shallow = ShallowModule(in_chans, dim, 3, 1)\n",
    "        self.stage1 = EncoderStage(depths[1], dim, num_heads[0], chsa_head_ratio, FFAT, window_size, head_dim, qkv_bias, mv_ver,\n",
    "                                   hidden_ratio, act_layer, norm_layer, attn_drop, proj_drop, drop_path, helper, mv_act)\n",
    "\n",
    "\n",
    "        # FFSANet\n",
    "        self.cubic_11_1 = cubic_attention(dim//2, group=1, kernel=11)\n",
    "        self.cubic_7_1 = cubic_attention(dim//2, group=1, kernel=7)\n",
    "        \n",
    "        self.cubic_11_2 = cubic_attention(dim//2, group=1, kernel=11)\n",
    "        self.cubic_7_2 = cubic_attention(dim//2, group=1, kernel=7)\n",
    "        \n",
    "        self.cubic_11_3 = cubic_attention(dim//2, group=1, kernel=11)\n",
    "        self.cubic_7_3 = cubic_attention(dim//2, group=1, kernel=7)\n",
    "        # FFSANet\n",
    "\n",
    "\n",
    "        self.down1 = Downsizing(dim, dim, norm_layer, mv_ver, mv_act)\n",
    "        self.stage2 = EncoderStage(depths[1], dim, num_heads[1], chsa_head_ratio, FFAT, window_size, head_dim, qkv_bias, mv_ver,\n",
    "                                   hidden_ratio, act_layer, norm_layer, attn_drop, proj_drop, drop_path, helper, mv_act)\n",
    "        self.down2 = Downsizing(dim, dim, norm_layer, mv_ver, mv_act)\n",
    "        self.stage3 = EncoderStage(depths[1], dim, num_heads[2], chsa_head_ratio, FFAT, window_size, head_dim, qkv_bias, mv_ver,\n",
    "                                   hidden_ratio, act_layer, norm_layer, attn_drop, proj_drop, drop_path, helper, mv_act)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if mv_ver==3:\n",
    "            self.mobivari = MobiVari1(self.cat_dim, 3, 1, act=mv_act, out_dim=dim)\n",
    "        elif mv_ver==2:\n",
    "            self.mobivari = MobiVari2(self.cat_dim, 3, 1, act=mv_act, out_dim=dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
    "            # NAFBlock not Accepted\n",
    "            # NAFBlock---------------------------------------------\n",
    "        elif mv_ver==1:\n",
    "            self.mobivari = NAFBlock(c=self.dim)\n",
    "            # -------------------------------------------------------\n",
    "        self.act = act_layer()\n",
    "        self.norm = norm_layer(dim)\n",
    "        # self.bottleneck = Bottleneck(dim, len(depths)-1, act_layer, norm_layer, mv_ver, mv_act)\n",
    "\n",
    "        self.stage4 = EncoderStage(depths[1], dim, num_heads[3], chsa_head_ratio, FFAT, window_size, head_dim, qkv_bias, mv_ver,\n",
    "                                   hidden_ratio, act_layer, norm_layer, attn_drop, proj_drop, drop_path, helper, mv_act)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        if mv_ver==3:\n",
    "            self.mobivari_mix = MobiVari1(dim+dim//4+dim//16+dim, 3, 1, act=mv_act, out_dim=dim)\n",
    "        elif mv_ver==2:\n",
    "            self.mobivari_mix = MobiVari2(dim+dim//4+dim//16+dim, 3, 1, act=mv_act, out_dim=dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
    "        # self.attn_mix = HRAMi(dim, 3, 1, mv_ver, mv_act)\n",
    "\n",
    "\n",
    "        self.to_target = Reconstruction(target_mode, dim, 3, 1, tail_mv, mv_ver, mv_act, exp_factor, expand_groups)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "    def forward_size_norm(self, x):\n",
    "        _,_,h,w = x.size()\n",
    "        padh = self.unit-(h%self.unit) if h%self.unit!=0 else 0\n",
    "        padw = self.unit-(w%self.unit) if w%self.unit!=0 else 0\n",
    "        x = TF.pad(x, (0,0,padw,padh))\n",
    "        return TF.normalize(x, self.mean, self.std) if self.img_norm else x\n",
    "\n",
    "    def forward(self, x):\n",
    "        _,_,H_ori,W_ori = x.size()\n",
    "        x = self.forward_size_norm(x)\n",
    "        B, C, H, W = x.size()\n",
    "        x_lq = x if self.scale == 1 else None # due to residual image (except for only SR)\n",
    "\n",
    "        shallow = self.shallow(x)\n",
    "        o1_, attn1 = self.stage1(shallow) # [B, C, H, W]\n",
    "\n",
    "\n",
    "        # FFAT\n",
    "        out = torch.chunk(o1_, 2, dim=1)\n",
    "        out_11_1 = self.cubic_11_1(out[0])\n",
    "        out_7_1 = self.cubic_7_1(out[1])\n",
    "        o1_ = torch.cat((out_11_1, out_7_1), dim=1)\n",
    "        # FFAT\n",
    "\n",
    "\n",
    "        o1 = self.down1(o1_) # [B, C, H//2, W//2]\n",
    "\n",
    "        o2_, attn2 = self.stage2(o1) # [B, C, H//2, W//2]\n",
    "\n",
    "\n",
    "        # FFAT\n",
    "        out = torch.chunk(o2_, 2, dim=1)\n",
    "        out_11_2 = self.cubic_11_2(out[0])\n",
    "        out_7_2 = self.cubic_7_2(out[1])\n",
    "        o2_ = torch.cat((out_11_2, out_7_2), dim=1)\n",
    "        # FFAT\n",
    "\n",
    "\n",
    "        o2 = self.down2(o2_) # [B, C, H//4, W//4]\n",
    "\n",
    "        o3_, attn3 = self.stage3(o2) # [B, C, H//4, W//4]\n",
    "\n",
    "\n",
    "        # FFAT\n",
    "        out = torch.chunk(o3_, 2, dim=1)\n",
    "        out_11_3 = self.cubic_11_3(out[0])\n",
    "        out_7_3 = self.cubic_7_3(out[1])\n",
    "        o3_ = torch.cat((out_11_3, out_7_3), dim=1)\n",
    "        # FFAT\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Bottleneck\n",
    "        def shallow_down( x, r):\n",
    "          for _ in range(r):\n",
    "              x = F.max_pool2d(x, 2)\n",
    "          return F.leaky_relu(x)\n",
    "        x_list = [shallow, o1_, o2_, o3_]\n",
    "        xs = x_list[0]\n",
    "        new_x = []\n",
    "        for i in range(len(x_list[1:])):\n",
    "            x_ = x_list[i+1]+shallow_down(xs, i)\n",
    "            new_x.append(F.pixel_shuffle(x_, 2**i))\n",
    "        ob = self.norm(self.mobivari(torch.cat(new_x, dim=1)))\n",
    "        # ob = self.bottleneck([shallow, o1_, o2_, o3_]) # [B, C, H, W]\n",
    "\n",
    "\n",
    "\n",
    "        o4, attn4 = self.stage4(ob) # [B, C, H, W]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# HRAMiT\n",
    "        attn_list = [attn1, attn2, attn3, attn4]\n",
    "        for i, attn in enumerate(attn_list[:-1]):\n",
    "            attn = F.pixel_shuffle(attn, 2**i)\n",
    "            x = attn if i==0 else torch.cat([x, attn], dim=1)\n",
    "        x = torch.cat([x, attn_list[-1]], dim=1)\n",
    "        mix = self.mobivari_mix(x)\n",
    "        # mix = self.attn_mix([attn1, attn2, attn3, attn4]) # [B, C, H, W]\n",
    "\n",
    "\n",
    "\n",
    "        o4 = o4 * mix # [B, C, H, W]\n",
    "\n",
    "        out = self.to_target(o4+shallow) # global skip connection\n",
    "        out = out + x_lq if self.scale == 1 else out # residual output (except for only SR)\n",
    "\n",
    "        out = denormalize(out, self.mean, self.std) if self.img_norm else out\n",
    "        out = out[:, :, :H_ori*self.scale, :W_ori*self.scale]\n",
    "        return out\n",
    "\n",
    "    def flops(self, resolutions):\n",
    "        H_ori,W_ori = resolutions\n",
    "        padh = self.unit-(H_ori%self.unit) if H_ori%self.unit!=0 else 0\n",
    "        padw = self.unit-(W_ori%self.unit) if W_ori%self.unit!=0 else 0\n",
    "        H,W = H_ori+padh,W_ori+padw\n",
    "        flops = 0\n",
    "        flops += self.shallow.flops((H,W))\n",
    "        flops += self.stage1.flops((H,W))\n",
    "        flops += self.down1.flops((H,W))\n",
    "        flops += self.stage2.flops((H//2,W//2))\n",
    "        flops += self.down2.flops((H//2,W//2))\n",
    "        flops += self.stage3.flops((H//4,W//4))\n",
    "        flops += self.bottleneck.flops((H,W))\n",
    "        flops += self.stage4.flops((H,W))\n",
    "        flops += self.attn_mix.flops((H,W))\n",
    "        flops += self.dim * H*W # o4 = o4*mix\n",
    "        flops += self.to_target.flops((H,W))\n",
    "        return flops\n",
    "\n",
    "    def _init_weights(self, m):\n",
    "        # Swin V2 manner\n",
    "        if isinstance(m, nn.Linear):\n",
    "            trunc_normal_(m.weight, std=.02)\n",
    "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.LayerNorm):\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "            nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "    @torch.jit.ignore\n",
    "    def no_weight_decay(self):\n",
    "        nwd = set()\n",
    "        for n, _ in self.named_parameters():\n",
    "            if 'relative_position_bias_table' in n:\n",
    "                nwd.add(n)\n",
    "        return nwd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ----------------------------Shadow_R-------------------------\n",
    "    # @torch.no_grad()\n",
    "    # def _init_weights(module_list, scale=1, bias_fill=0, **kwargs):\n",
    "    #   \"\"\"Initialize network weights.\n",
    "\n",
    "    #   Args:\n",
    "    #       module_list (list[nn.Module] | nn.Module): Modules to be initialized.\n",
    "    #       scale (float): Scale initialized weights, especially for residual\n",
    "    #           blocks. Default: 1.\n",
    "    #       bias_fill (float): The value to fill bias. Default: 0\n",
    "    #       kwargs (dict): Other arguments for initialization function.\n",
    "    #   \"\"\"\n",
    "    #   if not isinstance(module_list, list):\n",
    "    #       module_list = [module_list]\n",
    "    #   for module in module_list:\n",
    "    #       for m in module.modules():\n",
    "    #           if isinstance(m, nn.Conv2d):\n",
    "    #               nn.init.kaiming_normal_(m.weight, **kwargs)\n",
    "    #               m.weight.data *= scale\n",
    "    #               if m.bias is not None:\n",
    "    #                   m.bias.data.fill_(bias_fill)\n",
    "    #           elif isinstance(m, nn.Linear):\n",
    "    #               nn.init.kaiming_normal_(m.weight, **kwargs)\n",
    "    #               m.weight.data *= scale\n",
    "    #               if m.bias is not None:\n",
    "    #                   m.bias.data.fill_(bias_fill)\n",
    "    #           elif isinstance(m, _BatchNorm):\n",
    "    #               nn.init.constant_(m.weight, 1)\n",
    "    #               if m.bias is not None:\n",
    "    #                   m.bias.data.fill_(bias_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Fk6xNHiphkkk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 672,151 trainable parameters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\functional.py:513: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\TensorShape.cpp:3610.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "G1 = RAMiT_FFAT(target_mode='light_realdn',img_norm=False, mv_ver = 3)\n",
    "G1 = nn.DataParallel(G1)\n",
    "G1.to('cuda')\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in G1.parameters() if p.requires_grad)\n",
    "\n",
    "# Create the model\n",
    "\n",
    "# Print the number of parameters\n",
    "print(f'The model has {count_parameters(G1):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the checkpoint\n",
    "# checkpoint = torch.load(r'C:\\Users\\Admin\\Desktop\\AI_RAMiT\\FFAT_RAMiT\\SBSR70.pth', map_location='cuda')\n",
    "\n",
    "# # Remove \"module.\" prefix if it exists in the keys\n",
    "# from collections import OrderedDict\n",
    "# new_state_dict = OrderedDict()\n",
    "# for k, v in checkpoint.items():\n",
    "#     name = k[7:] if k.startswith(\"module.\") else k  # remove \"module.\" prefix\n",
    "#     new_state_dict[name] = v\n",
    "\n",
    "# # Load the modified state_dict into the model\n",
    "# G1 = RAMiT_FFAT(target_mode='light_realdn', img_norm=False, mv_ver=3)\n",
    "# G1.load_state_dict(new_state_dict)\n",
    "\n",
    "# # Apply DataParallel and move to GPU\n",
    "# G1 = nn.DataParallel(G1)\n",
    "# G1.to('cuda')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "cEs_UewFzrIu"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'G1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchinfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m----> 2\u001b[0m summary(\u001b[43mG1\u001b[49m, (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m360\u001b[39m, \u001b[38;5;241m480\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'G1' is not defined"
     ]
    }
   ],
   "source": [
    "from torchinfo import summary\n",
    "summary(G1, (1, 3, 360, 480))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJ5hGAuX9Lr3"
   },
   "source": [
    "<!-- ## FFAT + RAMiT + NAFNet -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error RAMiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "LMopfquO9WCb"
   },
   "outputs": [],
   "source": [
    "# # RAMiT (Reciprocal Attention Mixing Transformer)\n",
    "\n",
    "# from pdb import set_trace as stx\n",
    "# import numbers\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# from torch import nn, Tensor\n",
    "# from einops import rearrange\n",
    "# import torch.nn.functional as F\n",
    "# import torch\n",
    "# from torchvision.transforms import functional as TF\n",
    "# import math\n",
    "# from RAMiT.my_model.common.mean_std import mean_std\n",
    "# from RAMiT.utils.etc_utils import denormalize\n",
    "# from timm.models.fx_features import register_notrace_function\n",
    "# from timm.models.layers import trunc_normal_, to_2tuple\n",
    "# from torch.nn.modules.batchnorm import _BatchNorm\n",
    "# def make_model(args, opts, pe):\n",
    "#     model = RAMiT(target_mode=args.target_mode,\n",
    "#                   img_norm=args.img_norm,\n",
    "#                   in_chans=opts['in_chans'],\n",
    "#                   dim = opts['dim'],\n",
    "#                   depths = opts['depths'],\n",
    "#                   num_heads = opts['num_heads'],\n",
    "#                   head_dim = opts['head_dim'],\n",
    "#                   chsa_head_ratio = opts['chsa_head_ratio'],\n",
    "#                   window_size = opts['window_size'],\n",
    "#                   hidden_ratio = opts['hidden_ratio'],\n",
    "#                   qkv_bias = opts['qkv_bias'],\n",
    "#                   mv_ver = opts['mv_ver'],\n",
    "#                   exp_factor=opts['exp_factor'],\n",
    "#                   expand_groups=opts['expand_groups'],\n",
    "#                   act_layer = opts['act_layer'],\n",
    "#                   norm_layer = opts['norm_layer'],\n",
    "#                   tail_mv = opts['tail_mv'],\n",
    "#                   attn_drop = opts['attn_drop'],\n",
    "#                   proj_drop = opts['proj_drop'],\n",
    "#                   drop_path = opts['drop_path'],\n",
    "#                   helper = opts['helper'],\n",
    "#                   mv_act = opts['mv_act']\n",
    "#                  )\n",
    "#     num_params = sum([p.numel() for n,p in model.named_parameters()])\n",
    "#     print(format(num_params, ','))\n",
    "#     return model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # ---------------------------------------------------------------------------Shadow_R---------------------------------------------------------------------------\n",
    "\n",
    "# def to_3d(x):\n",
    "#     return rearrange(x, 'b c h w -> b (h w) c')\n",
    "\n",
    "# def to_4d(x,h,w):\n",
    "#     return rearrange(x, 'b (h w) c -> b c h w',h=h,w=w)\n",
    "\n",
    "# class BiasFree_LayerNorm(nn.Module):\n",
    "#     def __init__(self, normalized_shape):\n",
    "#         super(BiasFree_LayerNorm, self).__init__()\n",
    "#         if isinstance(normalized_shape, numbers.Integral):\n",
    "#             normalized_shape = (normalized_shape,)\n",
    "#         normalized_shape = torch.Size(normalized_shape)\n",
    "\n",
    "#         assert len(normalized_shape) == 1\n",
    "\n",
    "#         self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "#         self.normalized_shape = normalized_shape\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         sigma = x.var(-1, keepdim=True, unbiased=False)\n",
    "#         return x / torch.sqrt(sigma+1e-5) * self.weight\n",
    "\n",
    "# class WithBias_LayerNorm(nn.Module):\n",
    "#     def __init__(self, normalized_shape):\n",
    "#         super(WithBias_LayerNorm, self).__init__()\n",
    "#         if isinstance(normalized_shape, numbers.Integral):\n",
    "#             normalized_shape = (normalized_shape,)\n",
    "#         normalized_shape = torch.Size(normalized_shape)\n",
    "\n",
    "#         assert len(normalized_shape) == 1\n",
    "\n",
    "#         self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
    "#         self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
    "#         self.normalized_shape = normalized_shape\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         mu = x.mean(-1, keepdim=True)\n",
    "#         sigma = x.var(-1, keepdim=True, unbiased=False)\n",
    "#         return (x - mu) / torch.sqrt(sigma+1e-5) * self.weight + self.bias\n",
    "\n",
    "\n",
    "# class LayerNorm(nn.Module):\n",
    "#     def __init__(self, dim, LayerNorm_type):\n",
    "#         super(LayerNorm, self).__init__()\n",
    "#         if LayerNorm_type =='BiasFree':\n",
    "#             self.body = BiasFree_LayerNorm(dim)\n",
    "#         else:\n",
    "#             self.body = WithBias_LayerNorm(dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         h, w = x.shape[-2:]\n",
    "#         return to_4d(self.body(to_3d(x)), h, w)\n",
    "\n",
    "\n",
    "\n",
    "# ##########################################################################\n",
    "# ## Gated-Dconv Feed-Forward Network (GDFN)\n",
    "# class FeedForwardFFAT(nn.Module):\n",
    "#     def __init__(self, dim, ffn_expansion_factor, bias):\n",
    "#         super(FeedForwardFFAT, self).__init__()\n",
    "\n",
    "#         hidden_features = int(dim*ffn_expansion_factor)\n",
    "\n",
    "#         self.project_in = nn.Conv2d(dim, hidden_features*2, kernel_size=1, bias=bias)\n",
    "\n",
    "#         self.dwconv = nn.Conv2d(hidden_features*2, hidden_features*2, kernel_size=3, stride=1, padding=1, groups=hidden_features*2, bias=bias)\n",
    "\n",
    "#         self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = self.project_in(x)\n",
    "#         x1, x2 = self.dwconv(x).chunk(2, dim=1)\n",
    "#         x = F.gelu(x1) * x2\n",
    "#         x = self.project_out(x)\n",
    "#         return x\n",
    "\n",
    "\n",
    "\n",
    "# class Attention(nn.Module):\n",
    "#     def __init__(self, dim, num_heads, bias):\n",
    "#         super(Attention, self).__init__()\n",
    "\n",
    "#         self.num_heads = num_heads\n",
    "\n",
    "#         self.to_hidden = nn.Conv2d(dim, dim * 6, kernel_size=1, bias=bias)\n",
    "#         self.to_hidden_dw = nn.Conv2d(dim * 6, dim * 6, kernel_size=3, stride=1, padding=1, groups=dim * 6, bias=bias)\n",
    "\n",
    "#         self.project_out = nn.Conv2d(dim * 2, dim, kernel_size=1, bias=bias)\n",
    "\n",
    "#         self.norm = LayerNorm(dim * 2, LayerNorm_type='WithBias')\n",
    "\n",
    "#         self.patch_size = 2 # 8\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         hidden = self.to_hidden(x)\n",
    "\n",
    "#         q, k, v = self.to_hidden_dw(hidden).chunk(3, dim=1)\n",
    "\n",
    "#         q_patch = rearrange(q, 'b c (h patch1) (w patch2) -> b c h w patch1 patch2', patch1=self.patch_size,\n",
    "#                             patch2=self.patch_size)\n",
    "#         k_patch = rearrange(k, 'b c (h patch1) (w patch2) -> b c h w patch1 patch2', patch1=self.patch_size,\n",
    "#                             patch2=self.patch_size)\n",
    "#         q_fft = torch.fft.rfft2(q_patch.float())\n",
    "#         k_fft = torch.fft.rfft2(k_patch.float())\n",
    "\n",
    "#         out = q_fft * k_fft\n",
    "#         out = torch.fft.irfft2(out, s=(self.patch_size, self.patch_size))\n",
    "#         out = rearrange(out, 'b c h w patch1 patch2 -> b c (h patch1) (w patch2)', patch1=self.patch_size,\n",
    "#                         patch2=self.patch_size)\n",
    "\n",
    "#         out = self.norm(out)\n",
    "\n",
    "#         output = v * out\n",
    "#         output = self.project_out(output)\n",
    "\n",
    "#         return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ##########################################################################\n",
    "# class TransformerBlock(nn.Module):\n",
    "#     def __init__(self, dim, num_heads, ffn_expansion_factor, bias, LayerNorm_type):\n",
    "#         super(TransformerBlock, self).__init__()\n",
    "\n",
    "#         self.norm1 = LayerNorm(dim, LayerNorm_type)\n",
    "#         self.attn = Attention(dim, num_heads, bias)\n",
    "#         self.norm2 = LayerNorm(dim, LayerNorm_type)\n",
    "#         self.ffn = FeedForwardFFAT(dim, ffn_expansion_factor, bias)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x = x + self.attn(self.norm1(x))\n",
    "#         x = x + self.ffn(self.norm2(x))\n",
    "\n",
    "#         return x\n",
    "# # ---------------------------------------------------------------------------Shadow_R---------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# class ShallowModule(nn.Module):\n",
    "#     def __init__(self, in_chans, out_chans, kernel_size=3, stride=1):\n",
    "#         super(ShallowModule, self).__init__()\n",
    "\n",
    "#         self.in_chans = in_chans\n",
    "#         self.out_chans = out_chans\n",
    "#         self.kernel_size = kernel_size\n",
    "#         self.stride = stride\n",
    "\n",
    "#         self.conv = nn.Conv2d(in_chans, out_chans, kernel_size, stride, kernel_size//2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.conv(x)\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         return resolutions[0]*resolutions[1] * self.kernel_size*self.kernel_size * self.in_chans * self.out_chans\n",
    "\n",
    "# class QKVProjection(nn.Module):\n",
    "#     def __init__(self, dim, num_head, qkv_bias=True):\n",
    "#         super(QKVProjection, self).__init__()\n",
    "#         self.dim = dim\n",
    "#         self.num_head = num_head\n",
    "\n",
    "#         self.qkv = nn.Conv2d(dim, 3*dim, 1, bias=qkv_bias)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, C, H, W = x.size()\n",
    "#         qkv = self.qkv(x)\n",
    "#         qkv = rearrange(qkv, 'b (l c) h w -> b l c h w', l=self.num_head)\n",
    "#         return qkv\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         return resolutions[0]*resolutions[1] * 1*1 * self.dim * 3*self.dim\n",
    "\n",
    "# def get_relative_position_index(win_h, win_w):\n",
    "#     # get pair-wise relative position index for each token inside the window\n",
    "#     coords = torch.stack(torch.meshgrid([torch.arange(win_h), torch.arange(win_w)]))  # 2, Wh, Ww\n",
    "#     coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
    "#     relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww (xaxis matrix & yaxis matrix)\n",
    "#     relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
    "#     relative_coords[:, :, 0] += win_h - 1  # shift to start from 0\n",
    "#     relative_coords[:, :, 1] += win_w - 1\n",
    "#     relative_coords[:, :, 0] *= 2 * win_w - 1\n",
    "#     return relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
    "\n",
    "# class SpatialSelfAttention(nn.Module):\n",
    "#     def __init__(self, dim, num_head, total_head, window_size=8, shift=0, attn_drop=0.0, proj_drop=0.0, helper=True):\n",
    "#         super(SpatialSelfAttention, self).__init__()\n",
    "#         self.dim = dim\n",
    "#         self.num_head = num_head\n",
    "#         self.total_head = total_head\n",
    "#         self.window_size = window_size\n",
    "#         self.window_area = window_size**2\n",
    "#         self.shift = shift\n",
    "#         self.helper = helper\n",
    "\n",
    "#         self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_head, 1, 1))), requires_grad=True)\n",
    "\n",
    "#         # define a parameter table of relative position bias, shape: 2*Wh-1 * 2*Ww-1, nH\n",
    "#         self.relative_position_bias_table = nn.Parameter(torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_head))\n",
    "\n",
    "#         # get pair-wise relative position index for each token inside the window\n",
    "#         self.register_buffer(\"relative_position_index\", get_relative_position_index(window_size, window_size))\n",
    "\n",
    "#         self.attn_drop = nn.Dropout(attn_drop)\n",
    "\n",
    "#         self.proj = nn.Conv2d(dim*num_head, dim*num_head, 1)\n",
    "#         self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "#     def _get_rel_pos_bias(self) -> torch.Tensor:\n",
    "#         relative_position_bias = self.relative_position_bias_table[\n",
    "#             self.relative_position_index.view(-1)].view(self.window_area, self.window_area, -1)  # Wh*Ww,Wh*Ww,nH\n",
    "#         relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
    "#         return relative_position_bias.unsqueeze(0)\n",
    "\n",
    "#     def forward(self, qkv, ch=None):\n",
    "#         B, L, C, H, W = qkv.size()\n",
    "#         # window shift\n",
    "#         if self.shift > 0:\n",
    "#             qkv = torch.roll(qkv, shifts=(-self.shift, -self.shift), dims=(-2,-1))\n",
    "\n",
    "#         # window partition\n",
    "#         q,k,v = rearrange(qkv, 'b l c (h wh) (w ww) -> (b h w) l (wh ww) c',\n",
    "#                           wh=self.window_size, ww=self.window_size).chunk(3, dim=-1) # [B_, L1, hw, C/L] respectively\n",
    "#         if ch is not None and self.helper: # [B, C, H, W]\n",
    "#             if self.shift > 0:\n",
    "#                 ch = torch.roll(ch, shifts=(-self.shift, -self.shift), dims=(-2,-1))\n",
    "#             ch = rearrange(ch, 'b (l c) (h wh) (w ww) -> (b h w) l (wh ww) c',\n",
    "#                            l=self.total_head-self.num_head, wh=self.window_size, ww=self.window_size) # [B_, L1, hw, C/L]\n",
    "#             ch = torch.mean(ch, dim=1, keepdim=True) # head squeeze [B_, 1, hw, C/L]\n",
    "#             v = v*ch # [B_, L1, hw, C/L]\n",
    "\n",
    "#         attn = F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(2,-1) # [B_, L1, hw, hw]\n",
    "#         logit_scale = torch.clamp(self.logit_scale, max=math.log(1. / 0.01)).exp()\n",
    "#         attn = attn * logit_scale\n",
    "\n",
    "#         attn = attn + self._get_rel_pos_bias()\n",
    "#         attn = self.attn_drop(F.softmax(attn, dim=-1))\n",
    "\n",
    "#         x = attn @ v # [B_, L1, hw, C/L]\n",
    "\n",
    "#         # window unpartition + head merge\n",
    "#         x = window_unpartition(x, (H,W), self.window_size) # [B, L1*C/L, H, W]\n",
    "#         x = self.proj_drop(self.proj(x))\n",
    "\n",
    "#         # window reverse shift\n",
    "#         if self.shift > 0:\n",
    "#             x = torch.roll(x, shifts=(self.shift, self.shift), dims=(-2,-1))\n",
    "\n",
    "#         return x\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         H,W = resolutions\n",
    "#         num_wins = H//self.window_size * W//self.window_size\n",
    "#         flops = self.num_head * H*W * self.dim if self.helper else 0 # v = v*ch\n",
    "#         flops += num_wins * self.num_head * self.window_area * self.dim * self.window_area # attn = Q@K^T\n",
    "#         flops += num_wins * self.num_head * self.window_area * self.window_area * self.dim # attn@V\n",
    "#         flops += H*W * 1*1 * self.num_head*self.dim * self.num_head*self.dim # self.proj\n",
    "#         return flops\n",
    "\n",
    "# @register_notrace_function\n",
    "# def window_unpartition(x, resolutions, window_size):\n",
    "#     return rearrange(x, '(b h w) l (wh ww) c -> b (l c) (h wh) (w ww)',\n",
    "#                      h=resolutions[0]//window_size, w=resolutions[1]//window_size, wh=window_size)\n",
    "\n",
    "# class ChannelSelfAttention(nn.Module):\n",
    "#     def __init__(self, dim, num_head, total_head, attn_drop=0.0, proj_drop=0.0, helper=True):\n",
    "#         super(ChannelSelfAttention, self).__init__()\n",
    "#         self.dim = dim\n",
    "#         self.num_head = num_head\n",
    "#         self.total_head = total_head\n",
    "#         self.helper = helper\n",
    "\n",
    "#         self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_head, 1, 1))), requires_grad=True)\n",
    "\n",
    "#         self.attn_drop = nn.Dropout(attn_drop)\n",
    "\n",
    "#         self.proj = nn.Conv2d(dim*num_head, dim*num_head, 1)\n",
    "#         self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "#     def forward(self, qkv, sp=None):\n",
    "#         B, L, C, H, W = qkv.size()\n",
    "\n",
    "#         q,k,v = rearrange(qkv, 'b l c h w -> b l c (h w)').chunk(3, dim=-2) # [B, L2, C/L, HW]\n",
    "#         if sp is not None and self.helper:\n",
    "#             sp = torch.mean(sp, dim=1, keepdim=True) # channel squeeze # [B, 1, H, W]\n",
    "#             sp = rearrange(sp, 'b (l c) h w -> b l c (h w)', l=1) # [B, 1, 1, HW]\n",
    "#             v = v*sp # [B, L2, C/L, HW]\n",
    "\n",
    "#         attn = F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(2,-1) # [B, L2, C/L, C/L]\n",
    "#         logit_scale = torch.clamp(self.logit_scale, max=math.log(1. / 0.01)).exp()\n",
    "#         attn = attn * logit_scale\n",
    "\n",
    "#         attn = F.softmax(attn, dim=-1)\n",
    "#         attn = self.attn_drop(attn)\n",
    "\n",
    "#         x = attn @ v # [B, L2, C/L, HW]\n",
    "\n",
    "#         # head merge\n",
    "#         x = rearrange(x, 'b l c (h w) -> b (l c) h w', h=H) # [B, L2*C/L, H, W]\n",
    "#         x = self.proj_drop(self.proj(x)) # [B, L2*C/L, H, W]\n",
    "\n",
    "#         return x\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         H,W = resolutions\n",
    "#         flops = self.num_head * self.dim * H*W if self.helper else 0 # v = v*sp\n",
    "#         flops += self.num_head * self.dim * H*W * self.dim # attn = Q@K^T\n",
    "#         flops += self.num_head * self.dim * self.dim * H*W # attn@V\n",
    "#         flops += H*W * 1*1 * self.num_head*self.dim * self.num_head*self.dim # self.proj\n",
    "#         return flops\n",
    "\n",
    "# class ReshapeLayerNorm(nn.Module):\n",
    "#     def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
    "#         super(ReshapeLayerNorm, self).__init__()\n",
    "\n",
    "#         self.dim = dim\n",
    "#         self.norm = norm_layer(dim)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, C, H, W = x.size()\n",
    "#         x = rearrange(x, 'b c h w -> b (h w) c')\n",
    "#         x = self.norm(x)\n",
    "#         x = rearrange(x, 'b (h w) c -> b c h w', h=H)\n",
    "#         return x\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         H,W = resolutions\n",
    "#         flops = 0\n",
    "#         flops += H*W * self.dim\n",
    "#         return flops\n",
    "\n",
    "# class MobiVari1(nn.Module): # MobileNet v1 Variants\n",
    "#     def __init__(self, dim, kernel_size, stride, act=nn.LeakyReLU, out_dim=None):\n",
    "#         super(MobiVari1, self).__init__()\n",
    "#         self.dim = dim\n",
    "#         self.kernel_size = kernel_size\n",
    "#         self.out_dim = out_dim or dim\n",
    "\n",
    "#         self.dw_conv = nn.Conv2d(dim, dim, kernel_size, stride, kernel_size//2, groups=dim)\n",
    "#         self.pw_conv = nn.Conv2d(dim, self.out_dim, 1, 1, 0)\n",
    "#         self.act = act()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         out = self.act(self.pw_conv(self.act(self.dw_conv(x))+x))\n",
    "#         return out + x if self.dim==self.out_dim else out\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         H,W = resolutions\n",
    "#         flops = H*W * self.kernel_size*self.kernel_size * self.dim  +  H*W * 1*1 * self.dim * self.out_dim # self.dw_conv + self.pw_conv\n",
    "#         return flops\n",
    "\n",
    "# class MobiVari2(MobiVari1): # MobileNet v2 Variants\n",
    "#     def __init__(self, dim, kernel_size, stride, act=nn.LeakyReLU, out_dim=None, exp_factor=1.2, expand_groups=4):\n",
    "#         super(MobiVari2, self).__init__(dim, kernel_size, stride, act, out_dim)\n",
    "#         self.expand_groups = expand_groups\n",
    "#         expand_dim = int(dim*exp_factor)\n",
    "#         expand_dim = expand_dim+(expand_groups-expand_dim%expand_groups)\n",
    "#         self.expand_dim = expand_dim\n",
    "\n",
    "#         self.exp_conv = nn.Conv2d(dim, self.expand_dim, 1, 1, 0, groups=expand_groups)\n",
    "#         self.dw_conv = nn.Conv2d(expand_dim, expand_dim, kernel_size, stride, kernel_size//2, groups=expand_dim)\n",
    "#         self.pw_conv = nn.Conv2d(expand_dim, self.out_dim, 1, 1, 0)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         x1 = self.act(self.exp_conv(x))\n",
    "#         out = self.pw_conv(self.act(self.dw_conv(x1)+x1))\n",
    "#         return out + x if self.dim==self.out_dim else out\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         H,W = resolutions\n",
    "#         flops = H*W * 1*1 * (self.dim//self.expand_groups) * self.expand_dim # self.exp_conv\n",
    "#         flops += H*W * self.kernel_size*self.kernel_size * self.expand_dim # self.dw_conv\n",
    "#         flops += H*W * 1*1 * self.expand_dim * self.out_dim # self.pw_conv\n",
    "#         return flops\n",
    "\n",
    "# class FeedForward(nn.Module):\n",
    "#     def __init__(self, dim, hidden_ratio, act_layer=nn.GELU, bias=True, drop=0.0):\n",
    "#         super(FeedForward, self).__init__()\n",
    "\n",
    "#         self.dim = dim\n",
    "#         self.hidden_ratio = hidden_ratio\n",
    "\n",
    "#         self.hidden = nn.Conv2d(dim, int(dim*hidden_ratio), 1, bias=bias)\n",
    "#         self.drop1 = nn.Dropout(drop)\n",
    "#         self.out = nn.Conv2d(int(dim*hidden_ratio), dim, 1, bias=bias)\n",
    "#         self.drop2 = nn.Dropout(drop)\n",
    "#         self.act = act_layer()\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.drop2(self.out(self.drop1(self.act(self.hidden(x)))))\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         H,W = resolutions\n",
    "#         flops = 2 * H*W * 1*1 * self.dim * self.dim*self.hidden_ratio # self.hidden + self.out\n",
    "#         return flops\n",
    "\n",
    "# class NoLayer(nn.Identity):\n",
    "#     def __init__(self):\n",
    "#         super(NoLayer, self).__init__()\n",
    "#     def flops(self, resolutions):\n",
    "#         return 0\n",
    "#     def forward(self, x, **kwargs):\n",
    "#         return x.flatten(1,2)\n",
    "\n",
    "# class DRAMiTransformer(nn.Module): # Reciprocal Attention Transformer Block\n",
    "#     def __init__(self, dim, num_head, chsa_head_ratio, FFAT, window_size=8, shift=0, head_dim=None, qkv_bias=True, mv_ver=1,\n",
    "#                  hidden_ratio=2.0, act_layer=nn.GELU, norm_layer=ReshapeLayerNorm, attn_drop=0.0, proj_drop=0.0, drop_path=0.0, helper=True,\n",
    "#                  mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4,\n",
    "#                  num_blocks = [4,6,6,8], heads = [1,2,4,8], ffn_expansion_factor = 2.66, bias = False, LayerNorm_type = 'WithBias'):\n",
    "#         super(DRAMiTransformer, self).__init__()\n",
    "\n",
    "#         self.dim = dim\n",
    "#         self.num_head = num_head\n",
    "#         self.window_size = window_size\n",
    "#         self.chsa_head = int(num_head*chsa_head_ratio)\n",
    "#         self.shift = shift\n",
    "#         self.helper = helper\n",
    "#         self.FFAT = FFAT\n",
    "\n",
    "#         self.qkv_proj = QKVProjection(dim, num_head, qkv_bias=qkv_bias)\n",
    "#         self.sp_attn = SpatialSelfAttention(dim//num_head, num_head-self.chsa_head, num_head,\n",
    "#                                             window_size, shift, attn_drop, proj_drop, helper) if num_head-self.chsa_head != 0 else NoLayer()\n",
    "#         self.ch_attn = ChannelSelfAttention(dim//num_head, self.chsa_head, num_head, attn_drop, proj_drop, helper) if self.chsa_head != 0 else NoLayer()\n",
    "\n",
    "\n",
    "#         #FFAT\n",
    "#         if self.FFAT:\n",
    "#           self.encoder_level1 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "#         #FFAT\n",
    "\n",
    "\n",
    "#         if mv_ver==1:\n",
    "#             self.mobivari = MobiVari1(dim, 3, 1, act=mv_act)\n",
    "#         elif mv_ver==2:\n",
    "#             self.mobivari = MobiVari2(dim, 3, 1, act=mv_act, out_dim=None, exp_factor=exp_factor, expand_groups=expand_groups)\n",
    "\n",
    "#             # NAFBlock---------------------------------------------\n",
    "#         elif mv_ver==3:\n",
    "#             self.mobivari = NAFBlock(c=self.dim)\n",
    "#             # -------------------------------------------------------\n",
    "\n",
    "#         self.norm1 = norm_layer(dim)\n",
    "\n",
    "#         self.ffn = FeedForward(dim, hidden_ratio, act_layer=act_layer)\n",
    "#         self.norm2 = norm_layer(dim)\n",
    "\n",
    "#         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
    "\n",
    "#     def forward(self, x, sp_=None, ch_=None):\n",
    "#         B, C, H, W = x.size()\n",
    "\n",
    "#         # QKV projection + head split\n",
    "#         qkv = self.qkv_proj(x) # [B, L, C, H, W]\n",
    "\n",
    "#         # SP-SA / CH-SA\n",
    "#         sp = self.sp_attn(qkv[:,:self.num_head-self.chsa_head], ch=ch_) # [B, L1*C/L, H, W]\n",
    "#         ch = self.ch_attn(qkv[:,self.num_head-self.chsa_head:], sp=sp_) # [B, L2*C/L, H, W]\n",
    "\n",
    "\n",
    "#         #FFAT\n",
    "#         if self.FFAT:\n",
    "#           FFAT_block = self.encoder_level1(torch.cat([sp, ch], dim=1))\n",
    "#           attn0 = self.mobivari(FFAT_block)\n",
    "#         #FFAT\n",
    "\n",
    "\n",
    "#         else:\n",
    "#           attn0 = self.mobivari(torch.cat([sp, ch], dim=1)) # merge [B, C, H, W]\n",
    "#         attn = self.drop_path(self.norm1(attn0)) + x # LN, skip connection [B, C, H, W]\n",
    "\n",
    "#         # FFN\n",
    "#         out = self.drop_path(self.norm2(self.ffn(attn))) + attn # FFN, LN, skip connection [B, C, H, W]\n",
    "\n",
    "#         return out, sp, ch, attn0\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         flops = self.qkv_proj.flops(resolutions)\n",
    "#         flops += self.sp_attn.flops(resolutions)\n",
    "#         flops += self.ch_attn.flops(resolutions)\n",
    "#         flops += self.mobivari.flops(resolutions)\n",
    "#         flops += self.norm1.flops(resolutions)\n",
    "#         flops += self.ffn.flops(resolutions)\n",
    "#         flops += self.norm2.flops(resolutions)\n",
    "#         params = sum([p.numel() for n,p in self.named_parameters()])\n",
    "#         return flops\n",
    "\n",
    "# class EncoderStage(nn.Module):\n",
    "#     def __init__(self, depth, dim, num_head, chsa_head_ratio, FFAT, window_size=8, head_dim=None,\n",
    "#                  qkv_bias=True, mv_ver=1, hidden_ratio=2.0, act_layer=nn.GELU, norm_layer=ReshapeLayerNorm,\n",
    "#                  attn_drop=0.0, proj_drop=0.0, drop_path=0.0, helper=True, mv_act=nn.LeakyReLU,):\n",
    "#         super(EncoderStage, self).__init__()\n",
    "\n",
    "#         self.depth = depth\n",
    "#         self.dim = dim\n",
    "#         self.num_head = num_head\n",
    "#         self.window_size = window_size\n",
    "#         shift = window_size//2\n",
    "\n",
    "#         self.blocks = nn.ModuleList()\n",
    "#         for d in range(depth):\n",
    "#             self.blocks.add_module(f'block{d}', DRAMiTransformer(dim, num_head, chsa_head_ratio, FFAT,\n",
    "#                                                                  window_size, 0 if d%2==0 else shift, head_dim, qkv_bias,\n",
    "#                                                                  mv_ver, hidden_ratio, act_layer, norm_layer,\n",
    "#                                                                  attn_drop, proj_drop, drop_path, helper, mv_act,\n",
    "#                                                                  ))\n",
    "#     def forward(self, x):\n",
    "#         sp, ch = None, None\n",
    "#         for i, blk in enumerate(self.blocks):\n",
    "#             x, sp, ch, attn = blk(x, sp, ch)\n",
    "#             sp, ch = (None, None) if (sp.size(1)==0 or ch.size(1)==0) else (sp, ch) # pure SPSA or CHSA\n",
    "#         return x, attn\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         flops = 0\n",
    "#         for blk in self.blocks:\n",
    "#             flops += blk.flops(resolutions)\n",
    "#         return flops\n",
    "\n",
    "# class Downsizing(nn.Module):\n",
    "#     \"\"\" Patch Merging Layer.\n",
    "\n",
    "#     Args:\n",
    "#         dim (int): Number of input dimension (channels).\n",
    "#         downsample_dim (int, optional): Number of output dimension (channels) (dim if not set).  Default: None\n",
    "#         norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
    "#     \"\"\"\n",
    "\n",
    "#     def __init__(self, dim, downsample_dim=None, norm_layer=ReshapeLayerNorm, mv_ver=1, mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4):\n",
    "#         super(Downsizing, self).__init__()\n",
    "#         self.dim = dim\n",
    "#         self.downsample_dim = downsample_dim or dim\n",
    "#         self.norm = norm_layer(4*dim)\n",
    "#         if mv_ver==1:\n",
    "#             self.reduction = MobiVari1(4*dim, 3, 1, act=mv_act, out_dim=self.downsample_dim)\n",
    "#         elif mv_ver==2:\n",
    "#             self.reduction = MobiVari2(4*dim, 3, 1, act=mv_act, out_dim=self.downsample_dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
    "\n",
    "#             # NAFBlock---------------------------------------------\n",
    "#         elif mv_ver==3:\n",
    "#             self.reduction = NAFBlock(c=4*dim)\n",
    "#             # -------------------------------------------------------\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         B, C, H, W = x.size()\n",
    "\n",
    "#         # Concat 2x2\n",
    "#         x0 = x[:, :, 0::2, 0::2]  # [B, C, H/2, W/2], top-left\n",
    "#         x1 = x[:, :, 0::2, 1::2]  # [B, C, H/2, W/2], top-right\n",
    "#         x2 = x[:, :, 1::2, 0::2]  # [B, C, H/2, W/2], bottom-left\n",
    "#         x3 = x[:, :, 1::2, 1::2]  # [B, C, H/2, W/2], bottom-right\n",
    "#         x = torch.cat([x0, x1, x2, x3], dim=1)  # [B, 4C, H/2, W/2]\n",
    "#         return self.reduction(self.norm(x)) # [B, C, H/2, W/2]\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         H,W = resolutions\n",
    "#         flops = self.norm.flops((H//2,W//2)) + self.reduction.flops((H//2,W//2))\n",
    "#         return flops\n",
    "\n",
    "# class Bottleneck(nn.Module):\n",
    "#     def __init__(self, dim, num_stages, act_layer=nn.GELU, norm_layer=ReshapeLayerNorm, mv_ver=1, mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4):\n",
    "#         super(Bottleneck, self).__init__()\n",
    "\n",
    "#         self.dim = dim\n",
    "#         self.cat_dim = sum([dim//(4**x) for x in range(num_stages)])\n",
    "#         if mv_ver==1:\n",
    "#             self.mobivari = MobiVari1(self.cat_dim, 3, 1, act=mv_act, out_dim=dim)\n",
    "#         elif mv_ver==2:\n",
    "#             self.mobivari = MobiVari2(self.cat_dim, 3, 1, act=mv_act, out_dim=dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
    "\n",
    "#             # NAFBlock---------------------------------------------\n",
    "#         elif mv_ver==3:\n",
    "#             self.mobivari = NAFBlock(c=self.dim)\n",
    "#             # -------------------------------------------------------\n",
    "#         self.act = act_layer()\n",
    "#         self.norm = norm_layer(dim)\n",
    "\n",
    "#     def shallow_down(self, x, r):\n",
    "#         for _ in range(r):\n",
    "#             x = F.max_pool2d(x, 2)\n",
    "#         return F.leaky_relu(x)\n",
    "\n",
    "#     def forward(self, x_list):\n",
    "#         xs = x_list[0]\n",
    "#         new_x = []\n",
    "#         for i in range(len(x_list[1:])):\n",
    "#             x_ = x_list[i+1]+self.shallow_down(xs, i)\n",
    "#             new_x.append(F.pixel_shuffle(x_, 2**i))\n",
    "#         new_x = self.norm(self.mobivari(torch.cat(new_x, dim=1)))\n",
    "#         return new_x\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         H,W = resolutions\n",
    "#         flops = 0\n",
    "#         # self.shallow_down (iterative max-pool)\n",
    "#         flops += (H//2)*(W//2) * 2*2 * self.dim # shallow-down into stage2 output (iter1)\n",
    "#         flops += (H//2)*(W//2) * 2*2 * self.dim # shallow-down into stage3 output (iter1)\n",
    "#         flops += (H//4)*(W//4) * 2*2 * self.dim # shallow-down into stage3 output (iter2)\n",
    "#         flops += self.mobivari.flops((H,W))\n",
    "#         flops += self.norm.flops((H,W))\n",
    "#         return flops\n",
    "\n",
    "# class HRAMi(nn.Module):\n",
    "#     def __init__(self, dim, kernel_size=3, stride=1, mv_ver=1, mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4):\n",
    "#         super(HRAMi, self).__init__()\n",
    "\n",
    "#         self.dim = dim\n",
    "#         self.kernel_size = kernel_size\n",
    "\n",
    "#         if mv_ver==1:\n",
    "#             self.mobivari = MobiVari1(dim+dim//4+dim//16+dim, kernel_size, stride, act=mv_act, out_dim=dim)\n",
    "#         elif mv_ver==2:\n",
    "#             self.mobivari = MobiVari2(dim+dim//4+dim//16+dim, kernel_size, stride, act=mv_act, out_dim=dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
    "#             # NAFBlock---------------------------------------------\n",
    "#         elif mv_ver==3:\n",
    "#             self.mobivari = NAFBlock(c=self.dim)\n",
    "#             # -------------------------------------------------------\n",
    "\n",
    "#     def forward(self, attn_list):\n",
    "#         for i, attn in enumerate(attn_list[:-1]):\n",
    "#             attn = F.pixel_shuffle(attn, 2**i)\n",
    "#             x = attn if i==0 else torch.cat([x, attn], dim=1)\n",
    "#         x = torch.cat([x, attn_list[-1]], dim=1)\n",
    "#         x = self.mobivari(x)\n",
    "#         return x\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         return self.mobivari.flops(resolutions)\n",
    "\n",
    "# class Reconstruction(nn.Module):\n",
    "#     def __init__(self, target_mode, dim, kernel_size=3, stride=1, num_mv=2, mv_ver=1, mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4):\n",
    "#         super(Reconstruction, self).__init__()\n",
    "\n",
    "#         self.upscale = upscale = int(target_mode[-1]) if target_mode in ['light_x2', 'light_x3', 'light_x4'] else 1\n",
    "#         self.target_mode = target_mode\n",
    "#         self.dim = dim\n",
    "#         self.out_chans = 3 if 'gray' not in target_mode else 1\n",
    "#         self.kernel_size = kernel_size\n",
    "#         self.stride = stride\n",
    "#         self.num_mv = num_mv\n",
    "\n",
    "#         self.mobivari = nn.ModuleList()\n",
    "#         for i in range(num_mv):\n",
    "#             if mv_ver==1:\n",
    "#                 self.mobivari.add_module(f'mobivari{i}', MobiVari1(dim, kernel_size, stride, mv_act))\n",
    "#             elif mv_ver==2:\n",
    "#                 self.mobivari.add_module(f'mobivari{i}', MobiVari2(dim, kernel_size, stride, mv_act, None, exp_factor, expand_groups))\n",
    "#                 # NAFBlock---------------------------------------------\n",
    "#             elif mv_ver==3:\n",
    "#                 self.mobivari.add_module(f'mobivari{i}', NAFBlock(c=self.dim))\n",
    "#                 # -------------------------------------------------------\n",
    "\n",
    "\n",
    "#         self.conv = nn.Conv2d(dim, self.out_chans*(upscale**2), kernel_size, stride, kernel_size//2)\n",
    "#         self.shuffle = nn.PixelShuffle(upscale) if upscale!=1 else nn.Identity()\n",
    "#         self.final_conv = nn.Conv2d(self.out_chans, self.out_chans, kernel_size, stride, kernel_size//2)\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         for mobivari in self.mobivari:\n",
    "#             x = mobivari(x)\n",
    "#         return self.final_conv(self.shuffle(self.conv(x)))\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         H,W = resolutions\n",
    "#         flops = 0\n",
    "#         for mobivari in self.mobivari: # self.mobivari\n",
    "#             flops += mobivari.flops((H,W))\n",
    "#         flops += H*W * self.kernel_size*self.kernel_size * self.dim * self.out_chans*(self.upscale**2) # self.conv\n",
    "#         flops += H*W * self.kernel_size*self.kernel_size * self.out_chans * self.out_chans # self.final_conv\n",
    "#         return flops\n",
    "\n",
    "# class RAMiT_FFAT(nn.Module):\n",
    "#     def __init__(self, in_chans=3, dim=64, depths=(1,6,4,4,6), num_heads=(4,4,4,4), head_dim=None, chsa_head_ratio=0.25,\n",
    "#                  window_size=8, hidden_ratio=2.0, qkv_bias=True, mv_ver=1, exp_factor=1.2, expand_groups=4,\n",
    "#                  act_layer=nn.GELU, norm_layer=ReshapeLayerNorm, tail_mv=2, weight_init='', target_mode='light_x2', img_norm=True,\n",
    "#                  attn_drop=0.0, proj_drop=0.0, drop_path=0.0, helper=True, mv_act=nn.LeakyReLU,\n",
    "#                  num_blocks = [1,4,6,6,8],\n",
    "#                  heads = [1,2,4,8],\n",
    "#                  ffn_expansion_factor = 2.66,\n",
    "#                  bias = False,\n",
    "#                  LayerNorm_type = 'WithBias',   ## Other option 'BiasFree'\n",
    "#                  FFAT = False, **kwargs):\n",
    "#         super(RAMiT_FFAT, self).__init__()\n",
    "#         self.cat_dim = sum([dim//(4**x) for x in range(len(depths)-1)]) # added to use the torchinfor.summary\n",
    "#         self.unit = 2**(len(depths)-2)*window_size\n",
    "\n",
    "#         self.in_chans = in_chans\n",
    "#         self.dim = dim\n",
    "#         self.depths = depths\n",
    "#         self.num_heads = num_heads\n",
    "#         self.head_dim = head_dim\n",
    "#         self.window_size = window_size\n",
    "#         self.hidden_ratio = hidden_ratio\n",
    "#         self.qkv_bias = qkv_bias\n",
    "#         self.act_layer = act_layer\n",
    "#         norm_layer = ReshapeLayerNorm if norm_layer == 'ReshapeLayerNorm' else norm_layer\n",
    "#         self.norm_layer = norm_layer = ReshapeLayerNorm if norm_layer == 'ReshapeLayerNorm' else norm_layer\n",
    "#         self.tail_mv = tail_mv\n",
    "\n",
    "#         self.scale = int(target_mode[-1]) if target_mode in ['light_x2', 'light_x3', 'light_x4'] else 1\n",
    "#         self.mean, self.std = mean_std(self.scale, target_mode)\n",
    "#         self.target_mode = target_mode\n",
    "#         self.img_norm = img_norm\n",
    "\n",
    "#         dpr = [x.item() for x in torch.linspace(0, drop_path, sum(depths))]  # stochastic depth decay rule\n",
    "\n",
    "#         self.shallow = ShallowModule(in_chans, dim, 3, 1)\n",
    "#         self.stage1 = EncoderStage(depths[0], dim, num_heads[0], chsa_head_ratio, FFAT, window_size, head_dim, qkv_bias, mv_ver,\n",
    "#                                    hidden_ratio, act_layer, norm_layer, attn_drop, proj_drop, drop_path, helper, mv_act)\n",
    "\n",
    "\n",
    "#         # FFAT\n",
    "#         self.encoder_level1 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "\n",
    "#         self.encoder_level2 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "\n",
    "#         self.encoder_level3 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
    "#         # FFAT\n",
    "\n",
    "\n",
    "#         self.down1 = Downsizing(dim, dim, norm_layer, mv_ver, mv_act)\n",
    "#         self.stage2 = EncoderStage(depths[0], dim, num_heads[1], chsa_head_ratio, FFAT, window_size, head_dim, qkv_bias, mv_ver,\n",
    "#                                    hidden_ratio, act_layer, norm_layer, attn_drop, proj_drop, drop_path, helper, mv_act)\n",
    "#         self.down2 = Downsizing(dim, dim, norm_layer, mv_ver, mv_act)\n",
    "#         self.stage3 = EncoderStage(depths[0], dim, num_heads[2], chsa_head_ratio, FFAT, window_size, head_dim, qkv_bias, mv_ver,\n",
    "#                                    hidden_ratio, act_layer, norm_layer, attn_drop, proj_drop, drop_path, helper, mv_act)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         if mv_ver==1:\n",
    "#             self.mobivari_b = MobiVari1(self.cat_dim, 3, 1, act=mv_act, out_dim=dim)\n",
    "#         elif mv_ver==2:\n",
    "#             self.mobivari_b = MobiVari2(self.cat_dim, 3, 1, act=mv_act, out_dim=dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
    "#             # NAFBlock---------------------------------------------\n",
    "#         elif mv_ver==3:\n",
    "#             self.mobivari_b = NAFBlock(c=self.dim, out_channel=dim)\n",
    "#             # -------------------------------------------------------\n",
    "#         self.act = act_layer()\n",
    "#         self.norm = norm_layer(dim)\n",
    "#         # self.bottleneck = Bottleneck(dim, len(depths)-1, act_layer, norm_layer, mv_ver, mv_act)\n",
    "\n",
    "#         self.stage4 = EncoderStage(depths[0], dim, num_heads[3], chsa_head_ratio, FFAT, window_size, head_dim, qkv_bias, mv_ver,\n",
    "#                                    hidden_ratio, act_layer, norm_layer, attn_drop, proj_drop, drop_path, helper, mv_act)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#         if mv_ver==1:\n",
    "#             self.mobivari_mix = MobiVari1(dim+dim//4+dim//16+dim, 3, 1, act=mv_act, out_dim=dim)\n",
    "#         elif mv_ver==2:\n",
    "#             self.mobivari_mix = MobiVari2(dim+dim//4+dim//16+dim, 3, 1, act=mv_act, out_dim=dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
    "#              # NAFBlock---------------------------------------------\n",
    "#         elif mv_ver==3:\n",
    "#             self.mobivari_mix = NAFBlock(c=dim+dim//4+dim//16+dim, out_channel=dim)\n",
    "#             # -------------------------------------------------------\n",
    "#         # self.attn_mix = HRAMi(dim, 3, 1, mv_ver, mv_act)\n",
    "\n",
    "\n",
    "#         self.to_target = Reconstruction(target_mode, dim, 3, 1, tail_mv, mv_ver, mv_act, exp_factor, expand_groups)\n",
    "\n",
    "#         self.apply(self._init_weights)\n",
    "\n",
    "#     def forward_size_norm(self, x):\n",
    "#         _,_,h,w = x.size()\n",
    "#         padh = self.unit-(h%self.unit) if h%self.unit!=0 else 0\n",
    "#         padw = self.unit-(w%self.unit) if w%self.unit!=0 else 0\n",
    "#         x = TF.pad(x, (0,0,padw,padh))\n",
    "#         return TF.normalize(x, self.mean, self.std) if self.img_norm else x\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         _,_,H_ori,W_ori = x.size()\n",
    "#         x = self.forward_size_norm(x)\n",
    "#         B, C, H, W = x.size()\n",
    "#         x_lq = x if self.scale == 1 else None # due to residual image (except for only SR)\n",
    "\n",
    "#         shallow = self.shallow(x)\n",
    "#         o1_, attn1 = self.stage1(shallow) # [B, C, H, W]\n",
    "\n",
    "\n",
    "#         # FFAT\n",
    "#         o1_ = self.encoder_level1(o1_)\n",
    "#         # FFAT\n",
    "\n",
    "\n",
    "#         o1 = self.down1(o1_) # [B, C, H//2, W//2]\n",
    "\n",
    "#         o2_, attn2 = self.stage2(o1) # [B, C, H//2, W//2]\n",
    "\n",
    "\n",
    "#         # FFAT\n",
    "#         o2_ = self.encoder_level2(o2_)\n",
    "#         # FFAT\n",
    "\n",
    "\n",
    "#         o2 = self.down2(o2_) # [B, C, H//4, W//4]\n",
    "\n",
    "#         o3_, attn3 = self.stage3(o2) # [B, C, H//4, W//4]\n",
    "\n",
    "\n",
    "#         # FFAT\n",
    "#         o3_ = self.encoder_level3(o3_)\n",
    "#         # FFAT\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # Bottleneck\n",
    "#         def shallow_down( x, r):\n",
    "#           for _ in range(r):\n",
    "#               x = F.max_pool2d(x, 2)\n",
    "#           return F.leaky_relu(x)\n",
    "#         x_list = [shallow, o1_, o2_, o3_]\n",
    "#         xs = x_list[0]\n",
    "#         new_x = []\n",
    "#         for i in range(len(x_list[1:])):\n",
    "#             x_ = x_list[i+1]+shallow_down(xs, i)\n",
    "#             new_x.append(F.pixel_shuffle(x_, 2**i))\n",
    "#         ob = self.norm(self.mobivari_b(torch.cat(new_x, dim=1)))\n",
    "#         # ob = self.bottleneck([shallow, o1_, o2_, o3_]) # [B, C, H, W]\n",
    "\n",
    "\n",
    "\n",
    "#         o4, attn4 = self.stage4(ob) # [B, C, H, W]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # HRAMiT\n",
    "#         attn_list = [attn1, attn2, attn3, attn4]\n",
    "#         for i, attn in enumerate(attn_list[:-1]):\n",
    "#             attn = F.pixel_shuffle(attn, 2**i)\n",
    "#             x = attn if i==0 else torch.cat([x, attn], dim=1)\n",
    "#         x = torch.cat([x, attn_list[-1]], dim=1)\n",
    "#         mix = self.mobivari_mix(x)\n",
    "#         # mix = self.attn_mix([attn1, attn2, attn3, attn4]) # [B, C, H, W]\n",
    "\n",
    "\n",
    "\n",
    "#         o4 = o4 * mix # [B, C, H, W]\n",
    "\n",
    "#         out = self.to_target(o4+shallow) # global skip connection\n",
    "#         out = out + x_lq if self.scale == 1 else out # residual output (except for only SR)\n",
    "\n",
    "#         out = denormalize(out, self.mean, self.std) if self.img_norm else out\n",
    "#         out = out[:, :, :H_ori*self.scale, :W_ori*self.scale]\n",
    "#         return out\n",
    "\n",
    "#     def flops(self, resolutions):\n",
    "#         H_ori,W_ori = resolutions\n",
    "#         padh = self.unit-(H_ori%self.unit) if H_ori%self.unit!=0 else 0\n",
    "#         padw = self.unit-(W_ori%self.unit) if W_ori%self.unit!=0 else 0\n",
    "#         H,W = H_ori+padh,W_ori+padw\n",
    "#         flops = 0\n",
    "#         flops += self.shallow.flops((H,W))\n",
    "#         flops += self.stage1.flops((H,W))\n",
    "#         flops += self.down1.flops((H,W))\n",
    "#         flops += self.stage2.flops((H//2,W//2))\n",
    "#         flops += self.down2.flops((H//2,W//2))\n",
    "#         flops += self.stage3.flops((H//4,W//4))\n",
    "#         flops += self.bottleneck.flops((H,W))\n",
    "#         flops += self.stage4.flops((H,W))\n",
    "#         flops += self.attn_mix.flops((H,W))\n",
    "#         flops += self.dim * H*W # o4 = o4*mix\n",
    "#         flops += self.to_target.flops((H,W))\n",
    "#         return flops\n",
    "\n",
    "#     def _init_weights(self, m):\n",
    "#         # Swin V2 manner\n",
    "#         if isinstance(m, nn.Linear):\n",
    "#             trunc_normal_(m.weight, std=.02)\n",
    "#             if isinstance(m, nn.Linear) and m.bias is not None:\n",
    "#                 nn.init.constant_(m.bias, 0)\n",
    "#         elif isinstance(m, nn.LayerNorm):\n",
    "#             nn.init.constant_(m.bias, 0)\n",
    "#             nn.init.constant_(m.weight, 1.0)\n",
    "\n",
    "#     @torch.jit.ignore\n",
    "#     def no_weight_decay(self):\n",
    "#         nwd = set()\n",
    "#         for n, _ in self.named_parameters():\n",
    "#             if 'relative_position_bias_table' in n:\n",
    "#                 nwd.add(n)\n",
    "#         return nwd\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # ----------------------------Shadow_R-------------------------\n",
    "#     # @torch.no_grad()\n",
    "#     # def _init_weights(module_list, scale=1, bias_fill=0, **kwargs):\n",
    "#     #   \"\"\"Initialize network weights.\n",
    "\n",
    "#     #   Args:\n",
    "#     #       module_list (list[nn.Module] | nn.Module): Modules to be initialized.\n",
    "#     #       scale (float): Scale initialized weights, especially for residual\n",
    "#     #           blocks. Default: 1.\n",
    "#     #       bias_fill (float): The value to fill bias. Default: 0\n",
    "#     #       kwargs (dict): Other arguments for initialization function.\n",
    "#     #   \"\"\"\n",
    "#     #   if not isinstance(module_list, list):\n",
    "#     #       module_list = [module_list]\n",
    "#     #   for module in module_list:\n",
    "#     #       for m in module.modules():\n",
    "#     #           if isinstance(m, nn.Conv2d):\n",
    "#     #               nn.init.kaiming_normal_(m.weight, **kwargs)\n",
    "#     #               m.weight.data *= scale\n",
    "#     #               if m.bias is not None:\n",
    "#     #                   m.bias.data.fill_(bias_fill)\n",
    "#     #           elif isinstance(m, nn.Linear):\n",
    "#     #               nn.init.kaiming_normal_(m.weight, **kwargs)\n",
    "#     #               m.weight.data *= scale\n",
    "#     #               if m.bias is not None:\n",
    "#     #                   m.bias.data.fill_(bias_fill)\n",
    "#     #           elif isinstance(m, _BatchNorm):\n",
    "#     #               nn.init.constant_(m.weight, 1)\n",
    "#     #               if m.bias is not None:\n",
    "#     #                   m.bias.data.fill_(bias_fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8XqRCHSS9cY1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 436,713 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "# model = RAMiT_FFAT(target_mode='light_realdn',img_norm=False, mv_ver=1)\n",
    "# model.to('cuda')\n",
    "# def count_parameters(model):\n",
    "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "# # Create the model\n",
    "\n",
    "# # Print the number of parameters\n",
    "# print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "T1h2p0sx9fSX"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [ShallowModule: 1, Conv2d: 2, EncoderStage: 1, DRAMiTransformer: 3, QKVProjection: 4, Conv2d: 5, SpatialSelfAttention: 4, Dropout: 5, Conv2d: 5, Dropout: 5, ChannelSelfAttention: 4, Dropout: 5, Conv2d: 5, Dropout: 5, MobiVari1: 4, Conv2d: 5, LeakyReLU: 5, Conv2d: 5, LeakyReLU: 5, ReshapeLayerNorm: 4, LayerNorm: 5, Identity: 4, FeedForward: 4, Conv2d: 5, GELU: 5, Dropout: 5, Conv2d: 5, Dropout: 5, ReshapeLayerNorm: 4, LayerNorm: 5, Identity: 4, Sequential: 1, TransformerBlock: 2, LayerNorm: 3, WithBias_LayerNorm: 4, Attention: 3, Conv2d: 4, Conv2d: 4, LayerNorm: 4, WithBias_LayerNorm: 5, Conv2d: 4, LayerNorm: 3, WithBias_LayerNorm: 4, FeedForwardFFAT: 3, Conv2d: 4, Conv2d: 4, Conv2d: 4, Downsizing: 1, ReshapeLayerNorm: 2, LayerNorm: 3, MobiVari1: 2, Conv2d: 3, LeakyReLU: 3, Conv2d: 3, LeakyReLU: 3, EncoderStage: 1, DRAMiTransformer: 3, QKVProjection: 4, Conv2d: 5, SpatialSelfAttention: 4, Dropout: 5, Conv2d: 5, Dropout: 5, ChannelSelfAttention: 4, Dropout: 5, Conv2d: 5, Dropout: 5, MobiVari1: 4, Conv2d: 5, LeakyReLU: 5, Conv2d: 5, LeakyReLU: 5, ReshapeLayerNorm: 4, LayerNorm: 5, Identity: 4, FeedForward: 4, Conv2d: 5, GELU: 5, Dropout: 5, Conv2d: 5, Dropout: 5, ReshapeLayerNorm: 4, LayerNorm: 5, Identity: 4, Sequential: 1, TransformerBlock: 2, LayerNorm: 3, WithBias_LayerNorm: 4, Attention: 3, Conv2d: 4, Conv2d: 4, LayerNorm: 4, WithBias_LayerNorm: 5, Conv2d: 4, LayerNorm: 3, WithBias_LayerNorm: 4, FeedForwardFFAT: 3, Conv2d: 4, Conv2d: 4, Conv2d: 4, Downsizing: 1, ReshapeLayerNorm: 2, LayerNorm: 3, MobiVari1: 2, Conv2d: 3, LeakyReLU: 3, Conv2d: 3, LeakyReLU: 3, EncoderStage: 1, DRAMiTransformer: 3, QKVProjection: 4, Conv2d: 5, SpatialSelfAttention: 4, Dropout: 5, Conv2d: 5, Dropout: 5, ChannelSelfAttention: 4, Dropout: 5, Conv2d: 5, Dropout: 5, MobiVari1: 4, Conv2d: 5, LeakyReLU: 5, Conv2d: 5, LeakyReLU: 5, ReshapeLayerNorm: 4, LayerNorm: 5, Identity: 4, FeedForward: 4, Conv2d: 5, GELU: 5, Dropout: 5, Conv2d: 5, Dropout: 5, ReshapeLayerNorm: 4, LayerNorm: 5, Identity: 4, Sequential: 1, TransformerBlock: 2, LayerNorm: 3, WithBias_LayerNorm: 4, Attention: 3, Conv2d: 4, Conv2d: 4, LayerNorm: 4, WithBias_LayerNorm: 5, Conv2d: 4, LayerNorm: 3, WithBias_LayerNorm: 4, FeedForwardFFAT: 3, Conv2d: 4, Conv2d: 4, Conv2d: 4]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torchinfo\\torchinfo.py:295\u001b[0m, in \u001b[0;36mforward_pass\u001b[1;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[0;32m    294\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[1;32m--> 295\u001b[0m     _ \u001b[38;5;241m=\u001b[39m model(\u001b[38;5;241m*\u001b[39mx, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    296\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mdict\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\nn\\modules\\module.py:1603\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1601\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1603\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "Cell \u001b[1;32mIn[11], line 908\u001b[0m, in \u001b[0;36mRAMiT_FFAT.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    907\u001b[0m     new_x\u001b[38;5;241m.\u001b[39mappend(F\u001b[38;5;241m.\u001b[39mpixel_shuffle(x_, \u001b[38;5;241m2\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mi))\n\u001b[1;32m--> 908\u001b[0m ob \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmobivari_b\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    909\u001b[0m \u001b[38;5;66;03m# ob = self.bottleneck([shallow, o1_, o2_, o3_]) # [B, C, H, W]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\nn\\modules\\module.py:1603\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1601\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1603\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "Cell \u001b[1;32mIn[11], line 430\u001b[0m, in \u001b[0;36mMobiVari1.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m--> 430\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpw_conv(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdw_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\u001b[38;5;241m+\u001b[39mx))\n\u001b[0;32m    431\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out \u001b[38;5;241m+\u001b[39m x \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdim\u001b[38;5;241m==\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mout_dim \u001b[38;5;28;01melse\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\nn\\modules\\module.py:1603\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1601\u001b[0m     args \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(args)\n\u001b[1;32m-> 1603\u001b[0m result \u001b[38;5;241m=\u001b[39m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1604\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\nn\\modules\\conv.py:458\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\nn\\modules\\conv.py:454\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    452\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    453\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 454\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    455\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Given groups=85, weight of size [85, 1, 3, 3], expected input[1, 84, 256, 256] to have 85 channels, but got 84 channels instead",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchinfo\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summary\n\u001b[1;32m----> 2\u001b[0m \u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torchinfo\\torchinfo.py:223\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, input_data, batch_dim, cache_forward_pass, col_names, col_width, depth, device, dtypes, mode, row_settings, verbose, **kwargs)\u001b[0m\n\u001b[0;32m    216\u001b[0m validate_user_params(\n\u001b[0;32m    217\u001b[0m     input_data, input_size, columns, col_width, device, dtypes, verbose\n\u001b[0;32m    218\u001b[0m )\n\u001b[0;32m    220\u001b[0m x, correct_input_size \u001b[38;5;241m=\u001b[39m process_input(\n\u001b[0;32m    221\u001b[0m     input_data, input_size, batch_dim, device, dtypes\n\u001b[0;32m    222\u001b[0m )\n\u001b[1;32m--> 223\u001b[0m summary_list \u001b[38;5;241m=\u001b[39m forward_pass(\n\u001b[0;32m    224\u001b[0m     model, x, batch_dim, cache_forward_pass, device, model_mode, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    225\u001b[0m )\n\u001b[0;32m    226\u001b[0m formatting \u001b[38;5;241m=\u001b[39m FormattingOptions(depth, verbose, columns, col_width, rows)\n\u001b[0;32m    227\u001b[0m results \u001b[38;5;241m=\u001b[39m ModelStatistics(\n\u001b[0;32m    228\u001b[0m     summary_list, correct_input_size, get_total_memory_used(x), formatting\n\u001b[0;32m    229\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torchinfo\\torchinfo.py:304\u001b[0m, in \u001b[0;36mforward_pass\u001b[1;34m(model, x, batch_dim, cache_forward_pass, device, mode, **kwargs)\u001b[0m\n\u001b[0;32m    302\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    303\u001b[0m     executed_layers \u001b[38;5;241m=\u001b[39m [layer \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m summary_list \u001b[38;5;28;01mif\u001b[39;00m layer\u001b[38;5;241m.\u001b[39mexecuted]\n\u001b[1;32m--> 304\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to run torchinfo. See above stack traces for more details. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuted layers up to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexecuted_layers\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    307\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m hooks:\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to run torchinfo. See above stack traces for more details. Executed layers up to: [ShallowModule: 1, Conv2d: 2, EncoderStage: 1, DRAMiTransformer: 3, QKVProjection: 4, Conv2d: 5, SpatialSelfAttention: 4, Dropout: 5, Conv2d: 5, Dropout: 5, ChannelSelfAttention: 4, Dropout: 5, Conv2d: 5, Dropout: 5, MobiVari1: 4, Conv2d: 5, LeakyReLU: 5, Conv2d: 5, LeakyReLU: 5, ReshapeLayerNorm: 4, LayerNorm: 5, Identity: 4, FeedForward: 4, Conv2d: 5, GELU: 5, Dropout: 5, Conv2d: 5, Dropout: 5, ReshapeLayerNorm: 4, LayerNorm: 5, Identity: 4, Sequential: 1, TransformerBlock: 2, LayerNorm: 3, WithBias_LayerNorm: 4, Attention: 3, Conv2d: 4, Conv2d: 4, LayerNorm: 4, WithBias_LayerNorm: 5, Conv2d: 4, LayerNorm: 3, WithBias_LayerNorm: 4, FeedForwardFFAT: 3, Conv2d: 4, Conv2d: 4, Conv2d: 4, Downsizing: 1, ReshapeLayerNorm: 2, LayerNorm: 3, MobiVari1: 2, Conv2d: 3, LeakyReLU: 3, Conv2d: 3, LeakyReLU: 3, EncoderStage: 1, DRAMiTransformer: 3, QKVProjection: 4, Conv2d: 5, SpatialSelfAttention: 4, Dropout: 5, Conv2d: 5, Dropout: 5, ChannelSelfAttention: 4, Dropout: 5, Conv2d: 5, Dropout: 5, MobiVari1: 4, Conv2d: 5, LeakyReLU: 5, Conv2d: 5, LeakyReLU: 5, ReshapeLayerNorm: 4, LayerNorm: 5, Identity: 4, FeedForward: 4, Conv2d: 5, GELU: 5, Dropout: 5, Conv2d: 5, Dropout: 5, ReshapeLayerNorm: 4, LayerNorm: 5, Identity: 4, Sequential: 1, TransformerBlock: 2, LayerNorm: 3, WithBias_LayerNorm: 4, Attention: 3, Conv2d: 4, Conv2d: 4, LayerNorm: 4, WithBias_LayerNorm: 5, Conv2d: 4, LayerNorm: 3, WithBias_LayerNorm: 4, FeedForwardFFAT: 3, Conv2d: 4, Conv2d: 4, Conv2d: 4, Downsizing: 1, ReshapeLayerNorm: 2, LayerNorm: 3, MobiVari1: 2, Conv2d: 3, LeakyReLU: 3, Conv2d: 3, LeakyReLU: 3, EncoderStage: 1, DRAMiTransformer: 3, QKVProjection: 4, Conv2d: 5, SpatialSelfAttention: 4, Dropout: 5, Conv2d: 5, Dropout: 5, ChannelSelfAttention: 4, Dropout: 5, Conv2d: 5, Dropout: 5, MobiVari1: 4, Conv2d: 5, LeakyReLU: 5, Conv2d: 5, LeakyReLU: 5, ReshapeLayerNorm: 4, LayerNorm: 5, Identity: 4, FeedForward: 4, Conv2d: 5, GELU: 5, Dropout: 5, Conv2d: 5, Dropout: 5, ReshapeLayerNorm: 4, LayerNorm: 5, Identity: 4, Sequential: 1, TransformerBlock: 2, LayerNorm: 3, WithBias_LayerNorm: 4, Attention: 3, Conv2d: 4, Conv2d: 4, LayerNorm: 4, WithBias_LayerNorm: 5, Conv2d: 4, LayerNorm: 3, WithBias_LayerNorm: 4, FeedForwardFFAT: 3, Conv2d: 4, Conv2d: 4, Conv2d: 4]"
     ]
    }
   ],
   "source": [
    "# from torchinfo import summary\n",
    "# summary(model, (1, 3, 256, 256))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zzTpesFq0AP"
   },
   "source": [
    "<!-- ## Data loader -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "chSRMdZxOO4V"
   },
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# from torchvision import transforms\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# import os\n",
    "# import numpy as np\n",
    "# import torch\n",
    "\n",
    "\n",
    "\n",
    "# class ImageRestorationDataset(Dataset):\n",
    "#     def __init__(self, root_dir, transform=None, num_samples=None):\n",
    "#         self.root_dir = root_dir\n",
    "#         self.transform = transform\n",
    "#         self.input_dir = os.path.join(root_dir, 'crop_inputs')\n",
    "#         self.target_dir = os.path.join(root_dir, 'crop_targets')\n",
    "#         self.input_filenames = [f for f in sorted(os.listdir(self.input_dir)) if os.path.isfile(os.path.join(self.input_dir, f))]\n",
    "#         self.target_filenames = [f for f in sorted(os.listdir(self.target_dir)) if os.path.isfile(os.path.join(self.target_dir, f))]\n",
    "\n",
    "#         if num_samples is not None:\n",
    "#             self.input_filenames = self.input_filenames[:num_samples]\n",
    "#             self.target_filenames = self.target_filenames[:num_samples]\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.input_filenames)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         input_path = os.path.join(self.input_dir, self.input_filenames[idx])\n",
    "#         target_path = os.path.join(self.target_dir, self.target_filenames[idx])\n",
    "#         input_image = Image.open(input_path).convert('RGB')\n",
    "#         target_image = Image.open(target_path).convert('RGB')\n",
    "\n",
    "\n",
    "#         if self.transform:\n",
    "#             input_image = self.transform(input_image)\n",
    "#             target_image = self.transform(target_image)\n",
    "\n",
    "#             # input_imaget = np.array(input_image, dtype=np.uint8)\n",
    "#             # target_image = np.array(target_image, dtype=np.uint8)\n",
    "#             # input_imaget = input_imaget.astype(np.float16)\n",
    "#             # target_image = target_image.astype(np.float16)\n",
    "#             # input_imaget = input_imaget.transpose(2, 1, 0)\n",
    "#             # target_image = target_image.transpose(2, 1, 0)\n",
    "#             # input_imaget = torch.from_numpy(input_imaget).type(torch.float16)\n",
    "#             # target_image = torch.from_numpy(target_image).type(torch.float16)\n",
    "\n",
    "#         # print(type(self.input_filenames))\n",
    "#         return input_image, target_image\n",
    "\n",
    "# # Define the transformations\n",
    "# transform = transforms.Compose([\n",
    "#     # transforms.Resize((128, 128)),  # Example resize\n",
    "#     transforms.ToTensor(),  # Convert to PyTorch tensors\n",
    "#     transforms.Normalize(mean=(0.0000, 0.0000, 0.0000), std=(1.0000, 1.0000, 1.0000))\n",
    "\n",
    "# ])\n",
    "\n",
    "# # Load the dataset\n",
    "# train_dataset = ImageRestorationDataset(root_dir='/content/drive/MyDrive/istd_dataset/train', transform = transform, num_samples= 8400)\n",
    "# val_dataset = ImageRestorationDataset(root_dir='/content/drive/MyDrive/istd_dataset/val',transform = transform, num_samples= 1680)\n",
    "# # print(type(train_dataset))\n",
    "# # Create DataLoaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
    "\n",
    "# # Example: Iterate through the dataset\n",
    "# # for inputs, targets in train_loader:\n",
    "#     # print(f\"Batch of input images has shape: {inputs.shape}\")\n",
    "#     # print(f\"Batch of target images has shape: {targets.shape}\")\n",
    "#     # Further processing...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EE4SLCh4-DwS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 1330\n",
      "Number of validation samples: 540\n",
      "Number of training batches: 665\n",
      "Number of validation batches: 68\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as TF\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "class ShadowRemovalDataset(Dataset):\n",
    "    def __init__(self, gt_dir, lq_dir, transform=None, augment=False):\n",
    "        self.gt_dir = gt_dir\n",
    "        self.lq_dir = lq_dir\n",
    "        self.transform = transform\n",
    "        self.augment = augment\n",
    "\n",
    "        # Filter out directories and non-image files\n",
    "        self.gt_images = sorted([f for f in os.listdir(gt_dir) if os.path.isfile(os.path.join(gt_dir, f)) and f.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'tiff'))])\n",
    "        self.lq_images = sorted([f for f in os.listdir(lq_dir) if os.path.isfile(os.path.join(lq_dir, f)) and f.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'tiff'))])\n",
    "        # self.gt_images = self.gt_images[:80]\n",
    "        # self.lq_images = self.gt_images[:80]\n",
    "    def __len__(self):\n",
    "        return len(self.gt_images)\n",
    "\n",
    "    def augment_images(self, gt_image, lq_image):\n",
    "        # Random rotation\n",
    "        angle = random.choice([0, 90, 180, 270])\n",
    "        gt_image = TF.rotate(gt_image, angle)\n",
    "        lq_image = TF.rotate(lq_image, angle)\n",
    "\n",
    "        # Random horizontal flipping\n",
    "        if random.random() > 0.5:\n",
    "            gt_image = TF.hflip(gt_image)\n",
    "            lq_image = TF.hflip(lq_image)\n",
    "\n",
    "        # Random vertical flipping\n",
    "        if random.random() > 0.5:\n",
    "            gt_image = TF.vflip(gt_image)\n",
    "            lq_image = TF.vflip(lq_image)\n",
    "\n",
    "        return gt_image, lq_image\n",
    "\n",
    "    def random_crop(self, gt_image, lq_image):\n",
    "        i, j, h, w = transforms.RandomCrop.get_params(\n",
    "            #Random crop size\n",
    "            gt_image, output_size=(360, 480))\n",
    "        gt_image = TF.crop(gt_image, i, j, h, w)\n",
    "        lq_image = TF.crop(lq_image, i, j, h, w)\n",
    "        return gt_image, lq_image\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        gt_img_path = os.path.join(self.gt_dir, self.gt_images[idx])\n",
    "        lq_img_path = os.path.join(self.lq_dir, self.lq_images[idx])\n",
    "\n",
    "        gt_image = Image.open(gt_img_path).convert(\"RGB\")\n",
    "        lq_image = Image.open(lq_img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.augment:\n",
    "            #gt_image, lq_image = self.augment_images(gt_image, lq_image)\n",
    "            gt_image, lq_image = self.random_crop(gt_image, lq_image)\n",
    "\n",
    "        if self.transform:\n",
    "            gt_image = self.transform(gt_image)\n",
    "            lq_image = self.transform(lq_image)\n",
    "\n",
    "        return lq_image, gt_image\n",
    "\n",
    "# Load the dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# Define paths to your dataset in Google Drive\n",
    "train_gt_dir = r'C:\\Users\\Admin\\Desktop\\shadow_removal_results\\ISTD\\train\\targets+'\n",
    "train_lq_dir = r'C:\\Users\\Admin\\Desktop\\shadow_removal_results\\ISTD\\train\\inputs'\n",
    "val_gt_dir = r'C:\\Users\\Admin\\Desktop\\shadow_removal_results\\ISTD\\val\\targets+'\n",
    "val_lq_dir = r'C:\\Users\\Admin\\Desktop\\shadow_removal_results\\ISTD\\val\\inputs'\n",
    "\n",
    "# Replace with your dataset path\n",
    "train_dataset = ShadowRemovalDataset(gt_dir=train_gt_dir, lq_dir=train_lq_dir, transform=transform, augment=True)\n",
    "val_dataset = ShadowRemovalDataset(gt_dir=val_gt_dir, lq_dir=val_lq_dir, transform=transform, augment=False)\n",
    "\n",
    "batch_size = 2 # Set the batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, pin_memory=True)\n",
    "\n",
    "# Check the number of files (samples) in the datasets\n",
    "print(f\"Number of training samples: {len(train_dataset)}\")\n",
    "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
    "\n",
    "# Check the number of batches in the DataLoaders\n",
    "print(f\"Number of training batches: {len(train_loader)}\")\n",
    "print(f\"Number of validation batches: {len(val_loader)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-1GpNypQq5gR"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "wHBpsePEyAEZ"
   },
   "outputs": [],
   "source": [
    "# import time\n",
    "# from tqdm import tqdm\n",
    "# from torchmetrics.functional import structural_similarity_index_measure as ssim\n",
    "# from lpips import LPIPS\n",
    "\n",
    "# # transfrom the data from tensors to numpys to calculate the PSNR\n",
    "# def tensor_to_numpy(tensor):\n",
    "#     tensor = tensor.permute(1, 2, 0).cpu().numpy()  # Change from (C, H, W) to (H, W, C)\n",
    "#     tensor = (tensor * 255).astype(np.uint8)  # Optional: Scale to [0, 255] and convert to uint8\n",
    "#     return tensor\n",
    "# # ----------------------------------------------------\n",
    "\n",
    "# # uste the transfromed data to calculate the PSNR\n",
    "# def calculate_psnr(img1_tensor, img2_tensor):\n",
    "#     img1 = tensor_to_numpy(img1_tensor)\n",
    "#     img2 = tensor_to_numpy(img2_tensor)\n",
    "#     psnr_value = psnr(img1, img2)\n",
    "#     return psnr_value\n",
    "# # -------------------------------------------------\n",
    "\n",
    "# lpips_loss = LPIPS(net='alex').to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "vpzE-J7Ft38b"
   },
   "outputs": [],
   "source": [
    "# def evaluate(model: nn.Module, val_loader) -> float:\n",
    "#     model.eval()  # Turn on evaluation mode\n",
    "\n",
    "#     # initialize all losses\n",
    "#     criterion = nn.L1Loss()\n",
    "#     # lpips_loss = LPIPS(net='alex').to('cuda')\n",
    "#     # -----------------------------------------------\n",
    "\n",
    "#     # initialize losses values\n",
    "#     total_loss = 0.0\n",
    "#     total_psnr = 0.0\n",
    "#     total_ssim = 0.0\n",
    "#     total_lpips = 0.0\n",
    "#   # -----------------------------------------\n",
    "\n",
    "# # list of low result\n",
    "#     output_l = []\n",
    "#     targets_l = []\n",
    "#     inputs_l = []\n",
    "# # ------------------------------------\n",
    "\n",
    "#     # list of top results\n",
    "#     tev_out = []\n",
    "#     tev_inputs = []\n",
    "#     tev_targets = []\n",
    "# # ---------------------------------------\n",
    "\n",
    "#     with torch.no_grad():\n",
    "#         # with tqdm(total=len(val_loader), desc=\"Validating\", unit=\"batch\") as val_pbar:\n",
    "#             for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "#                 inputs, targets = inputs.to('cuda'), targets.to('cuda')\n",
    "#                 output = model(inputs)\n",
    "\n",
    "#                 # L1 loss\n",
    "#                 loss = criterion(output, targets)\n",
    "#                 # ------------------------------------------\n",
    "\n",
    "#                 # ssim loss\n",
    "#                 ssim_L = ssim(output, targets, data_range=1.0)\n",
    "#                 # -------------------------------------------\n",
    "\n",
    "#                 # lpips loss\n",
    "#                 # batch_lpips = 0.0\n",
    "#                 img1 = (targets * 2) - 1\n",
    "#                 img2 = (output * 2) - 1\n",
    "#                 # for i in range(inputs.size(0)):\n",
    "#                 #   batch_lpips += lpips_loss(img1[i], img2[i]).item()\n",
    "#                 # batch_lpips /= inputs.size(0)\n",
    "#                 batch_lpips = lpips_loss(img1, img2).mean()\n",
    "#                 # -----------------------------------------------\n",
    "\n",
    "#                 # Calculate the PSNR for each batch\n",
    "#                 batch_psnr = 0\n",
    "#                 for i in range(inputs.size(0)):\n",
    "#                     batch_psnr += calculate_psnr(output[i].detach(), targets[i])\n",
    "#                 batch_psnr /= inputs.size(0)\n",
    "#                 # ------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#                 # total loss for all losses\n",
    "#                 total_loss += loss.item() # total loss for L1\n",
    "#                 total_psnr += batch_psnr # total loss for psnr\n",
    "#                 total_ssim += ssim_L.item() # total loss for ssim\n",
    "#                 total_lpips += batch_lpips # total loss for lpips\n",
    "#                 # -----------------------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "#                 # top and low results\n",
    "#                 if batch_psnr > 10 and batch_psnr <= 16:\n",
    "#                   output_l.append(output.to('cpu'))\n",
    "#                   targets_l.append(targets.to('cpu'))\n",
    "#                   inputs_l.append(inputs.to('cpu'))\n",
    "#                 elif batch_psnr >= 29:\n",
    "#                   tev_out.append(output.to('cpu'))\n",
    "#                   tev_inputs.append(inputs.to('cpu'))\n",
    "#                   tev_targets.append(targets.to('cpu'))\n",
    "#                   # ------------------------------------------\n",
    "\n",
    "\n",
    "#                 # Update progress bar for the current batch\n",
    "#                 # val_pbar.set_postfix({\"loss\": loss.item(), \"batch_psnr\": batch_psnr, 'batch_ssim': ssim_L.item(), 'batch_lpips': batch_lpips})\n",
    "#                 # val_pbar.update(1)\n",
    "\n",
    "#     avg_loss = total_loss / len(val_loader)\n",
    "#     avg_psnr = total_psnr / len(val_loader)\n",
    "#     avg_ssim = total_ssim / len(val_loader)\n",
    "#     avg_lpips = total_lpips / len(val_loader)\n",
    "#     return  tev_out, tev_inputs, tev_targets, output_l, targets_l, inputs_l, avg_loss,  avg_psnr, avg_ssim, avg_lpips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "-S8fcHRPRGbi"
   },
   "outputs": [],
   "source": [
    "# def train(model: nn.Module, train_loader, epochs) -> None:\n",
    "#     model.train()  # Turn on train mode\n",
    "\n",
    "#     lr = 0.001\n",
    "#     criterion = nn.L1Loss()\n",
    "#     # lpips_loss = LPIPS(net='alex').to('cuda')\n",
    "\n",
    "#     optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.99), eps=1e-08)\n",
    "\n",
    "#     top_output = []\n",
    "#     top_inputs = []\n",
    "#     top_targets = []\n",
    "#     low_output = []\n",
    "#     low_inputs = []\n",
    "#     low_targets = []\n",
    "#     for epoch in range(1, epochs+1):\n",
    "#             total_loss = 0.0\n",
    "#             total_psnr = 0.0\n",
    "#             total_ssim = 0.0\n",
    "#             total_lpips = 0.0\n",
    "#             start_time = time.time()\n",
    "#             batch_num = 1\n",
    "#         # Initialize tqdm progress bar\n",
    "#         # with tqdm(total=len(train_loader), desc=f\"Epoch {epoch}/{epochs}\", unit=\"batch\") as pbar:\n",
    "#             for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "#                 inputs, targets = inputs.to('cuda'), targets.to('cuda')\n",
    "#                 optimizer.zero_grad()\n",
    "#                 output = model(inputs)\n",
    "\n",
    "#                 # L1 loss + backprop\n",
    "#                 loss = criterion(output, targets)\n",
    "#                 loss.backward()\n",
    "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "#                 optimizer.step()\n",
    "#                 # ------------------------------------------------\n",
    "\n",
    "#                 # ssim loss\n",
    "#                 ssim_L = ssim(output, targets, data_range=1.0)\n",
    "#                 # -----------------------------------------------\n",
    "\n",
    "#                 # lpips loss\n",
    "#                 batch_lpips = 0.0\n",
    "#                 img1 = (targets * 2) - 1\n",
    "#                 img2 = (output * 2) - 1\n",
    "#                 # for i in range(inputs.size(0)):\n",
    "#                 #   batch_lpips += lpips_loss(img1[i], img2[i]).item()\n",
    "#                 # batch_lpips /= inputs.size(0)\n",
    "#                 with torch.no_grad():\n",
    "#                   batch_lpips = lpips_loss(img1, img2).mean()\n",
    "#                 # -----------------------------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#                 # Calculate the PSNR for each batch\n",
    "#                 batch_psnr = 0\n",
    "#                 for i in range(inputs.size(0)):\n",
    "#                     batch_psnr += calculate_psnr(output[i].detach(), targets[i])\n",
    "#                 batch_psnr /= inputs.size(0)\n",
    "\n",
    "#                 # ------------------------------------------------------\n",
    "\n",
    "#                 # total loss for all losses\n",
    "#                 total_loss += loss.item() # total loss for L1\n",
    "#                 total_psnr += batch_psnr # total loss for psnr\n",
    "#                 total_ssim += ssim_L.item() # total loss for ssim\n",
    "#                 total_lpips += batch_lpips # total loss for lpips\n",
    "#                 # -----------------------------------------------------\n",
    "\n",
    "#                 print(f' batch_num: {batch_num} | loss: {loss.item():.5f} | psnr_btch: {batch_psnr:.5f} | ssim: {ssim_L.item():.5f}| lpips: {batch_lpips:.5f}')\n",
    "#                 batch_num += 1\n",
    "#                 # Update the best enhanced images\n",
    "#                 if batch_psnr > 10 and batch_psnr < 16:\n",
    "#                     low_output.append(output.to('cpu'))\n",
    "#                     low_inputs.append(inputs.to('cpu'))\n",
    "#                     low_targets.append(targets.to('cpu'))\n",
    "#                 elif batch_psnr > 29:\n",
    "#                     top_output.append(output.to('cpu'))\n",
    "#                     top_inputs.append(inputs.to('cpu'))\n",
    "#                     top_targets.append(targets.to('cpu'))\n",
    "#                 # -----------------------------------------------------------\n",
    "\n",
    "#                 # Update progress bar\n",
    "#                 # pbar.set_postfix({\"loss\": loss.item(), \"batch_psnr\": batch_psnr})\n",
    "#                 # pbar.update(1)\n",
    "\n",
    "#             # Log end of epoch details\n",
    "#             cur_loss = total_loss / len(train_loader)\n",
    "#             cur_psnr = total_psnr / len(train_loader)\n",
    "#             cur_ssim = total_ssim / len(train_loader)\n",
    "#             cur_lpips = total_lpips / len(train_loader)\n",
    "#             # val_loss, e_out, total_psnr = evaluate(model, val_loader)\n",
    "#             print(f'End of Epoch {epoch}  | LOSS {cur_loss:5.4f} | PSNR {cur_psnr:.4f} | SSIM: {ssim_L.item():.5f} | LPIPS: {batch_lpips:.5f}')\n",
    "#             tev_out, tev_inputs, tev_targets, output_l, targets_l, inputs_l, avg_loss,  avg_psnr, avg_ssim, avg_lpips = evaluate(model, val_loader)\n",
    "#             print(f'\\n End of valadation {epoch} | LOSS: {avg_loss:5.4f} | PSNR: {avg_psnr:.4f} | SSIM: {avg_ssim:.5f} | LPIPS: {avg_lpips:.5f}')\n",
    "\n",
    "#     print('Training completed.')\n",
    "#     return top_output, top_inputs, top_targets, low_output, low_inputs, low_targets, tev_out, tev_inputs, tev_targets, output_l, targets_l, inputs_l"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eaQ_clr1GKRr"
   },
   "source": [
    "## MD_Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "_g3jWYSLOVHD"
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision.utils import save_image\n",
    "# from tqdm import tqdm\n",
    "# from skimage.metrics import structural_similarity as ssim\n",
    "# from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "# from lpips import LPIPS\n",
    "# import torchvision.models as models\n",
    "\n",
    "# # Constants\n",
    "# LEARNING_RATE = 0.001\n",
    "# NUM_EPOCHS = 100\n",
    "# SAVE_INTERVAL = 5\n",
    "# START_EPOCH = 0  # Start from epoch 80\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# #G1.load_state_dict(torch.load(r\"C:\\Users\\nasef\\Desktop\\SBSR\\Models\\SBSR32\\SBSR30.pth\"))\n",
    "\n",
    "\n",
    "# optimizer_G = optim.Adam(G1.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999))\n",
    "\n",
    "# # Add the cosine annealing scheduler\n",
    "# scheduler_G = optim.lr_scheduler.CosineAnnealingLR(optimizer_G, T_max=NUM_EPOCHS - START_EPOCH, eta_min=0.000001)\n",
    "\n",
    "# # Loss functions\n",
    "# l1_loss = nn.L1Loss()\n",
    "# mse_loss = nn.MSELoss()\n",
    "# lpips_loss_fn = LPIPS(net='alex').to(DEVICE)  # Using AlexNet for LPIPS loss\n",
    "\n",
    "# # Define the VGG loss\n",
    "# class VGGLoss(nn.Module):\n",
    "#     def __init__(self, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "#         super(VGGLoss, self).__init__()\n",
    "#         vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features.to(device).eval()  # Move VGG layers to device and set to eval mode\n",
    "#         self.vgg_layers = nn.Sequential(*list(vgg)[:36]).to(device)\n",
    "#         self.device = device\n",
    "#         for param in self.vgg_layers.parameters():\n",
    "#             param.requires_grad = False\n",
    "\n",
    "#     def forward(self, x, y):\n",
    "#         x = x.to(self.device)\n",
    "#         y = y.to(self.device)\n",
    "#         x_vgg = self.vgg_layers(x)\n",
    "#         y_vgg = self.vgg_layers(y)\n",
    "#         loss = nn.functional.l1_loss(x_vgg, y_vgg)\n",
    "#         return loss\n",
    "\n",
    "# # Define Focal Frequency Loss\n",
    "# class FocalFrequencyLoss(nn.Module):\n",
    "#     def __init__(self, alpha=1.0, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "#         super(FocalFrequencyLoss, self).__init__()\n",
    "#         self.alpha = alpha\n",
    "#         self.device = device\n",
    "\n",
    "#     def forward(self, input, target):\n",
    "#         input = input.to(self.device)\n",
    "#         target = target.to(self.device)\n",
    "#         input_fft = torch.fft.fft2(input)\n",
    "#         target_fft = torch.fft.fft2(target)\n",
    "#         diff = input_fft - target_fft\n",
    "#         abs_diff = torch.abs(diff)\n",
    "#         loss = torch.pow(abs_diff, self.alpha)\n",
    "#         return torch.mean(loss)\n",
    "\n",
    "# # Loss weights\n",
    "# lambda_vgg = 0.01\n",
    "# lambda_ff = 0.1\n",
    "\n",
    "# # Combined loss\n",
    "# class CombinedLoss(nn.Module):\n",
    "#     def __init__(self, lambda_vgg, lambda_ff, device=DEVICE):\n",
    "#         super(CombinedLoss, self).__init__()\n",
    "#         self.lambda_vgg = lambda_vgg\n",
    "#         self.lambda_ff = lambda_ff\n",
    "#         self.vgg_loss = VGGLoss(device=device)\n",
    "#         self.ff_loss = FocalFrequencyLoss(device=device)\n",
    "\n",
    "#     def forward(self, input, target):\n",
    "#         l1 = l1_loss(input, target)\n",
    "#         vgg = self.vgg_loss(input, target)\n",
    "#         ff = self.ff_loss(input, target)\n",
    "#         return l1 + self.lambda_vgg * vgg + self.lambda_ff * ff\n",
    "\n",
    "# # Example usage\n",
    "# combined_loss = CombinedLoss(lambda_vgg=lambda_vgg, lambda_ff=lambda_ff, device=DEVICE)\n",
    "\n",
    "# # Directories for checkpoints and samples\n",
    "# checkpoint_dir = r'C:\\Users\\Admin\\Desktop\\AI_RAMiT\\RAMiT'\n",
    "# sample_dir = r'C:\\Users\\Admin\\Desktop\\AI_RAMiT\\RAMiT'\n",
    "# os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "# os.makedirs(sample_dir, exist_ok=True)\n",
    "# save_interval = SAVE_INTERVAL  # Save checkpoint and samples every 5 epochs\n",
    "\n",
    "# for epoch in range(START_EPOCH, NUM_EPOCHS):\n",
    "#     # Training\n",
    "#     G1.train()\n",
    "#     total_g_loss = 0.0\n",
    "#     total_l1_loss = 0.0\n",
    "#     with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{NUM_EPOCHS}', unit='batch') as pbar:\n",
    "#         for i, (real_shadow, real_free) in enumerate(train_loader):\n",
    "#             real_shadow = real_shadow.to(DEVICE)\n",
    "#             real_free = real_free.to(DEVICE)\n",
    "#             optimizer_G.zero_grad()\n",
    "\n",
    "#             pred_free = G1(real_shadow)\n",
    "#             # Generator loss\n",
    "#             g1_loss = l1_loss(pred_free, real_free)\n",
    "#             g_loss = g1_loss\n",
    "#             g_loss.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(G1.parameters(), max_norm=1.0)\n",
    "#             optimizer_G.step()\n",
    "\n",
    "#             # Accumulate losses\n",
    "#             total_g_loss += g_loss.item()\n",
    "#             total_l1_loss += g1_loss.item()\n",
    "\n",
    "#             pbar.set_postfix(G_loss=g_loss.item(), G1_loss=g1_loss.item())\n",
    "#             pbar.update(1)\n",
    "\n",
    "#     # Calculate average losses\n",
    "#     avg_g_loss = total_g_loss / len(train_loader)\n",
    "#     avg_l1_loss = total_l1_loss / len(train_loader)\n",
    "\n",
    "#     print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}] - Average G Loss: {avg_g_loss:.4f}\")\n",
    "#     scheduler_G.step()\n",
    "\n",
    "#     m = 0\n",
    "#     # Validation and Checkpoints\n",
    "#     if (epoch + 1+70) % save_interval == 0:\n",
    "#         torch.cuda.empty_cache()\n",
    "#         G1.eval()\n",
    "#         with torch.no_grad():\n",
    "#             val_g_loss = 0.0\n",
    "#             val_l1_loss = 0.0\n",
    "#             val_ssim_loss = 0.0\n",
    "#             val_lpips_loss = 0.0\n",
    "#             val_psnr_mask = 0.0\n",
    "\n",
    "#             with tqdm(total=len(val_loader), desc=f'Validation Epoch {epoch + 1}/{NUM_EPOCHS}', unit='batch') as pbar_val:\n",
    "#                 for i, (real_shadow, real_free) in enumerate(val_loader):\n",
    "#                     real_shadow = real_shadow.to(DEVICE)\n",
    "#                     real_free = real_free.to(DEVICE)\n",
    "\n",
    "#                     pred_free = G1(real_shadow)\n",
    "#                     # Generator loss\n",
    "#                     g1_loss = l1_loss(pred_free, real_free)\n",
    "#                     g_loss = g1_loss\n",
    "\n",
    "#                     val_g_loss += g_loss.item()\n",
    "#                     val_l1_loss += g1_loss.item()\n",
    "\n",
    "#                     # Calculate SSIM and LPIPS\n",
    "#                     pred_free_np = pred_free.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "#                     real_free_np = real_free.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "#                     #ssim_val = ssim(pred_free_np, real_free_np, data_range=1, channel_axis=-1)\n",
    "#                     lpips_val = lpips_loss_fn(pred_free, real_free).mean().item()  # Average LPIPS over batch\n",
    "#                     #val_ssim_loss += ssim_val\n",
    "#                     val_lpips_loss += lpips_val\n",
    "\n",
    "#                     # Calculate PSNR for final shadow-free images\n",
    "#                     for j in range(real_free_np.shape[0]):\n",
    "#                         val_psnr_mask += psnr(real_free_np[j], pred_free_np[j], data_range=1)\n",
    "#                         val_ssim_loss += ssim(real_free_np[j], pred_free_np[j], data_range=1, channel_axis=-1)\n",
    "\n",
    "\n",
    "#                     pbar_val.update(1)\n",
    "#                     save_image(pred_free, os.path.join(sample_dir, f'pred_free_epoch_{epoch + 1}_{m}.png'))\n",
    "#                     m += 1\n",
    "\n",
    "#             # Averaging over the entire validation set\n",
    "#             val_g_loss /= len(val_loader)\n",
    "#             val_l1_loss /= len(val_loader)\n",
    "#             val_ssim_loss /= len(val_loader.dataset)\n",
    "#             val_lpips_loss /= len(val_loader)\n",
    "#             val_psnr_mask /= len(val_loader.dataset)\n",
    "\n",
    "#             print(f\"Validation after Epoch [{epoch + 1}/{NUM_EPOCHS}] \"\n",
    "#                   f\"Val G Loss: {val_g_loss:.4f}, Val L1 Loss: {val_l1_loss:.4f}, \"\n",
    "#                   f\"PSNR_MASK: {val_psnr_mask:.4f}, SSIM: {val_ssim_loss:.4f}, LPIPS: {val_lpips_loss:.4f}\")\n",
    "\n",
    "#             # Save model checkpoints\n",
    "#             torch.save(G1.state_dict(), os.path.join(checkpoint_dir, f'SBSR{epoch + 1}.pth'))\n",
    "\n",
    "# print(\"Training completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RMAiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/500:   0%|          | 0/333 [00:00<?, ?batch/s]c:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "Epoch 1/500:   0%|          | 1/333 [00:01<06:27,  1.17s/batch, G1_loss=0.183, G_loss=0.183]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 103\u001b[0m\n\u001b[0;32m    101\u001b[0m total_l1_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m tqdm(total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(train_loader), desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m, unit\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m pbar:\n\u001b[1;32m--> 103\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, (real_shadow, real_free) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(train_loader):\n\u001b[0;32m    104\u001b[0m         real_shadow \u001b[38;5;241m=\u001b[39m real_shadow\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m    105\u001b[0m         real_free \u001b[38;5;241m=\u001b[39m real_free\u001b[38;5;241m.\u001b[39mto(DEVICE)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[5], line 62\u001b[0m, in \u001b[0;36mShadowRemovalDataset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     59\u001b[0m     gt_image, lq_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrandom_crop(gt_image, lq_image)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n\u001b[1;32m---> 62\u001b[0m     gt_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     63\u001b[0m     lq_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(lq_image)\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m lq_image, gt_image\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torchvision\\transforms\\transforms.py:137\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[1;34m(self, pic)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, pic):\n\u001b[0;32m    130\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;124;03m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[38;5;124;03m        Tensor: Converted image.\u001b[39;00m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 137\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpic\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torchvision\\transforms\\functional.py:176\u001b[0m, in \u001b[0;36mto_tensor\u001b[1;34m(pic)\u001b[0m\n\u001b[0;32m    174\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mpermute((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m))\u001b[38;5;241m.\u001b[39mcontiguous()\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(img, torch\u001b[38;5;241m.\u001b[39mByteTensor):\n\u001b[1;32m--> 176\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_float_dtype\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# from torch.utils.data import DataLoader\n",
    "# from torchvision.utils import save_image\n",
    "# from tqdm import tqdm\n",
    "# from skimage.metrics import structural_similarity as ssim\n",
    "# from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "# from lpips import LPIPS\n",
    "# import torchvision.models as models\n",
    "\n",
    "# # Constants\n",
    "# LEARNING_RATE = 0.001\n",
    "# NUM_EPOCHS = 500\n",
    "# SAVE_INTERVAL = 5\n",
    "# START_EPOCH = 0  # Start from epoch 80\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# #G1.load_state_dict(torch.load(r\"C:\\Users\\nasef\\Desktop\\SBSR\\Models\\SBSR32\\SBSR30.pth\"))\n",
    "\n",
    "\n",
    "# optimizer_G = optim.Adam(G1.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999))\n",
    "\n",
    "# # Add the cosine annealing scheduler\n",
    "# scheduler_G = optim.lr_scheduler.CosineAnnealingLR(optimizer_G, T_max=NUM_EPOCHS - START_EPOCH, eta_min=0.000001)\n",
    "\n",
    "# # Loss functions\n",
    "# l1_loss = nn.L1Loss()\n",
    "# mse_loss = nn.MSELoss()\n",
    "# lpips_loss_fn = LPIPS(net='alex').to(DEVICE)  # Using AlexNet for LPIPS loss\n",
    "\n",
    "# # Define the VGG loss\n",
    "# class VGGLoss(nn.Module):\n",
    "#     def __init__(self, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "#         super(VGGLoss, self).__init__()\n",
    "#         vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features.to(device).eval()  # Move VGG layers to device and set to eval mode\n",
    "#         self.vgg_layers = nn.Sequential(*list(vgg)[:36]).to(device)\n",
    "#         self.device = device\n",
    "#         for param in self.vgg_layers.parameters():\n",
    "#             param.requires_grad = False\n",
    "\n",
    "#     def forward(self, x, y):\n",
    "#         x = x.to(self.device)\n",
    "#         y = y.to(self.device)\n",
    "#         x_vgg = self.vgg_layers(x)\n",
    "#         y_vgg = self.vgg_layers(y)\n",
    "#         loss = nn.functional.l1_loss(x_vgg, y_vgg)\n",
    "#         return loss\n",
    "\n",
    "# # Define Focal Frequency Loss\n",
    "# class FocalFrequencyLoss(nn.Module):\n",
    "#     def __init__(self, alpha=1.0, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "#         super(FocalFrequencyLoss, self).__init__()\n",
    "#         self.alpha = alpha\n",
    "#         self.device = device\n",
    "\n",
    "#     def forward(self, input, target):\n",
    "#         input = input.to(self.device)\n",
    "#         target = target.to(self.device)\n",
    "#         input_fft = torch.fft.fft2(input)\n",
    "#         target_fft = torch.fft.fft2(target)\n",
    "#         diff = input_fft - target_fft\n",
    "#         abs_diff = torch.abs(diff)\n",
    "#         loss = torch.pow(abs_diff, self.alpha)\n",
    "#         return torch.mean(loss)\n",
    "\n",
    "# # Loss weights\n",
    "# lambda_vgg = 0.01\n",
    "# lambda_ff = 0.1\n",
    "\n",
    "# # Combined loss\n",
    "# class CombinedLoss(nn.Module):\n",
    "#     def __init__(self, lambda_vgg, lambda_ff, device=DEVICE):\n",
    "#         super(CombinedLoss, self).__init__()\n",
    "#         self.lambda_vgg = lambda_vgg\n",
    "#         self.lambda_ff = lambda_ff\n",
    "#         self.vgg_loss = VGGLoss(device=device)\n",
    "#         self.ff_loss = FocalFrequencyLoss(device=device)\n",
    "\n",
    "#     def forward(self, input, target):\n",
    "#         l1 = l1_loss(input, target)\n",
    "#         vgg = self.vgg_loss(input, target)\n",
    "#         ff = self.ff_loss(input, target)\n",
    "#         return l1 + self.lambda_vgg * vgg + self.lambda_ff * ff\n",
    "\n",
    "# # Example usage\n",
    "# combined_loss = CombinedLoss(lambda_vgg=lambda_vgg, lambda_ff=lambda_ff, device=DEVICE)\n",
    "\n",
    "# # Directories for checkpoints and samples\n",
    "# checkpoint_dir = r'C:\\Users\\Admin\\Desktop\\AI_RAMiT\\RAMiT'\n",
    "# sample_dir = r'C:\\Users\\Admin\\Desktop\\AI_RAMiT\\RAMiT'\n",
    "# os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "# os.makedirs(sample_dir, exist_ok=True)\n",
    "# save_interval = SAVE_INTERVAL  # Save checkpoint and samples every 5 epochs\n",
    "\n",
    "# for epoch in range(START_EPOCH, NUM_EPOCHS):\n",
    "#     # Training\n",
    "#     G1.train()\n",
    "#     total_g_loss = 0.0\n",
    "#     total_l1_loss = 0.0\n",
    "#     with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{NUM_EPOCHS}', unit='batch') as pbar:\n",
    "#         for i, (real_shadow, real_free) in enumerate(train_loader):\n",
    "#             real_shadow = real_shadow.to(DEVICE)\n",
    "#             real_free = real_free.to(DEVICE)\n",
    "#             optimizer_G.zero_grad()\n",
    "\n",
    "#             pred_free = G1(real_shadow)\n",
    "#             # Generator loss\n",
    "#             g1_loss = l1_loss(pred_free, real_free)\n",
    "#             g_loss = g1_loss\n",
    "#             g_loss.backward()\n",
    "#             torch.nn.utils.clip_grad_norm_(G1.parameters(), max_norm=1.0)\n",
    "#             optimizer_G.step()\n",
    "\n",
    "#             # Accumulate losses\n",
    "#             total_g_loss += g_loss.item()\n",
    "#             total_l1_loss += g1_loss.item()\n",
    "\n",
    "#             pbar.set_postfix(G_loss=g_loss.item(), G1_loss=g1_loss.item())\n",
    "#             pbar.update(1)\n",
    "\n",
    "#     # Calculate average losses\n",
    "#     avg_g_loss = total_g_loss / len(train_loader)\n",
    "#     avg_l1_loss = total_l1_loss / len(train_loader)\n",
    "\n",
    "#     print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}] - Average G Loss: {avg_g_loss:.4f}\")\n",
    "#     scheduler_G.step()\n",
    "\n",
    "#     m = 0\n",
    "#     # Validation and Checkpoints\n",
    "#     if (epoch + 1+70) % save_interval == 0:\n",
    "#         torch.cuda.empty_cache()\n",
    "#         G1.eval()\n",
    "#         with torch.no_grad():\n",
    "#             val_g_loss = 0.0\n",
    "#             val_l1_loss = 0.0\n",
    "#             val_ssim_loss = 0.0\n",
    "#             val_lpips_loss = 0.0\n",
    "#             val_psnr_mask = 0.0\n",
    "\n",
    "#             with tqdm(total=len(val_loader), desc=f'Validation Epoch {epoch + 1}/{NUM_EPOCHS}', unit='batch') as pbar_val:\n",
    "#                 for i, (real_shadow, real_free) in enumerate(val_loader):\n",
    "#                     real_shadow = real_shadow.to(DEVICE)\n",
    "#                     real_free = real_free.to(DEVICE)\n",
    "\n",
    "#                     pred_free = G1(real_shadow)\n",
    "#                     # Generator loss\n",
    "#                     g1_loss = l1_loss(pred_free, real_free)\n",
    "#                     g_loss = g1_loss\n",
    "\n",
    "#                     val_g_loss += g_loss.item()\n",
    "#                     val_l1_loss += g1_loss.item()\n",
    "\n",
    "#                     # Calculate SSIM and LPIPS\n",
    "#                     pred_free_np = pred_free.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "#                     real_free_np = real_free.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "#                     #ssim_val = ssim(pred_free_np, real_free_np, data_range=1, channel_axis=-1)\n",
    "#                     lpips_val = lpips_loss_fn(pred_free, real_free).mean().item()  # Average LPIPS over batch\n",
    "#                     #val_ssim_loss += ssim_val\n",
    "#                     val_lpips_loss += lpips_val\n",
    "\n",
    "#                     # Calculate PSNR for final shadow-free images\n",
    "#                     for j in range(real_free_np.shape[0]):\n",
    "#                         val_psnr_mask += psnr(real_free_np[j], pred_free_np[j], data_range=1)\n",
    "#                         val_ssim_loss += ssim(real_free_np[j], pred_free_np[j], data_range=1, channel_axis=-1)\n",
    "\n",
    "\n",
    "#                     pbar_val.update(1)\n",
    "#                     save_image(pred_free, os.path.join(sample_dir, f'pred_free_epoch_{epoch + 1}_{m}.png'))\n",
    "#                     m += 1\n",
    "\n",
    "#             # Averaging over the entire validation set\n",
    "#             val_g_loss /= len(val_loader)\n",
    "#             val_l1_loss /= len(val_loader)\n",
    "#             val_ssim_loss /= len(val_loader.dataset)\n",
    "#             val_lpips_loss /= len(val_loader)\n",
    "#             val_psnr_mask /= len(val_loader.dataset)\n",
    "\n",
    "#             print(f\"Validation after Epoch [{epoch + 1}/{NUM_EPOCHS}] \"\n",
    "#                   f\"Val G Loss: {val_g_loss:.4f}, Val L1 Loss: {val_l1_loss:.4f}, \"\n",
    "#                   f\"PSNR_MASK: {val_psnr_mask:.4f}, SSIM: {val_ssim_loss:.4f}, LPIPS: {val_lpips_loss:.4f}\")\n",
    "\n",
    "#             # Save model checkpoints\n",
    "#             torch.save(G1.state_dict(), os.path.join(checkpoint_dir, f'SBSR{epoch + 1}.pth'))\n",
    "\n",
    "# print(\"Training completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FFAT_RAMiT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=AlexNet_Weights.IMAGENET1K_V1`. You can also use `weights=AlexNet_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: c:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\lpips\\lpips.py:107: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  self.load_state_dict(torch.load(model_path, map_location='cpu'), strict=False)\n",
      "Epoch 1/500:   0%|          | 0/665 [00:00<?, ?batch/s]c:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\cuda\\nccl.py:16: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn(\"PyTorch is not compiled with NCCL support\")\n",
      "Epoch 1/500: 100%|██████████| 665/665 [07:05<00:00,  1.56batch/s, G1_loss=0.0316, G_loss=0.0316]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500] - Average G Loss: 0.0663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0305, G_loss=0.0305]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/500] - Average G Loss: 0.0517\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.049, G_loss=0.049]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/500] - Average G Loss: 0.0500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.098, G_loss=0.098]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/500] - Average G Loss: 0.0473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.103, G_loss=0.103]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/500] - Average G Loss: 0.0469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 5/500: 100%|██████████| 68/68 [02:06<00:00,  1.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [5/500] Val G Loss: 0.0403, Val L1 Loss: 0.0403, PSNR_MASK: 25.3361, SSIM: 0.8982, LPIPS: 0.0759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0156, G_loss=0.0156]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/500] - Average G Loss: 0.0477\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0448, G_loss=0.0448]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/500] - Average G Loss: 0.0468\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0633, G_loss=0.0633]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/500] - Average G Loss: 0.0490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0332, G_loss=0.0332]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/500] - Average G Loss: 0.0461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0386, G_loss=0.0386]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/500] - Average G Loss: 0.0453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 10/500: 100%|██████████| 68/68 [02:03<00:00,  1.81s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [10/500] Val G Loss: 0.0405, Val L1 Loss: 0.0405, PSNR_MASK: 25.7090, SSIM: 0.9022, LPIPS: 0.0661\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/500: 100%|██████████| 665/665 [07:21<00:00,  1.51batch/s, G1_loss=0.0417, G_loss=0.0417]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/500] - Average G Loss: 0.0450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0376, G_loss=0.0376]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/500] - Average G Loss: 0.0437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0353, G_loss=0.0353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/500] - Average G Loss: 0.0442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0576, G_loss=0.0576]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/500] - Average G Loss: 0.0430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0315, G_loss=0.0315]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/500] - Average G Loss: 0.0419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 15/500: 100%|██████████| 68/68 [02:42<00:00,  2.39s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [15/500] Val G Loss: 0.0390, Val L1 Loss: 0.0390, PSNR_MASK: 25.5072, SSIM: 0.9015, LPIPS: 0.0680\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0326, G_loss=0.0326]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/500] - Average G Loss: 0.0400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0333, G_loss=0.0333]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/500] - Average G Loss: 0.0392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0639, G_loss=0.0639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/500] - Average G Loss: 0.0383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0534, G_loss=0.0534]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/500] - Average G Loss: 0.0373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.016, G_loss=0.016]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/500] - Average G Loss: 0.0362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 20/500: 100%|██████████| 68/68 [02:44<00:00,  2.42s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [20/500] Val G Loss: 0.0349, Val L1 Loss: 0.0349, PSNR_MASK: 26.7092, SSIM: 0.9063, LPIPS: 0.0617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0369, G_loss=0.0369]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/500] - Average G Loss: 0.0365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0168, G_loss=0.0168]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/500] - Average G Loss: 0.0359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.044, G_loss=0.044]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/500] - Average G Loss: 0.0358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/500: 100%|██████████| 665/665 [07:23<00:00,  1.50batch/s, G1_loss=0.0399, G_loss=0.0399]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/500] - Average G Loss: 0.0351\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0318, G_loss=0.0318]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/500] - Average G Loss: 0.0353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 25/500: 100%|██████████| 68/68 [02:43<00:00,  2.40s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [25/500] Val G Loss: 0.0329, Val L1 Loss: 0.0329, PSNR_MASK: 27.0414, SSIM: 0.9077, LPIPS: 0.0587\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0193, G_loss=0.0193]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [26/500] - Average G Loss: 0.0330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0414, G_loss=0.0414]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [27/500] - Average G Loss: 0.0323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/500: 100%|██████████| 665/665 [07:22<00:00,  1.50batch/s, G1_loss=0.0353, G_loss=0.0353]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [28/500] - Average G Loss: 0.0320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0437, G_loss=0.0437]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [29/500] - Average G Loss: 0.0318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/500: 100%|██████████| 665/665 [07:12<00:00,  1.54batch/s, G1_loss=0.0268, G_loss=0.0268]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [30/500] - Average G Loss: 0.0329\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 30/500: 100%|██████████| 68/68 [02:16<00:00,  2.00s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [30/500] Val G Loss: 0.0390, Val L1 Loss: 0.0390, PSNR_MASK: 26.7239, SSIM: 0.9045, LPIPS: 0.0614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0211, G_loss=0.0211]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [31/500] - Average G Loss: 0.0319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0282, G_loss=0.0282]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [32/500] - Average G Loss: 0.0311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0314, G_loss=0.0314]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [33/500] - Average G Loss: 0.0300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0459, G_loss=0.0459] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [34/500] - Average G Loss: 0.0310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0647, G_loss=0.0647]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [35/500] - Average G Loss: 0.0311\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 35/500: 100%|██████████| 68/68 [02:25<00:00,  2.14s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [35/500] Val G Loss: 0.0324, Val L1 Loss: 0.0324, PSNR_MASK: 27.4939, SSIM: 0.9107, LPIPS: 0.0544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/500: 100%|██████████| 665/665 [07:15<00:00,  1.53batch/s, G1_loss=0.0222, G_loss=0.0222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [36/500] - Average G Loss: 0.0302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/500: 100%|██████████| 665/665 [07:20<00:00,  1.51batch/s, G1_loss=0.0303, G_loss=0.0303]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [37/500] - Average G Loss: 0.0318\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0295, G_loss=0.0295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [38/500] - Average G Loss: 0.0301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0118, G_loss=0.0118]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [39/500] - Average G Loss: 0.0288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0243, G_loss=0.0243]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [40/500] - Average G Loss: 0.0304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 40/500: 100%|██████████| 68/68 [02:46<00:00,  2.45s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [40/500] Val G Loss: 0.0374, Val L1 Loss: 0.0374, PSNR_MASK: 26.7938, SSIM: 0.9097, LPIPS: 0.0550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0207, G_loss=0.0207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [41/500] - Average G Loss: 0.0296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0172, G_loss=0.0172]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [42/500] - Average G Loss: 0.0289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0687, G_loss=0.0687]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [43/500] - Average G Loss: 0.0288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.041, G_loss=0.041]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [44/500] - Average G Loss: 0.0296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0216, G_loss=0.0216]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [45/500] - Average G Loss: 0.0282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 45/500: 100%|██████████| 68/68 [02:47<00:00,  2.46s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [45/500] Val G Loss: 0.0317, Val L1 Loss: 0.0317, PSNR_MASK: 27.8583, SSIM: 0.9133, LPIPS: 0.0496\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0237, G_loss=0.0237]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [46/500] - Average G Loss: 0.0282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0209, G_loss=0.0209]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [47/500] - Average G Loss: 0.0275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0211, G_loss=0.0211]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [48/500] - Average G Loss: 0.0273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0228, G_loss=0.0228]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [49/500] - Average G Loss: 0.0288\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0264, G_loss=0.0264]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [50/500] - Average G Loss: 0.0274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 50/500: 100%|██████████| 68/68 [02:45<00:00,  2.43s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [50/500] Val G Loss: 0.0313, Val L1 Loss: 0.0313, PSNR_MASK: 26.9664, SSIM: 0.9106, LPIPS: 0.0555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.025, G_loss=0.025]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [51/500] - Average G Loss: 0.0282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0343, G_loss=0.0343]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [52/500] - Average G Loss: 0.0270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 53/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0308, G_loss=0.0308]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [53/500] - Average G Loss: 0.0264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 54/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0308, G_loss=0.0308]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [54/500] - Average G Loss: 0.0264\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 55/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0216, G_loss=0.0216]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [55/500] - Average G Loss: 0.0263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 55/500: 100%|██████████| 68/68 [02:46<00:00,  2.45s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [55/500] Val G Loss: 0.0305, Val L1 Loss: 0.0305, PSNR_MASK: 27.7958, SSIM: 0.9149, LPIPS: 0.0485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 56/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0217, G_loss=0.0217]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [56/500] - Average G Loss: 0.0262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 57/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0221, G_loss=0.0221]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [57/500] - Average G Loss: 0.0255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 58/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0193, G_loss=0.0193]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [58/500] - Average G Loss: 0.0256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 59/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.028, G_loss=0.028]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [59/500] - Average G Loss: 0.0265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 60/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0318, G_loss=0.0318]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [60/500] - Average G Loss: 0.0281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 60/500: 100%|██████████| 68/68 [02:47<00:00,  2.46s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [60/500] Val G Loss: 0.0289, Val L1 Loss: 0.0289, PSNR_MASK: 28.4671, SSIM: 0.9156, LPIPS: 0.0454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 61/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0239, G_loss=0.0239]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [61/500] - Average G Loss: 0.0256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 62/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.012, G_loss=0.012]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [62/500] - Average G Loss: 0.0263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 63/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0286, G_loss=0.0286]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [63/500] - Average G Loss: 0.0256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 64/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0256, G_loss=0.0256]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [64/500] - Average G Loss: 0.0259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 65/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0293, G_loss=0.0293]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [65/500] - Average G Loss: 0.0245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 65/500: 100%|██████████| 68/68 [02:45<00:00,  2.43s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [65/500] Val G Loss: 0.0289, Val L1 Loss: 0.0289, PSNR_MASK: 28.2025, SSIM: 0.9153, LPIPS: 0.0454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 66/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0209, G_loss=0.0209]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [66/500] - Average G Loss: 0.0252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 67/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0161, G_loss=0.0161]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [67/500] - Average G Loss: 0.0245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 68/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0215, G_loss=0.0215]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [68/500] - Average G Loss: 0.0258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 69/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0132, G_loss=0.0132]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [69/500] - Average G Loss: 0.0253\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 70/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0189, G_loss=0.0189]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [70/500] - Average G Loss: 0.0241\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 70/500: 100%|██████████| 68/68 [02:46<00:00,  2.45s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [70/500] Val G Loss: 0.0287, Val L1 Loss: 0.0287, PSNR_MASK: 28.5729, SSIM: 0.9173, LPIPS: 0.0419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 71/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0223, G_loss=0.0223]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [71/500] - Average G Loss: 0.0236\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 72/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0111, G_loss=0.0111]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [72/500] - Average G Loss: 0.0234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 73/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0298, G_loss=0.0298]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [73/500] - Average G Loss: 0.0243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 74/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0385, G_loss=0.0385]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [74/500] - Average G Loss: 0.0240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 75/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0357, G_loss=0.0357]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [75/500] - Average G Loss: 0.0231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 75/500: 100%|██████████| 68/68 [02:47<00:00,  2.47s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [75/500] Val G Loss: 0.0313, Val L1 Loss: 0.0313, PSNR_MASK: 27.3557, SSIM: 0.9133, LPIPS: 0.0488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 76/500: 100%|██████████| 665/665 [07:40<00:00,  1.44batch/s, G1_loss=0.0329, G_loss=0.0329]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [76/500] - Average G Loss: 0.0234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 77/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0221, G_loss=0.0221]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [77/500] - Average G Loss: 0.0243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 78/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0194, G_loss=0.0194]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [78/500] - Average G Loss: 0.0221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 79/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0202, G_loss=0.0202]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [79/500] - Average G Loss: 0.0222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 80/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0342, G_loss=0.0342]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [80/500] - Average G Loss: 0.0223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 80/500: 100%|██████████| 68/68 [02:49<00:00,  2.50s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [80/500] Val G Loss: 0.0277, Val L1 Loss: 0.0277, PSNR_MASK: 28.8044, SSIM: 0.9165, LPIPS: 0.0429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 81/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0177, G_loss=0.0177]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [81/500] - Average G Loss: 0.0234\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 82/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0165, G_loss=0.0165]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [82/500] - Average G Loss: 0.0229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 83/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0205, G_loss=0.0205]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [83/500] - Average G Loss: 0.0225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 84/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0146, G_loss=0.0146]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [84/500] - Average G Loss: 0.0221\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 85/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0308, G_loss=0.0308]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [85/500] - Average G Loss: 0.0223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 85/500: 100%|██████████| 68/68 [02:45<00:00,  2.43s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [85/500] Val G Loss: 0.0305, Val L1 Loss: 0.0305, PSNR_MASK: 28.4372, SSIM: 0.9135, LPIPS: 0.0460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 86/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0187, G_loss=0.0187]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [86/500] - Average G Loss: 0.0217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 87/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0118, G_loss=0.0118]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [87/500] - Average G Loss: 0.0216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 88/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0151, G_loss=0.0151]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [88/500] - Average G Loss: 0.0227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 89/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0137, G_loss=0.0137]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [89/500] - Average G Loss: 0.0216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 90/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0209, G_loss=0.0209]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [90/500] - Average G Loss: 0.0223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 90/500: 100%|██████████| 68/68 [02:45<00:00,  2.44s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [90/500] Val G Loss: 0.0299, Val L1 Loss: 0.0299, PSNR_MASK: 28.0715, SSIM: 0.9125, LPIPS: 0.0462\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 91/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.021, G_loss=0.021]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [91/500] - Average G Loss: 0.0217\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 92/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0157, G_loss=0.0157]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [92/500] - Average G Loss: 0.0219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 93/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0239, G_loss=0.0239]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [93/500] - Average G Loss: 0.0209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 94/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0162, G_loss=0.0162]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [94/500] - Average G Loss: 0.0229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 95/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0158, G_loss=0.0158]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [95/500] - Average G Loss: 0.0215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 95/500: 100%|██████████| 68/68 [02:45<00:00,  2.43s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [95/500] Val G Loss: 0.0290, Val L1 Loss: 0.0290, PSNR_MASK: 28.6417, SSIM: 0.9161, LPIPS: 0.0421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 96/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0392, G_loss=0.0392]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [96/500] - Average G Loss: 0.0216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 97/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0218, G_loss=0.0218]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [97/500] - Average G Loss: 0.0220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 98/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0321, G_loss=0.0321]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [98/500] - Average G Loss: 0.0208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 99/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0238, G_loss=0.0238]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [99/500] - Average G Loss: 0.0206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 100/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0145, G_loss=0.0145]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/500] - Average G Loss: 0.0219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 100/500: 100%|██████████| 68/68 [02:45<00:00,  2.44s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [100/500] Val G Loss: 0.0279, Val L1 Loss: 0.0279, PSNR_MASK: 28.5613, SSIM: 0.9169, LPIPS: 0.0432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 101/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0166, G_loss=0.0166]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [101/500] - Average G Loss: 0.0203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 102/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0134, G_loss=0.0134]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [102/500] - Average G Loss: 0.0207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 103/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0172, G_loss=0.0172]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [103/500] - Average G Loss: 0.0206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 104/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0169, G_loss=0.0169]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [104/500] - Average G Loss: 0.0200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 105/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.011, G_loss=0.011]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [105/500] - Average G Loss: 0.0225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 105/500: 100%|██████████| 68/68 [02:45<00:00,  2.43s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [105/500] Val G Loss: 0.0282, Val L1 Loss: 0.0282, PSNR_MASK: 28.2986, SSIM: 0.9173, LPIPS: 0.0443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 106/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0299, G_loss=0.0299]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [106/500] - Average G Loss: 0.0208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 107/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0342, G_loss=0.0342]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [107/500] - Average G Loss: 0.0197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 108/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0116, G_loss=0.0116]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [108/500] - Average G Loss: 0.0196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 109/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0188, G_loss=0.0188]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [109/500] - Average G Loss: 0.0219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 110/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0175, G_loss=0.0175]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [110/500] - Average G Loss: 0.0203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 110/500: 100%|██████████| 68/68 [02:45<00:00,  2.44s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [110/500] Val G Loss: 0.0298, Val L1 Loss: 0.0298, PSNR_MASK: 28.3607, SSIM: 0.9162, LPIPS: 0.0420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 111/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0194, G_loss=0.0194]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [111/500] - Average G Loss: 0.0196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 112/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.023, G_loss=0.023]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [112/500] - Average G Loss: 0.0194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 113/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0206, G_loss=0.0206]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [113/500] - Average G Loss: 0.0196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 114/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0141, G_loss=0.0141]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [114/500] - Average G Loss: 0.0196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 115/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0198, G_loss=0.0198]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [115/500] - Average G Loss: 0.0210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 115/500: 100%|██████████| 68/68 [02:46<00:00,  2.45s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [115/500] Val G Loss: 0.0292, Val L1 Loss: 0.0292, PSNR_MASK: 28.5548, SSIM: 0.9152, LPIPS: 0.0451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 116/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0338, G_loss=0.0338]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [116/500] - Average G Loss: 0.0202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 117/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0151, G_loss=0.0151]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [117/500] - Average G Loss: 0.0193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 118/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0288, G_loss=0.0288]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [118/500] - Average G Loss: 0.0199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 119/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0121, G_loss=0.0121]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [119/500] - Average G Loss: 0.0191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 120/500: 100%|██████████| 665/665 [07:39<00:00,  1.45batch/s, G1_loss=0.0175, G_loss=0.0175]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [120/500] - Average G Loss: 0.0192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 120/500: 100%|██████████| 68/68 [02:55<00:00,  2.59s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [120/500] Val G Loss: 0.0270, Val L1 Loss: 0.0270, PSNR_MASK: 29.1714, SSIM: 0.9178, LPIPS: 0.0402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 121/500: 100%|██████████| 665/665 [07:26<00:00,  1.49batch/s, G1_loss=0.0389, G_loss=0.0389]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [121/500] - Average G Loss: 0.0191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 122/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0222, G_loss=0.0222]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [122/500] - Average G Loss: 0.0193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 123/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0117, G_loss=0.0117]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [123/500] - Average G Loss: 0.0191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 124/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0121, G_loss=0.0121]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [124/500] - Average G Loss: 0.0202\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 125/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0189, G_loss=0.0189]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [125/500] - Average G Loss: 0.0188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 125/500: 100%|██████████| 68/68 [02:47<00:00,  2.47s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [125/500] Val G Loss: 0.0298, Val L1 Loss: 0.0298, PSNR_MASK: 27.9690, SSIM: 0.9150, LPIPS: 0.0446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 126/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0182, G_loss=0.0182]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [126/500] - Average G Loss: 0.0190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 127/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0117, G_loss=0.0117]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [127/500] - Average G Loss: 0.0186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 128/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0203, G_loss=0.0203]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [128/500] - Average G Loss: 0.0190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 129/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0153, G_loss=0.0153]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [129/500] - Average G Loss: 0.0198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 130/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0156, G_loss=0.0156]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [130/500] - Average G Loss: 0.0186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 130/500: 100%|██████████| 68/68 [02:46<00:00,  2.45s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [130/500] Val G Loss: 0.0272, Val L1 Loss: 0.0272, PSNR_MASK: 29.1384, SSIM: 0.9164, LPIPS: 0.0410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 131/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0225, G_loss=0.0225]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [131/500] - Average G Loss: 0.0181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 132/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.02, G_loss=0.02]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [132/500] - Average G Loss: 0.0186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 133/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0128, G_loss=0.0128]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [133/500] - Average G Loss: 0.0186\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 134/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0232, G_loss=0.0232]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [134/500] - Average G Loss: 0.0198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 135/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0125, G_loss=0.0125]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [135/500] - Average G Loss: 0.0180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 135/500: 100%|██████████| 68/68 [02:47<00:00,  2.47s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [135/500] Val G Loss: 0.0265, Val L1 Loss: 0.0265, PSNR_MASK: 29.1879, SSIM: 0.9180, LPIPS: 0.0389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 136/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0105, G_loss=0.0105]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [136/500] - Average G Loss: 0.0199\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 137/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0134, G_loss=0.0134]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [137/500] - Average G Loss: 0.0182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 138/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0153, G_loss=0.0153]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [138/500] - Average G Loss: 0.0179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 139/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0196, G_loss=0.0196]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [139/500] - Average G Loss: 0.0178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 140/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0319, G_loss=0.0319]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [140/500] - Average G Loss: 0.0181\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 140/500: 100%|██████████| 68/68 [02:50<00:00,  2.50s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [140/500] Val G Loss: 0.0297, Val L1 Loss: 0.0297, PSNR_MASK: 28.2990, SSIM: 0.9124, LPIPS: 0.0461\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 141/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.011, G_loss=0.011]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [141/500] - Average G Loss: 0.0183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 142/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0191, G_loss=0.0191]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [142/500] - Average G Loss: 0.0185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 143/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0251, G_loss=0.0251]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [143/500] - Average G Loss: 0.0182\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 144/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0174, G_loss=0.0174]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [144/500] - Average G Loss: 0.0175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 145/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.00967, G_loss=0.00967]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [145/500] - Average G Loss: 0.0195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 145/500: 100%|██████████| 68/68 [02:47<00:00,  2.46s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [145/500] Val G Loss: 0.0297, Val L1 Loss: 0.0297, PSNR_MASK: 28.6804, SSIM: 0.9144, LPIPS: 0.0431\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 146/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0153, G_loss=0.0153]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [146/500] - Average G Loss: 0.0178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 147/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0217, G_loss=0.0217]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [147/500] - Average G Loss: 0.0178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 148/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0182, G_loss=0.0182]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [148/500] - Average G Loss: 0.0179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 149/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0196, G_loss=0.0196]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [149/500] - Average G Loss: 0.0175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 150/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.00929, G_loss=0.00929]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [150/500] - Average G Loss: 0.0175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 150/500: 100%|██████████| 68/68 [02:48<00:00,  2.48s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [150/500] Val G Loss: 0.0277, Val L1 Loss: 0.0277, PSNR_MASK: 28.9804, SSIM: 0.9151, LPIPS: 0.0422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 151/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0204, G_loss=0.0204]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [151/500] - Average G Loss: 0.0176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 152/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0129, G_loss=0.0129]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [152/500] - Average G Loss: 0.0174\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 153/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0144, G_loss=0.0144]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [153/500] - Average G Loss: 0.0173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 154/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0297, G_loss=0.0297]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [154/500] - Average G Loss: 0.0176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 155/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0201, G_loss=0.0201]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [155/500] - Average G Loss: 0.0177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 155/500: 100%|██████████| 68/68 [02:47<00:00,  2.47s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [155/500] Val G Loss: 0.0292, Val L1 Loss: 0.0292, PSNR_MASK: 28.7369, SSIM: 0.9143, LPIPS: 0.0430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 156/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.024, G_loss=0.024]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [156/500] - Average G Loss: 0.0172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 157/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.013, G_loss=0.013]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [157/500] - Average G Loss: 0.0178\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 158/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0188, G_loss=0.0188]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [158/500] - Average G Loss: 0.0171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 159/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0149, G_loss=0.0149]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [159/500] - Average G Loss: 0.0173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 160/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0112, G_loss=0.0112]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [160/500] - Average G Loss: 0.0170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 160/500: 100%|██████████| 68/68 [02:49<00:00,  2.49s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [160/500] Val G Loss: 0.0276, Val L1 Loss: 0.0276, PSNR_MASK: 29.0670, SSIM: 0.9137, LPIPS: 0.0435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 161/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0154, G_loss=0.0154]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [161/500] - Average G Loss: 0.0184\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 162/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0133, G_loss=0.0133]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [162/500] - Average G Loss: 0.0176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 163/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0199, G_loss=0.0199]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [163/500] - Average G Loss: 0.0167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 164/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.01, G_loss=0.01]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [164/500] - Average G Loss: 0.0170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 165/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0158, G_loss=0.0158]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [165/500] - Average G Loss: 0.0179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 165/500: 100%|██████████| 68/68 [02:48<00:00,  2.48s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [165/500] Val G Loss: 0.0293, Val L1 Loss: 0.0293, PSNR_MASK: 28.9100, SSIM: 0.9135, LPIPS: 0.0427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 166/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.016, G_loss=0.016]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [166/500] - Average G Loss: 0.0170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 167/500: 100%|██████████| 665/665 [07:37<00:00,  1.45batch/s, G1_loss=0.0224, G_loss=0.0224]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [167/500] - Average G Loss: 0.0169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 168/500: 100%|██████████| 665/665 [07:27<00:00,  1.49batch/s, G1_loss=0.0168, G_loss=0.0168]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [168/500] - Average G Loss: 0.0169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 169/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0188, G_loss=0.0188]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [169/500] - Average G Loss: 0.0170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 170/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.018, G_loss=0.018]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [170/500] - Average G Loss: 0.0183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 170/500: 100%|██████████| 68/68 [02:55<00:00,  2.59s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [170/500] Val G Loss: 0.0284, Val L1 Loss: 0.0284, PSNR_MASK: 29.0205, SSIM: 0.9157, LPIPS: 0.0419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 171/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.013, G_loss=0.013]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [171/500] - Average G Loss: 0.0173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 172/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0166, G_loss=0.0166]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [172/500] - Average G Loss: 0.0164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 173/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0228, G_loss=0.0228]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [173/500] - Average G Loss: 0.0164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 174/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0236, G_loss=0.0236]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [174/500] - Average G Loss: 0.0167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 175/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0218, G_loss=0.0218]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [175/500] - Average G Loss: 0.0166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 175/500: 100%|██████████| 68/68 [02:47<00:00,  2.46s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [175/500] Val G Loss: 0.0281, Val L1 Loss: 0.0281, PSNR_MASK: 29.0665, SSIM: 0.9156, LPIPS: 0.0404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 176/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0138, G_loss=0.0138]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [176/500] - Average G Loss: 0.0171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 177/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0123, G_loss=0.0123]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [177/500] - Average G Loss: 0.0164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 178/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.00957, G_loss=0.00957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [178/500] - Average G Loss: 0.0163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 179/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0197, G_loss=0.0197]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [179/500] - Average G Loss: 0.0165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 180/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0147, G_loss=0.0147]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [180/500] - Average G Loss: 0.0163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 180/500: 100%|██████████| 68/68 [02:47<00:00,  2.47s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [180/500] Val G Loss: 0.0262, Val L1 Loss: 0.0262, PSNR_MASK: 29.3221, SSIM: 0.9185, LPIPS: 0.0392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 181/500: 100%|██████████| 665/665 [07:36<00:00,  1.46batch/s, G1_loss=0.0216, G_loss=0.0216]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [181/500] - Average G Loss: 0.0163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 182/500: 100%|██████████| 665/665 [07:27<00:00,  1.49batch/s, G1_loss=0.00835, G_loss=0.00835]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [182/500] - Average G Loss: 0.0164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 183/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0155, G_loss=0.0155]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [183/500] - Average G Loss: 0.0173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 184/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0152, G_loss=0.0152]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [184/500] - Average G Loss: 0.0161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 185/500: 100%|██████████| 665/665 [07:24<00:00,  1.49batch/s, G1_loss=0.0165, G_loss=0.0165]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [185/500] - Average G Loss: 0.0162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 185/500: 100%|██████████| 68/68 [02:56<00:00,  2.60s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [185/500] Val G Loss: 0.0271, Val L1 Loss: 0.0271, PSNR_MASK: 29.1960, SSIM: 0.9142, LPIPS: 0.0421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 186/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0136, G_loss=0.0136]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [186/500] - Average G Loss: 0.0161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 187/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0115, G_loss=0.0115]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [187/500] - Average G Loss: 0.0162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 188/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0163, G_loss=0.0163]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [188/500] - Average G Loss: 0.0171\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 189/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0125, G_loss=0.0125]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [189/500] - Average G Loss: 0.0162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 190/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0158, G_loss=0.0158]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [190/500] - Average G Loss: 0.0160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 190/500: 100%|██████████| 68/68 [02:47<00:00,  2.47s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [190/500] Val G Loss: 0.0294, Val L1 Loss: 0.0294, PSNR_MASK: 28.6782, SSIM: 0.9114, LPIPS: 0.0447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 191/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0126, G_loss=0.0126]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [191/500] - Average G Loss: 0.0161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 192/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0186, G_loss=0.0186]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [192/500] - Average G Loss: 0.0159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 193/500: 100%|██████████| 665/665 [07:24<00:00,  1.50batch/s, G1_loss=0.0298, G_loss=0.0298]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [193/500] - Average G Loss: 0.0157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 194/500: 100%|██████████| 665/665 [07:25<00:00,  1.49batch/s, G1_loss=0.0145, G_loss=0.0145]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [194/500] - Average G Loss: 0.0160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 195/500: 100%|██████████| 665/665 [07:06<00:00,  1.56batch/s, G1_loss=0.00887, G_loss=0.00887]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [195/500] - Average G Loss: 0.0158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 195/500: 100%|██████████| 68/68 [02:06<00:00,  1.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [195/500] Val G Loss: 0.0258, Val L1 Loss: 0.0258, PSNR_MASK: 29.3138, SSIM: 0.9171, LPIPS: 0.0403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 196/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0149, G_loss=0.0149]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [196/500] - Average G Loss: 0.0160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 197/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0164, G_loss=0.0164]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [197/500] - Average G Loss: 0.0168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 198/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0139, G_loss=0.0139]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [198/500] - Average G Loss: 0.0158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 199/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.01, G_loss=0.01]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [199/500] - Average G Loss: 0.0159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 200/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0221, G_loss=0.0221]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [200/500] - Average G Loss: 0.0154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 200/500: 100%|██████████| 68/68 [02:06<00:00,  1.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [200/500] Val G Loss: 0.0270, Val L1 Loss: 0.0270, PSNR_MASK: 29.2280, SSIM: 0.9111, LPIPS: 0.0435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 201/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0202, G_loss=0.0202]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [201/500] - Average G Loss: 0.0157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 202/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.022, G_loss=0.022]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [202/500] - Average G Loss: 0.0157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 203/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0394, G_loss=0.0394]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [203/500] - Average G Loss: 0.0157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 204/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0214, G_loss=0.0214]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [204/500] - Average G Loss: 0.0156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 205/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0171, G_loss=0.0171]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [205/500] - Average G Loss: 0.0158\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 205/500: 100%|██████████| 68/68 [02:06<00:00,  1.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [205/500] Val G Loss: 0.0270, Val L1 Loss: 0.0270, PSNR_MASK: 29.2179, SSIM: 0.9143, LPIPS: 0.0409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 206/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0415, G_loss=0.0415]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [206/500] - Average G Loss: 0.0169\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 207/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.017, G_loss=0.017]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [207/500] - Average G Loss: 0.0154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 208/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.013, G_loss=0.013]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [208/500] - Average G Loss: 0.0153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 209/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0198, G_loss=0.0198]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [209/500] - Average G Loss: 0.0153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 210/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0103, G_loss=0.0103]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [210/500] - Average G Loss: 0.0157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 210/500: 100%|██████████| 68/68 [02:06<00:00,  1.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [210/500] Val G Loss: 0.0274, Val L1 Loss: 0.0274, PSNR_MASK: 29.2465, SSIM: 0.9158, LPIPS: 0.0399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 211/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.019, G_loss=0.019]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [211/500] - Average G Loss: 0.0152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 212/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.00971, G_loss=0.00971]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [212/500] - Average G Loss: 0.0152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 213/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.00992, G_loss=0.00992]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [213/500] - Average G Loss: 0.0153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 214/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0701, G_loss=0.0701]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [214/500] - Average G Loss: 0.0154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 215/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0214, G_loss=0.0214]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [215/500] - Average G Loss: 0.0153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 215/500: 100%|██████████| 68/68 [02:06<00:00,  1.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [215/500] Val G Loss: 0.0275, Val L1 Loss: 0.0275, PSNR_MASK: 29.0877, SSIM: 0.9141, LPIPS: 0.0402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 216/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0145, G_loss=0.0145]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [216/500] - Average G Loss: 0.0151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 217/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.012, G_loss=0.012]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [217/500] - Average G Loss: 0.0153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 218/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0133, G_loss=0.0133]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [218/500] - Average G Loss: 0.0152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 219/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0097, G_loss=0.0097]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [219/500] - Average G Loss: 0.0151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 220/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0131, G_loss=0.0131]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [220/500] - Average G Loss: 0.0148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 220/500: 100%|██████████| 68/68 [02:06<00:00,  1.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [220/500] Val G Loss: 0.0279, Val L1 Loss: 0.0279, PSNR_MASK: 29.0515, SSIM: 0.9145, LPIPS: 0.0408\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 221/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0156, G_loss=0.0156]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [221/500] - Average G Loss: 0.0151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 222/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0119, G_loss=0.0119]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [222/500] - Average G Loss: 0.0151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 223/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0171, G_loss=0.0171]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [223/500] - Average G Loss: 0.0148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 224/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0205, G_loss=0.0205]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [224/500] - Average G Loss: 0.0151\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 225/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0159, G_loss=0.0159]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [225/500] - Average G Loss: 0.0152\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 225/500: 100%|██████████| 68/68 [02:06<00:00,  1.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [225/500] Val G Loss: 0.0263, Val L1 Loss: 0.0263, PSNR_MASK: 29.2310, SSIM: 0.9169, LPIPS: 0.0397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 226/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0401, G_loss=0.0401]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [226/500] - Average G Loss: 0.0150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 227/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0106, G_loss=0.0106]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [227/500] - Average G Loss: 0.0149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 228/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0119, G_loss=0.0119]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [228/500] - Average G Loss: 0.0150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 229/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0209, G_loss=0.0209]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [229/500] - Average G Loss: 0.0153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 230/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0165, G_loss=0.0165]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [230/500] - Average G Loss: 0.0147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 230/500: 100%|██████████| 68/68 [02:06<00:00,  1.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [230/500] Val G Loss: 0.0275, Val L1 Loss: 0.0275, PSNR_MASK: 29.2636, SSIM: 0.9139, LPIPS: 0.0409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 231/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0168, G_loss=0.0168]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [231/500] - Average G Loss: 0.0147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 232/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0145, G_loss=0.0145]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [232/500] - Average G Loss: 0.0147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 233/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0157, G_loss=0.0157]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [233/500] - Average G Loss: 0.0146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 234/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0159, G_loss=0.0159]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [234/500] - Average G Loss: 0.0149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 235/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.00877, G_loss=0.00877]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [235/500] - Average G Loss: 0.0147\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 235/500: 100%|██████████| 68/68 [02:07<00:00,  1.87s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [235/500] Val G Loss: 0.0287, Val L1 Loss: 0.0287, PSNR_MASK: 29.0339, SSIM: 0.9153, LPIPS: 0.0406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 236/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0151, G_loss=0.0151]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [236/500] - Average G Loss: 0.0149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 237/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0147, G_loss=0.0147]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [237/500] - Average G Loss: 0.0145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 238/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00725, G_loss=0.00725]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [238/500] - Average G Loss: 0.0146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 239/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0129, G_loss=0.0129]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [239/500] - Average G Loss: 0.0146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 240/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0149, G_loss=0.0149]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [240/500] - Average G Loss: 0.0146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 240/500: 100%|██████████| 68/68 [02:06<00:00,  1.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [240/500] Val G Loss: 0.0281, Val L1 Loss: 0.0281, PSNR_MASK: 29.1140, SSIM: 0.9147, LPIPS: 0.0409\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 241/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0217, G_loss=0.0217]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [241/500] - Average G Loss: 0.0145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 242/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0143, G_loss=0.0143]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [242/500] - Average G Loss: 0.0144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 243/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.01, G_loss=0.01]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [243/500] - Average G Loss: 0.0145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 244/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0128, G_loss=0.0128]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [244/500] - Average G Loss: 0.0143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 245/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0111, G_loss=0.0111]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [245/500] - Average G Loss: 0.0142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 245/500: 100%|██████████| 68/68 [02:06<00:00,  1.85s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [245/500] Val G Loss: 0.0268, Val L1 Loss: 0.0268, PSNR_MASK: 29.2740, SSIM: 0.9159, LPIPS: 0.0393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 246/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.00969, G_loss=0.00969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [246/500] - Average G Loss: 0.0143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 247/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0129, G_loss=0.0129]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [247/500] - Average G Loss: 0.0144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 248/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00957, G_loss=0.00957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [248/500] - Average G Loss: 0.0144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 249/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0214, G_loss=0.0214]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [249/500] - Average G Loss: 0.0142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 250/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.00981, G_loss=0.00981]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [250/500] - Average G Loss: 0.0142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 250/500: 100%|██████████| 68/68 [02:08<00:00,  1.88s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [250/500] Val G Loss: 0.0289, Val L1 Loss: 0.0289, PSNR_MASK: 28.9125, SSIM: 0.9130, LPIPS: 0.0413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 251/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0176, G_loss=0.0176]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [251/500] - Average G Loss: 0.0143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 252/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0158, G_loss=0.0158]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [252/500] - Average G Loss: 0.0142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 253/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0106, G_loss=0.0106]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [253/500] - Average G Loss: 0.0142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 254/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0105, G_loss=0.0105]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [254/500] - Average G Loss: 0.0141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 255/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0102, G_loss=0.0102]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [255/500] - Average G Loss: 0.0141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 255/500: 100%|██████████| 68/68 [02:07<00:00,  1.88s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [255/500] Val G Loss: 0.0315, Val L1 Loss: 0.0315, PSNR_MASK: 28.1567, SSIM: 0.9118, LPIPS: 0.0429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 256/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0215, G_loss=0.0215]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [256/500] - Average G Loss: 0.0142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 257/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0135, G_loss=0.0135]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [257/500] - Average G Loss: 0.0140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 258/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.00646, G_loss=0.00646]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [258/500] - Average G Loss: 0.0141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 259/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0118, G_loss=0.0118]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [259/500] - Average G Loss: 0.0141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 260/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0133, G_loss=0.0133]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [260/500] - Average G Loss: 0.0142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 260/500: 100%|██████████| 68/68 [02:08<00:00,  1.88s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [260/500] Val G Loss: 0.0281, Val L1 Loss: 0.0281, PSNR_MASK: 29.1784, SSIM: 0.9137, LPIPS: 0.0398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 261/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0186, G_loss=0.0186]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [261/500] - Average G Loss: 0.0140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 262/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0197, G_loss=0.0197]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [262/500] - Average G Loss: 0.0138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 263/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0167, G_loss=0.0167]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [263/500] - Average G Loss: 0.0139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 264/500: 100%|██████████| 665/665 [07:11<00:00,  1.54batch/s, G1_loss=0.0112, G_loss=0.0112]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [264/500] - Average G Loss: 0.0139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 265/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0135, G_loss=0.0135]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [265/500] - Average G Loss: 0.0139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 265/500: 100%|██████████| 68/68 [02:10<00:00,  1.92s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [265/500] Val G Loss: 0.0283, Val L1 Loss: 0.0283, PSNR_MASK: 29.1559, SSIM: 0.9142, LPIPS: 0.0406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 266/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0077, G_loss=0.0077]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [266/500] - Average G Loss: 0.0137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 267/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0107, G_loss=0.0107]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [267/500] - Average G Loss: 0.0140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 268/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0161, G_loss=0.0161]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [268/500] - Average G Loss: 0.0137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 269/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0118, G_loss=0.0118]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [269/500] - Average G Loss: 0.0138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 270/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0108, G_loss=0.0108]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [270/500] - Average G Loss: 0.0138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 270/500: 100%|██████████| 68/68 [02:06<00:00,  1.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [270/500] Val G Loss: 0.0268, Val L1 Loss: 0.0268, PSNR_MASK: 29.2610, SSIM: 0.9137, LPIPS: 0.0397\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 271/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.01, G_loss=0.01]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [271/500] - Average G Loss: 0.0137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 272/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0255, G_loss=0.0255]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [272/500] - Average G Loss: 0.0137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 273/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0119, G_loss=0.0119]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [273/500] - Average G Loss: 0.0137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 274/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0169, G_loss=0.0169]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [274/500] - Average G Loss: 0.0136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 275/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00993, G_loss=0.00993]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [275/500] - Average G Loss: 0.0138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 275/500: 100%|██████████| 68/68 [02:06<00:00,  1.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [275/500] Val G Loss: 0.0268, Val L1 Loss: 0.0268, PSNR_MASK: 29.2414, SSIM: 0.9142, LPIPS: 0.0398\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 276/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0176, G_loss=0.0176]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [276/500] - Average G Loss: 0.0137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 277/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00903, G_loss=0.00903]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [277/500] - Average G Loss: 0.0138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 278/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0133, G_loss=0.0133]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [278/500] - Average G Loss: 0.0135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 279/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0108, G_loss=0.0108]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [279/500] - Average G Loss: 0.0135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 280/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0138, G_loss=0.0138]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [280/500] - Average G Loss: 0.0137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 280/500: 100%|██████████| 68/68 [02:06<00:00,  1.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [280/500] Val G Loss: 0.0275, Val L1 Loss: 0.0275, PSNR_MASK: 29.0505, SSIM: 0.9119, LPIPS: 0.0412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 281/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.00953, G_loss=0.00953]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [281/500] - Average G Loss: 0.0134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 282/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0121, G_loss=0.0121]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [282/500] - Average G Loss: 0.0135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 283/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0172, G_loss=0.0172]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [283/500] - Average G Loss: 0.0135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 284/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00989, G_loss=0.00989]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [284/500] - Average G Loss: 0.0134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 285/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00919, G_loss=0.00919]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [285/500] - Average G Loss: 0.0134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 285/500: 100%|██████████| 68/68 [02:06<00:00,  1.85s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [285/500] Val G Loss: 0.0280, Val L1 Loss: 0.0280, PSNR_MASK: 29.1070, SSIM: 0.9152, LPIPS: 0.0392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 286/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00926, G_loss=0.00926]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [286/500] - Average G Loss: 0.0134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 287/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00858, G_loss=0.00858]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [287/500] - Average G Loss: 0.0135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 288/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0117, G_loss=0.0117]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [288/500] - Average G Loss: 0.0134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 289/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0121, G_loss=0.0121]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [289/500] - Average G Loss: 0.0133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 290/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00913, G_loss=0.00913]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [290/500] - Average G Loss: 0.0133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 290/500: 100%|██████████| 68/68 [02:06<00:00,  1.85s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [290/500] Val G Loss: 0.0273, Val L1 Loss: 0.0273, PSNR_MASK: 29.1988, SSIM: 0.9150, LPIPS: 0.0392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 291/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0127, G_loss=0.0127]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [291/500] - Average G Loss: 0.0134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 292/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0208, G_loss=0.0208]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [292/500] - Average G Loss: 0.0133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 293/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0118, G_loss=0.0118]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [293/500] - Average G Loss: 0.0132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 294/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0148, G_loss=0.0148]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [294/500] - Average G Loss: 0.0132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 295/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0111, G_loss=0.0111]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [295/500] - Average G Loss: 0.0132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 295/500: 100%|██████████| 68/68 [02:05<00:00,  1.85s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [295/500] Val G Loss: 0.0269, Val L1 Loss: 0.0269, PSNR_MASK: 29.1975, SSIM: 0.9156, LPIPS: 0.0393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 296/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0123, G_loss=0.0123]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [296/500] - Average G Loss: 0.0131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 297/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0131, G_loss=0.0131]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [297/500] - Average G Loss: 0.0131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 298/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.009, G_loss=0.009]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [298/500] - Average G Loss: 0.0132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 299/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0152, G_loss=0.0152]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [299/500] - Average G Loss: 0.0132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 300/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0102, G_loss=0.0102]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/500] - Average G Loss: 0.0132\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 300/500: 100%|██████████| 68/68 [02:05<00:00,  1.85s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [300/500] Val G Loss: 0.0281, Val L1 Loss: 0.0281, PSNR_MASK: 29.0674, SSIM: 0.9159, LPIPS: 0.0393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 301/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0119, G_loss=0.0119]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [301/500] - Average G Loss: 0.0131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 302/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00977, G_loss=0.00977]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [302/500] - Average G Loss: 0.0133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 303/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0145, G_loss=0.0145]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [303/500] - Average G Loss: 0.0131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 304/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0229, G_loss=0.0229]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [304/500] - Average G Loss: 0.0131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 305/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0199, G_loss=0.0199]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [305/500] - Average G Loss: 0.0131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 305/500: 100%|██████████| 68/68 [02:05<00:00,  1.85s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [305/500] Val G Loss: 0.0274, Val L1 Loss: 0.0274, PSNR_MASK: 29.2541, SSIM: 0.9156, LPIPS: 0.0387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 306/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0148, G_loss=0.0148]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [306/500] - Average G Loss: 0.0131\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 307/500: 100%|██████████| 665/665 [07:10<00:00,  1.55batch/s, G1_loss=0.0114, G_loss=0.0114]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [307/500] - Average G Loss: 0.0130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 308/500: 100%|██████████| 665/665 [07:05<00:00,  1.56batch/s, G1_loss=0.00974, G_loss=0.00974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [308/500] - Average G Loss: 0.0129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 309/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0126, G_loss=0.0126]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [309/500] - Average G Loss: 0.0129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 310/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0157, G_loss=0.0157]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [310/500] - Average G Loss: 0.0130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 310/500: 100%|██████████| 68/68 [02:10<00:00,  1.92s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [310/500] Val G Loss: 0.0273, Val L1 Loss: 0.0273, PSNR_MASK: 29.1059, SSIM: 0.9131, LPIPS: 0.0402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 311/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0116, G_loss=0.0116]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [311/500] - Average G Loss: 0.0129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 312/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0059, G_loss=0.0059]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [312/500] - Average G Loss: 0.0129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 313/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0107, G_loss=0.0107]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [313/500] - Average G Loss: 0.0128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 314/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.015, G_loss=0.015]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [314/500] - Average G Loss: 0.0129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 315/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00877, G_loss=0.00877]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [315/500] - Average G Loss: 0.0129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 315/500: 100%|██████████| 68/68 [02:06<00:00,  1.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [315/500] Val G Loss: 0.0279, Val L1 Loss: 0.0279, PSNR_MASK: 29.0639, SSIM: 0.9116, LPIPS: 0.0401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 316/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0149, G_loss=0.0149]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [316/500] - Average G Loss: 0.0129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 317/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.00949, G_loss=0.00949]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [317/500] - Average G Loss: 0.0128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 318/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.012, G_loss=0.012]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [318/500] - Average G Loss: 0.0128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 319/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0188, G_loss=0.0188]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [319/500] - Average G Loss: 0.0128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 320/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0107, G_loss=0.0107]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [320/500] - Average G Loss: 0.0129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 320/500: 100%|██████████| 68/68 [02:06<00:00,  1.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [320/500] Val G Loss: 0.0273, Val L1 Loss: 0.0273, PSNR_MASK: 29.1850, SSIM: 0.9167, LPIPS: 0.0383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 321/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0113, G_loss=0.0113]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [321/500] - Average G Loss: 0.0127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 322/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0223, G_loss=0.0223]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [322/500] - Average G Loss: 0.0127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 323/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0115, G_loss=0.0115]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [323/500] - Average G Loss: 0.0127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 324/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0203, G_loss=0.0203]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [324/500] - Average G Loss: 0.0127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 325/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0151, G_loss=0.0151]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [325/500] - Average G Loss: 0.0127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 325/500: 100%|██████████| 68/68 [02:06<00:00,  1.85s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [325/500] Val G Loss: 0.0284, Val L1 Loss: 0.0284, PSNR_MASK: 29.0256, SSIM: 0.9149, LPIPS: 0.0391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 326/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.00759, G_loss=0.00759]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [326/500] - Average G Loss: 0.0126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 327/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00937, G_loss=0.00937]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [327/500] - Average G Loss: 0.0126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 328/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00964, G_loss=0.00964]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [328/500] - Average G Loss: 0.0127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 329/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0208, G_loss=0.0208]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [329/500] - Average G Loss: 0.0126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 330/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0146, G_loss=0.0146]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [330/500] - Average G Loss: 0.0126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 330/500: 100%|██████████| 68/68 [02:05<00:00,  1.85s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [330/500] Val G Loss: 0.0273, Val L1 Loss: 0.0273, PSNR_MASK: 29.1657, SSIM: 0.9168, LPIPS: 0.0382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 331/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0154, G_loss=0.0154]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [331/500] - Average G Loss: 0.0125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 332/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.022, G_loss=0.022]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [332/500] - Average G Loss: 0.0126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 333/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0126, G_loss=0.0126]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [333/500] - Average G Loss: 0.0125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 334/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0105, G_loss=0.0105]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [334/500] - Average G Loss: 0.0126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 335/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0108, G_loss=0.0108]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [335/500] - Average G Loss: 0.0125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 335/500: 100%|██████████| 68/68 [02:06<00:00,  1.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [335/500] Val G Loss: 0.0270, Val L1 Loss: 0.0270, PSNR_MASK: 29.1563, SSIM: 0.9151, LPIPS: 0.0391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 336/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00704, G_loss=0.00704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [336/500] - Average G Loss: 0.0125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 337/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0212, G_loss=0.0212]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [337/500] - Average G Loss: 0.0125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 338/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0126, G_loss=0.0126]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [338/500] - Average G Loss: 0.0125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 339/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00856, G_loss=0.00856]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [339/500] - Average G Loss: 0.0125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 340/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0259, G_loss=0.0259]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [340/500] - Average G Loss: 0.0125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 340/500: 100%|██████████| 68/68 [02:05<00:00,  1.85s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [340/500] Val G Loss: 0.0269, Val L1 Loss: 0.0269, PSNR_MASK: 29.2642, SSIM: 0.9155, LPIPS: 0.0384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 341/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.00708, G_loss=0.00708]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [341/500] - Average G Loss: 0.0124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 342/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0123, G_loss=0.0123]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [342/500] - Average G Loss: 0.0124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 343/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00829, G_loss=0.00829]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [343/500] - Average G Loss: 0.0125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 344/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.012, G_loss=0.012]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [344/500] - Average G Loss: 0.0124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 345/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.00531, G_loss=0.00531]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [345/500] - Average G Loss: 0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 345/500: 100%|██████████| 68/68 [02:06<00:00,  1.85s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [345/500] Val G Loss: 0.0278, Val L1 Loss: 0.0278, PSNR_MASK: 29.1366, SSIM: 0.9136, LPIPS: 0.0396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 346/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0234, G_loss=0.0234]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [346/500] - Average G Loss: 0.0124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 347/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0193, G_loss=0.0193]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [347/500] - Average G Loss: 0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 348/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0156, G_loss=0.0156]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [348/500] - Average G Loss: 0.0124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 349/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00777, G_loss=0.00777]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [349/500] - Average G Loss: 0.0124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 350/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0119, G_loss=0.0119]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [350/500] - Average G Loss: 0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 350/500: 100%|██████████| 68/68 [02:06<00:00,  1.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [350/500] Val G Loss: 0.0275, Val L1 Loss: 0.0275, PSNR_MASK: 29.1338, SSIM: 0.9149, LPIPS: 0.0389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 351/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0164, G_loss=0.0164]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [351/500] - Average G Loss: 0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 352/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0086, G_loss=0.0086]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [352/500] - Average G Loss: 0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 353/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.00782, G_loss=0.00782]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [353/500] - Average G Loss: 0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 354/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0118, G_loss=0.0118]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [354/500] - Average G Loss: 0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 355/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00959, G_loss=0.00959]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [355/500] - Average G Loss: 0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 355/500: 100%|██████████| 68/68 [02:06<00:00,  1.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [355/500] Val G Loss: 0.0277, Val L1 Loss: 0.0277, PSNR_MASK: 29.1358, SSIM: 0.9140, LPIPS: 0.0389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 356/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0135, G_loss=0.0135]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [356/500] - Average G Loss: 0.0123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 357/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00941, G_loss=0.00941]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [357/500] - Average G Loss: 0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 358/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0142, G_loss=0.0142]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [358/500] - Average G Loss: 0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 359/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00749, G_loss=0.00749]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [359/500] - Average G Loss: 0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 360/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.013, G_loss=0.013]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [360/500] - Average G Loss: 0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 360/500: 100%|██████████| 68/68 [02:06<00:00,  1.85s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [360/500] Val G Loss: 0.0275, Val L1 Loss: 0.0275, PSNR_MASK: 29.1708, SSIM: 0.9137, LPIPS: 0.0390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 361/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0109, G_loss=0.0109]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [361/500] - Average G Loss: 0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 362/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0137, G_loss=0.0137]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [362/500] - Average G Loss: 0.0122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 363/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0101, G_loss=0.0101]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [363/500] - Average G Loss: 0.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 364/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0128, G_loss=0.0128]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [364/500] - Average G Loss: 0.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 365/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0107, G_loss=0.0107]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [365/500] - Average G Loss: 0.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 365/500: 100%|██████████| 68/68 [02:06<00:00,  1.87s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [365/500] Val G Loss: 0.0274, Val L1 Loss: 0.0274, PSNR_MASK: 29.1209, SSIM: 0.9126, LPIPS: 0.0401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 366/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.00906, G_loss=0.00906]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [366/500] - Average G Loss: 0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 367/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0126, G_loss=0.0126]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [367/500] - Average G Loss: 0.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 368/500: 100%|██████████| 665/665 [07:12<00:00,  1.54batch/s, G1_loss=0.0104, G_loss=0.0104]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [368/500] - Average G Loss: 0.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 369/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.00961, G_loss=0.00961]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [369/500] - Average G Loss: 0.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 370/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0102, G_loss=0.0102]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [370/500] - Average G Loss: 0.0121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 370/500: 100%|██████████| 68/68 [02:10<00:00,  1.91s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [370/500] Val G Loss: 0.0276, Val L1 Loss: 0.0276, PSNR_MASK: 29.1967, SSIM: 0.9158, LPIPS: 0.0384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 371/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0131, G_loss=0.0131]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [371/500] - Average G Loss: 0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 372/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0119, G_loss=0.0119]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [372/500] - Average G Loss: 0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 373/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0105, G_loss=0.0105]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [373/500] - Average G Loss: 0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 374/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0155, G_loss=0.0155]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [374/500] - Average G Loss: 0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 375/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0104, G_loss=0.0104]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [375/500] - Average G Loss: 0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 375/500: 100%|██████████| 68/68 [02:06<00:00,  1.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [375/500] Val G Loss: 0.0273, Val L1 Loss: 0.0273, PSNR_MASK: 29.1685, SSIM: 0.9144, LPIPS: 0.0388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 376/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0257, G_loss=0.0257]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [376/500] - Average G Loss: 0.0119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 377/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0125, G_loss=0.0125]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [377/500] - Average G Loss: 0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 378/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0106, G_loss=0.0106]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [378/500] - Average G Loss: 0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 379/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00847, G_loss=0.00847]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [379/500] - Average G Loss: 0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 380/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0102, G_loss=0.0102]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [380/500] - Average G Loss: 0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 380/500: 100%|██████████| 68/68 [02:06<00:00,  1.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [380/500] Val G Loss: 0.0277, Val L1 Loss: 0.0277, PSNR_MASK: 29.0398, SSIM: 0.9131, LPIPS: 0.0394\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 381/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0231, G_loss=0.0231]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [381/500] - Average G Loss: 0.0119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 382/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00659, G_loss=0.00659]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [382/500] - Average G Loss: 0.0119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 383/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0135, G_loss=0.0135]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [383/500] - Average G Loss: 0.0120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 384/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0117, G_loss=0.0117]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [384/500] - Average G Loss: 0.0119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 385/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0101, G_loss=0.0101]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [385/500] - Average G Loss: 0.0119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 385/500: 100%|██████████| 68/68 [02:06<00:00,  1.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [385/500] Val G Loss: 0.0276, Val L1 Loss: 0.0276, PSNR_MASK: 29.1028, SSIM: 0.9144, LPIPS: 0.0386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 386/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.00771, G_loss=0.00771]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [386/500] - Average G Loss: 0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 387/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.00814, G_loss=0.00814]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [387/500] - Average G Loss: 0.0119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 388/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00832, G_loss=0.00832]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [388/500] - Average G Loss: 0.0119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 389/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00863, G_loss=0.00863]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [389/500] - Average G Loss: 0.0119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 390/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.00811, G_loss=0.00811]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [390/500] - Average G Loss: 0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 390/500: 100%|██████████| 68/68 [02:06<00:00,  1.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [390/500] Val G Loss: 0.0276, Val L1 Loss: 0.0276, PSNR_MASK: 29.1090, SSIM: 0.9144, LPIPS: 0.0390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 391/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.00887, G_loss=0.00887]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [391/500] - Average G Loss: 0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 392/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0196, G_loss=0.0196]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [392/500] - Average G Loss: 0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 393/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0131, G_loss=0.0131]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [393/500] - Average G Loss: 0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 394/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00793, G_loss=0.00793]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [394/500] - Average G Loss: 0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 395/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00694, G_loss=0.00694]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [395/500] - Average G Loss: 0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 395/500: 100%|██████████| 68/68 [02:06<00:00,  1.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [395/500] Val G Loss: 0.0280, Val L1 Loss: 0.0280, PSNR_MASK: 29.0408, SSIM: 0.9130, LPIPS: 0.0389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 396/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0166, G_loss=0.0166]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [396/500] - Average G Loss: 0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 397/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.00917, G_loss=0.00917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [397/500] - Average G Loss: 0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 398/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0195, G_loss=0.0195]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [398/500] - Average G Loss: 0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 399/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0105, G_loss=0.0105]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [399/500] - Average G Loss: 0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 400/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.00931, G_loss=0.00931]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [400/500] - Average G Loss: 0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 400/500: 100%|██████████| 68/68 [02:05<00:00,  1.85s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [400/500] Val G Loss: 0.0276, Val L1 Loss: 0.0276, PSNR_MASK: 29.0966, SSIM: 0.9130, LPIPS: 0.0390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 401/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0149, G_loss=0.0149]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [401/500] - Average G Loss: 0.0118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 402/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0133, G_loss=0.0133]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [402/500] - Average G Loss: 0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 403/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.00886, G_loss=0.00886]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [403/500] - Average G Loss: 0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 404/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.01, G_loss=0.01]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [404/500] - Average G Loss: 0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 405/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0148, G_loss=0.0148]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [405/500] - Average G Loss: 0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 405/500: 100%|██████████| 68/68 [02:06<00:00,  1.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [405/500] Val G Loss: 0.0277, Val L1 Loss: 0.0277, PSNR_MASK: 29.0700, SSIM: 0.9131, LPIPS: 0.0390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 406/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.015, G_loss=0.015]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [406/500] - Average G Loss: 0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 407/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0145, G_loss=0.0145]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [407/500] - Average G Loss: 0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 408/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00851, G_loss=0.00851]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [408/500] - Average G Loss: 0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 409/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00888, G_loss=0.00888]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [409/500] - Average G Loss: 0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 410/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0119, G_loss=0.0119]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [410/500] - Average G Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 410/500: 100%|██████████| 68/68 [02:06<00:00,  1.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [410/500] Val G Loss: 0.0278, Val L1 Loss: 0.0278, PSNR_MASK: 29.1101, SSIM: 0.9136, LPIPS: 0.0390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 411/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00899, G_loss=0.00899]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [411/500] - Average G Loss: 0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 412/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0108, G_loss=0.0108]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [412/500] - Average G Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 413/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.014, G_loss=0.014]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [413/500] - Average G Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 414/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0246, G_loss=0.0246]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [414/500] - Average G Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 415/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00812, G_loss=0.00812]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [415/500] - Average G Loss: 0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 415/500: 100%|██████████| 68/68 [02:06<00:00,  1.85s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [415/500] Val G Loss: 0.0279, Val L1 Loss: 0.0279, PSNR_MASK: 29.0901, SSIM: 0.9140, LPIPS: 0.0388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 416/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.00822, G_loss=0.00822]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [416/500] - Average G Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 417/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0106, G_loss=0.0106]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [417/500] - Average G Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 418/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.00854, G_loss=0.00854]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [418/500] - Average G Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 419/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0143, G_loss=0.0143]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [419/500] - Average G Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 420/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00945, G_loss=0.00945]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [420/500] - Average G Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 420/500: 100%|██████████| 68/68 [02:06<00:00,  1.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [420/500] Val G Loss: 0.0278, Val L1 Loss: 0.0278, PSNR_MASK: 29.0810, SSIM: 0.9142, LPIPS: 0.0388\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 421/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0106, G_loss=0.0106]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [421/500] - Average G Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 422/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0089, G_loss=0.0089]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [422/500] - Average G Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 423/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00887, G_loss=0.00887]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [423/500] - Average G Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 424/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00969, G_loss=0.00969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [424/500] - Average G Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 425/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0102, G_loss=0.0102]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [425/500] - Average G Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 425/500: 100%|██████████| 68/68 [02:06<00:00,  1.86s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [425/500] Val G Loss: 0.0279, Val L1 Loss: 0.0279, PSNR_MASK: 29.0340, SSIM: 0.9127, LPIPS: 0.0392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 426/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0101, G_loss=0.0101]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [426/500] - Average G Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 427/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.0169, G_loss=0.0169]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [427/500] - Average G Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 428/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00912, G_loss=0.00912]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [428/500] - Average G Loss: 0.0117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 429/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00713, G_loss=0.00713]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [429/500] - Average G Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 430/500: 100%|██████████| 665/665 [07:02<00:00,  1.57batch/s, G1_loss=0.00979, G_loss=0.00979]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [430/500] - Average G Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 430/500: 100%|██████████| 68/68 [02:08<00:00,  1.89s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [430/500] Val G Loss: 0.0278, Val L1 Loss: 0.0278, PSNR_MASK: 29.0356, SSIM: 0.9133, LPIPS: 0.0390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 431/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0146, G_loss=0.0146]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [431/500] - Average G Loss: 0.0116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 432/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.00974, G_loss=0.00974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [432/500] - Average G Loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 433/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0108, G_loss=0.0108]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [433/500] - Average G Loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 434/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0166, G_loss=0.0166]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [434/500] - Average G Loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 435/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0182, G_loss=0.0182]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [435/500] - Average G Loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 435/500: 100%|██████████| 68/68 [02:08<00:00,  1.89s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [435/500] Val G Loss: 0.0276, Val L1 Loss: 0.0276, PSNR_MASK: 29.0890, SSIM: 0.9136, LPIPS: 0.0387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 436/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.00811, G_loss=0.00811]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [436/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 437/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.00608, G_loss=0.00608]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [437/500] - Average G Loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 438/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0114, G_loss=0.0114]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [438/500] - Average G Loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 439/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0124, G_loss=0.0124]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [439/500] - Average G Loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 440/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0105, G_loss=0.0105]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [440/500] - Average G Loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 440/500: 100%|██████████| 68/68 [02:08<00:00,  1.88s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [440/500] Val G Loss: 0.0279, Val L1 Loss: 0.0279, PSNR_MASK: 29.0021, SSIM: 0.9124, LPIPS: 0.0391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 441/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.00933, G_loss=0.00933]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [441/500] - Average G Loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 442/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0118, G_loss=0.0118]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [442/500] - Average G Loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 443/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.013, G_loss=0.013]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [443/500] - Average G Loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 444/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.00736, G_loss=0.00736]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [444/500] - Average G Loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 445/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0268, G_loss=0.0268]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [445/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 445/500: 100%|██████████| 68/68 [02:08<00:00,  1.88s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [445/500] Val G Loss: 0.0276, Val L1 Loss: 0.0276, PSNR_MASK: 29.0824, SSIM: 0.9130, LPIPS: 0.0389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 446/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0101, G_loss=0.0101]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [446/500] - Average G Loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 447/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0145, G_loss=0.0145]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [447/500] - Average G Loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 448/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0111, G_loss=0.0111]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [448/500] - Average G Loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 449/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0121, G_loss=0.0121]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [449/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 450/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0177, G_loss=0.0177]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [450/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 450/500: 100%|██████████| 68/68 [02:08<00:00,  1.88s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [450/500] Val G Loss: 0.0278, Val L1 Loss: 0.0278, PSNR_MASK: 29.0371, SSIM: 0.9132, LPIPS: 0.0389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 451/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0078, G_loss=0.0078]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [451/500] - Average G Loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 452/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.00756, G_loss=0.00756]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [452/500] - Average G Loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 453/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.00846, G_loss=0.00846]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [453/500] - Average G Loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 454/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0167, G_loss=0.0167]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [454/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 455/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0113, G_loss=0.0113]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [455/500] - Average G Loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 455/500: 100%|██████████| 68/68 [02:08<00:00,  1.89s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [455/500] Val G Loss: 0.0278, Val L1 Loss: 0.0278, PSNR_MASK: 29.0427, SSIM: 0.9128, LPIPS: 0.0391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 456/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0158, G_loss=0.0158]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [456/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 457/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0338, G_loss=0.0338]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [457/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 458/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0128, G_loss=0.0128]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [458/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 459/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0111, G_loss=0.0111]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [459/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 460/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0182, G_loss=0.0182]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [460/500] - Average G Loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 460/500: 100%|██████████| 68/68 [02:08<00:00,  1.89s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [460/500] Val G Loss: 0.0280, Val L1 Loss: 0.0280, PSNR_MASK: 28.9985, SSIM: 0.9127, LPIPS: 0.0391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 461/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0254, G_loss=0.0254]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [461/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 462/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.016, G_loss=0.016]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [462/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 463/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0077, G_loss=0.0077]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [463/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 464/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.00962, G_loss=0.00962]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [464/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 465/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0101, G_loss=0.0101]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [465/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 465/500: 100%|██████████| 68/68 [02:08<00:00,  1.89s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [465/500] Val G Loss: 0.0277, Val L1 Loss: 0.0277, PSNR_MASK: 29.0430, SSIM: 0.9122, LPIPS: 0.0391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 466/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.00963, G_loss=0.00963]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [466/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 467/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0131, G_loss=0.0131]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [467/500] - Average G Loss: 0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 468/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.012, G_loss=0.012]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [468/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 469/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.00864, G_loss=0.00864]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [469/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 470/500: 100%|██████████| 665/665 [07:13<00:00,  1.53batch/s, G1_loss=0.0144, G_loss=0.0144]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [470/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 470/500: 100%|██████████| 68/68 [02:12<00:00,  1.95s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [470/500] Val G Loss: 0.0279, Val L1 Loss: 0.0279, PSNR_MASK: 29.0200, SSIM: 0.9120, LPIPS: 0.0392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 471/500: 100%|██████████| 665/665 [07:06<00:00,  1.56batch/s, G1_loss=0.00772, G_loss=0.00772]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [471/500] - Average G Loss: 0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 472/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.00909, G_loss=0.00909]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [472/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 473/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0118, G_loss=0.0118]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [473/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 474/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.00887, G_loss=0.00887]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [474/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 475/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.00772, G_loss=0.00772]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [475/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 475/500: 100%|██████████| 68/68 [02:07<00:00,  1.88s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [475/500] Val G Loss: 0.0278, Val L1 Loss: 0.0278, PSNR_MASK: 29.0486, SSIM: 0.9128, LPIPS: 0.0389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 476/500: 100%|██████████| 665/665 [07:05<00:00,  1.56batch/s, G1_loss=0.014, G_loss=0.014]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [476/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 477/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0083, G_loss=0.0083]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [477/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 478/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.00824, G_loss=0.00824]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [478/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 479/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.00755, G_loss=0.00755]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [479/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 480/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.013, G_loss=0.013]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [480/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 480/500: 100%|██████████| 68/68 [02:08<00:00,  1.88s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [480/500] Val G Loss: 0.0278, Val L1 Loss: 0.0278, PSNR_MASK: 29.0335, SSIM: 0.9123, LPIPS: 0.0391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 481/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.013, G_loss=0.013]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [481/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 482/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.00937, G_loss=0.00937]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [482/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 483/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.00953, G_loss=0.00953]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [483/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 484/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.00855, G_loss=0.00855]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [484/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 485/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0184, G_loss=0.0184]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [485/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 485/500: 100%|██████████| 68/68 [02:07<00:00,  1.88s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [485/500] Val G Loss: 0.0278, Val L1 Loss: 0.0278, PSNR_MASK: 29.0323, SSIM: 0.9125, LPIPS: 0.0391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 486/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.00817, G_loss=0.00817]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [486/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 487/500: 100%|██████████| 665/665 [07:03<00:00,  1.57batch/s, G1_loss=0.0157, G_loss=0.0157]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [487/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 488/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0101, G_loss=0.0101]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [488/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 489/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.00986, G_loss=0.00986]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [489/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 490/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0104, G_loss=0.0104]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [490/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 490/500: 100%|██████████| 68/68 [02:08<00:00,  1.88s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [490/500] Val G Loss: 0.0278, Val L1 Loss: 0.0278, PSNR_MASK: 29.0316, SSIM: 0.9125, LPIPS: 0.0391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 491/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.00858, G_loss=0.00858]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [491/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 492/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0075, G_loss=0.0075]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [492/500] - Average G Loss: 0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 493/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.00788, G_loss=0.00788]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [493/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 494/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.02, G_loss=0.02]      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [494/500] - Average G Loss: 0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 495/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0165, G_loss=0.0165]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [495/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 495/500: 100%|██████████| 68/68 [02:08<00:00,  1.88s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [495/500] Val G Loss: 0.0278, Val L1 Loss: 0.0278, PSNR_MASK: 29.0314, SSIM: 0.9125, LPIPS: 0.0391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 496/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.00928, G_loss=0.00928]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [496/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 497/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.018, G_loss=0.018]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [497/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 498/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.007, G_loss=0.007]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [498/500] - Average G Loss: 0.0113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 499/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0173, G_loss=0.0173]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [499/500] - Average G Loss: 0.0114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 500/500: 100%|██████████| 665/665 [07:04<00:00,  1.57batch/s, G1_loss=0.0105, G_loss=0.0105]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [500/500] - Average G Loss: 0.0115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 500/500: 100%|██████████| 68/68 [02:07<00:00,  1.88s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [500/500] Val G Loss: 0.0279, Val L1 Loss: 0.0279, PSNR_MASK: 29.0268, SSIM: 0.9125, LPIPS: 0.0390\n",
      "Training completed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from lpips import LPIPS\n",
    "import torchvision.models as models\n",
    "\n",
    "# Constants\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 500\n",
    "SAVE_INTERVAL = 5\n",
    "START_EPOCH = 0  # Start from epoch 80\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#G1.load_state_dict(torch.load(r\"C:\\Users\\nasef\\Desktop\\SBSR\\Models\\SBSR32\\SBSR30.pth\"))\n",
    "\n",
    "\n",
    "optimizer_G = optim.Adam(G1.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999))\n",
    "\n",
    "# Add the cosine annealing scheduler\n",
    "scheduler_G = optim.lr_scheduler.CosineAnnealingLR(optimizer_G, T_max=NUM_EPOCHS - START_EPOCH, eta_min=0.000001)\n",
    "\n",
    "# Loss functions\n",
    "l1_loss = nn.L1Loss()\n",
    "mse_loss = nn.MSELoss()\n",
    "lpips_loss_fn = LPIPS(net='alex').to(DEVICE)  # Using AlexNet for LPIPS loss\n",
    "\n",
    "# Define the VGG loss\n",
    "class VGGLoss(nn.Module):\n",
    "    def __init__(self, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "        super(VGGLoss, self).__init__()\n",
    "        vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features.to(device).eval()  # Move VGG layers to device and set to eval mode\n",
    "        self.vgg_layers = nn.Sequential(*list(vgg)[:36]).to(device)\n",
    "        self.device = device\n",
    "        for param in self.vgg_layers.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = x.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "        x_vgg = self.vgg_layers(x)\n",
    "        y_vgg = self.vgg_layers(y)\n",
    "        loss = nn.functional.l1_loss(x_vgg, y_vgg)\n",
    "        return loss\n",
    "\n",
    "# Define Focal Frequency Loss\n",
    "class FocalFrequencyLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "        super(FocalFrequencyLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input = input.to(self.device)\n",
    "        target = target.to(self.device)\n",
    "        input_fft = torch.fft.fft2(input)\n",
    "        target_fft = torch.fft.fft2(target)\n",
    "        diff = input_fft - target_fft\n",
    "        abs_diff = torch.abs(diff)\n",
    "        loss = torch.pow(abs_diff, self.alpha)\n",
    "        return torch.mean(loss)\n",
    "\n",
    "# Loss weights\n",
    "lambda_vgg = 0.01\n",
    "lambda_ff = 0.1\n",
    "\n",
    "# Combined loss\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, lambda_vgg, lambda_ff, device=DEVICE):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.lambda_vgg = lambda_vgg\n",
    "        self.lambda_ff = lambda_ff\n",
    "        self.vgg_loss = VGGLoss(device=device)\n",
    "        self.ff_loss = FocalFrequencyLoss(device=device)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        l1 = l1_loss(input, target)\n",
    "        vgg = self.vgg_loss(input, target)\n",
    "        ff = self.ff_loss(input, target)\n",
    "        return l1 + self.lambda_vgg * vgg + self.lambda_ff * ff\n",
    "\n",
    "# Example usage\n",
    "combined_loss = CombinedLoss(lambda_vgg=lambda_vgg, lambda_ff=lambda_ff, device=DEVICE)\n",
    "\n",
    "# Directories for checkpoints and samples\n",
    "checkpoint_dir = r'C:\\Users\\Admin\\Desktop\\AI_RAMiT\\FFAT_RAMiT'\n",
    "sample_dir = r'C:\\Users\\Admin\\Desktop\\AI_RAMiT\\FFAT_RAMiT_s'\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "os.makedirs(sample_dir, exist_ok=True)\n",
    "save_interval = SAVE_INTERVAL  # Save checkpoint and samples every 5 epochs\n",
    "\n",
    "for epoch in range(START_EPOCH, NUM_EPOCHS):\n",
    "    # Training\n",
    "    G1.train()\n",
    "    total_g_loss = 0.0\n",
    "    total_l1_loss = 0.0\n",
    "    with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{NUM_EPOCHS}', unit='batch') as pbar:\n",
    "        for i, (real_shadow, real_free) in enumerate(train_loader):\n",
    "            real_shadow = real_shadow.to(DEVICE)\n",
    "            real_free = real_free.to(DEVICE)\n",
    "            optimizer_G.zero_grad()\n",
    "\n",
    "            pred_free = G1(real_shadow)\n",
    "            # Generator loss\n",
    "            g1_loss = l1_loss(pred_free, real_free)\n",
    "            g_loss = g1_loss\n",
    "            g_loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(G1.parameters(), max_norm=1.0)\n",
    "            optimizer_G.step()\n",
    "\n",
    "            # Accumulate losses\n",
    "            total_g_loss += g_loss.item()\n",
    "            total_l1_loss += g1_loss.item()\n",
    "\n",
    "            pbar.set_postfix(G_loss=g_loss.item(), G1_loss=g1_loss.item())\n",
    "            pbar.update(1)\n",
    "\n",
    "    # Calculate average losses\n",
    "    avg_g_loss = total_g_loss / len(train_loader)\n",
    "    avg_l1_loss = total_l1_loss / len(train_loader)\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}] - Average G Loss: {avg_g_loss:.4f}\")\n",
    "    scheduler_G.step()\n",
    "\n",
    "    m = 0\n",
    "    # Validation and Checkpoints\n",
    "    if (epoch + 1) % save_interval == 0:\n",
    "        torch.cuda.empty_cache()\n",
    "        G1.eval()\n",
    "        with torch.no_grad():\n",
    "            val_g_loss = 0.0\n",
    "            val_l1_loss = 0.0\n",
    "            val_ssim_loss = 0.0\n",
    "            val_lpips_loss = 0.0\n",
    "            val_psnr_mask = 0.0\n",
    "\n",
    "            with tqdm(total=len(val_loader), desc=f'Validation Epoch {epoch + 1}/{NUM_EPOCHS}', unit='batch') as pbar_val:\n",
    "                for i, (real_shadow, real_free) in enumerate(val_loader):\n",
    "                    real_shadow = real_shadow.to(DEVICE)\n",
    "                    real_free = real_free.to(DEVICE)\n",
    "\n",
    "                    pred_free = G1(real_shadow)\n",
    "                    # Generator loss\n",
    "                    g1_loss = l1_loss(pred_free, real_free)\n",
    "                    g_loss = g1_loss\n",
    "\n",
    "                    val_g_loss += g_loss.item()\n",
    "                    val_l1_loss += g1_loss.item()\n",
    "\n",
    "                    # Calculate SSIM and LPIPS\n",
    "                    pred_free_np = pred_free.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "                    real_free_np = real_free.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "                    #ssim_val = ssim(pred_free_np, real_free_np, data_range=1, channel_axis=-1)\n",
    "                    lpips_val = lpips_loss_fn(pred_free, real_free).mean().item()  # Average LPIPS over batch\n",
    "                    #val_ssim_loss += ssim_val\n",
    "                    val_lpips_loss += lpips_val\n",
    "\n",
    "                    # Calculate PSNR for final shadow-free images\n",
    "                    for j in range(real_free_np.shape[0]):\n",
    "                        val_psnr_mask += psnr(real_free_np[j], pred_free_np[j], data_range=1)\n",
    "                        val_ssim_loss += ssim(real_free_np[j], pred_free_np[j], data_range=1, channel_axis=-1)\n",
    "\n",
    "\n",
    "                    pbar_val.update(1)\n",
    "                    save_image(pred_free, os.path.join(sample_dir, f'pred_free_epoch_{epoch + 1}_{m}.png'))\n",
    "                    m += 1\n",
    "\n",
    "            # Averaging over the entire validation set\n",
    "            val_g_loss /= len(val_loader)\n",
    "            val_l1_loss /= len(val_loader)\n",
    "            val_ssim_loss /= len(val_loader.dataset)\n",
    "            val_lpips_loss /= len(val_loader)\n",
    "            val_psnr_mask /= len(val_loader.dataset)\n",
    "\n",
    "            print(f\"Validation after Epoch [{epoch + 1}/{NUM_EPOCHS}] \"\n",
    "                  f\"Val G Loss: {val_g_loss:.4f}, Val L1 Loss: {val_l1_loss:.4f}, \"\n",
    "                  f\"PSNR_MASK: {val_psnr_mask:.4f}, SSIM: {val_ssim_loss:.4f}, LPIPS: {val_lpips_loss:.4f}\")\n",
    "\n",
    "            # Save model checkpoints\n",
    "            torch.save(G1.state_dict(), os.path.join(checkpoint_dir, f'SBSR{epoch + 1}.pth'))\n",
    "\n",
    "print(\"Training completed\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAMiT eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_127352\\2927647449.py:19: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(f'C:/Users/Admin/Desktop/AI_RAMiT/FFAT_RAMiT/SBSR{e}.pth', map_location='cuda')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 2/1: 100%|██████████| 54/54 [00:53<00:00,  1.00batch/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation after Epoch [2/1] Val G Loss: 0, Val L1 Loss: 0.1416, PSNR_MASK: 15.5914, PSNR_Y: 0.0000, SSIM: 0.5182, SSIM_crop4: 0.0000, LPIPS: 0.2441,epoch5\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation Epoch 2/1:   0%|          | 0/54 [00:00<?, ?batch/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\nn\\parallel\\parallel_apply.py\", line 83, in _worker\n    output = module(*input, **kwargs)\n  File \"c:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"c:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"c:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py\", line 172, in forward\n    raise RuntimeError(\"module must have its parameters and buffers \"\nRuntimeError: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cuda:1\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 59\u001b[0m\n\u001b[0;32m     56\u001b[0m real_shadow \u001b[38;5;241m=\u001b[39m real_shadow\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[0;32m     57\u001b[0m real_free \u001b[38;5;241m=\u001b[39m real_free\u001b[38;5;241m.\u001b[39mto(DEVICE)\n\u001b[1;32m---> 59\u001b[0m pred_free \u001b[38;5;241m=\u001b[39m \u001b[43mG1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_shadow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# Generator loss\u001b[39;00m\n\u001b[0;32m     61\u001b[0m g1_loss \u001b[38;5;241m=\u001b[39m l1_loss(pred_free, real_free)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\nn\\modules\\module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\nn\\modules\\module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:186\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[1;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m    185\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[1;32m--> 186\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py:201\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[1;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\nn\\parallel\\parallel_apply.py:108\u001b[0m, in \u001b[0;36mparallel_apply\u001b[1;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[0;32m    106\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[0;32m    107\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[1;32m--> 108\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[1;32mc:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\_utils.py:706\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m    703\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 706\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Caught RuntimeError in replica 1 on device 1.\nOriginal Traceback (most recent call last):\n  File \"c:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\nn\\parallel\\parallel_apply.py\", line 83, in _worker\n    output = module(*input, **kwargs)\n  File \"c:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"c:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"c:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\torch\\nn\\parallel\\data_parallel.py\", line 172, in forward\n    raise RuntimeError(\"module must have its parameters and buffers \"\nRuntimeError: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cuda:1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from lpips import LPIPS\n",
    "import torchvision.models as models\n",
    "e = 5\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "G1 = RAMiT_FFAT(target_mode='light_realdn',img_norm=False, mv_ver = 3)\n",
    "while e<=500:\n",
    "    from collections import OrderedDict\n",
    "\n",
    "    # Load checkpoint and create new state_dict with 'module.' prefix if needed\n",
    "    checkpoint = torch.load(f'C:/Users/Admin/Desktop/AI_RAMiT/FFAT_RAMiT/SBSR{e}.pth', map_location='cuda')\n",
    "    state_dict = checkpoint['state_dict'] if 'state_dict' in checkpoint else checkpoint\n",
    "    new_state_dict = OrderedDict()\n",
    "\n",
    "    # Add prefix 'module.' if needed\n",
    "    for k, v in state_dict.items():\n",
    "        name = k if k.startswith('module.') else 'module.' + k\n",
    "        new_state_dict[name] = v\n",
    "\n",
    "\n",
    "    # Apply DataParallel and move to GPU\n",
    "    G1 = G1.to('cuda')\n",
    "    G1 = torch.nn.DataParallel(G1, device_ids=[0, 1])\n",
    "\n",
    "    # G1.to('cuda')\n",
    "    epoch = 1\n",
    "    NUM_EPOCHS = 1\n",
    "# optimizer_G = optim.Adam(G1.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999))\n",
    "\n",
    "# # # Add the cosine annealing scheduler\n",
    "# scheduler_G = optim.lr_scheduler.CosineAnnealingLR(optimizer_G, T_max=NUM_EPOCHS - START_EPOCH, eta_min=0.000001)\n",
    "\n",
    "# Loss functions\n",
    "    l1_loss = nn.L1Loss()\n",
    "    mse_loss = nn.MSELoss()\n",
    "    lpips_loss_fn = LPIPS(net='alex').to(DEVICE)  # Using AlexNet for LPIPS loss\n",
    "    with torch.no_grad():\n",
    "                val_g_loss = 0.0\n",
    "                val_l1_loss = 0.0\n",
    "                val_ssim_loss = 0.0\n",
    "                val_ssim_crop4_loss = 0.0\n",
    "                val_lpips_loss = 0.0\n",
    "                val_psnr_mask = 0.0\n",
    "                val_psnr_y = 0.0\n",
    "\n",
    "                with tqdm(total=len(val_loader), desc=f'Validation Epoch {epoch + 1}/{NUM_EPOCHS}', unit='batch') as pbar_val:\n",
    "                    for i, (real_shadow, real_free) in enumerate(val_loader):\n",
    "                        real_shadow = real_shadow.to(DEVICE)\n",
    "                        real_free = real_free.to(DEVICE)\n",
    "\n",
    "                        pred_free = G1(real_shadow)\n",
    "                        # Generator loss\n",
    "                        g1_loss = l1_loss(pred_free, real_free)\n",
    "                        # g_loss = combined_loss(pred_free, real_free)\n",
    "\n",
    "                        # val_g_loss += g_loss.item()\n",
    "                        val_l1_loss += g1_loss.item()\n",
    "\n",
    "                        # Calculate SSIM, LPIPS, PSNR, PSNR Y, and SSIM crop 4\n",
    "                        pred_free_np = pred_free.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "                        real_free_np = real_free.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "                        lpips_val = lpips_loss_fn(pred_free, real_free).mean().item()  # Average LPIPS over batch\n",
    "                        val_lpips_loss += lpips_val\n",
    "\n",
    "                        for j in range(real_free_np.shape[0]):\n",
    "                            val_psnr_mask += psnr(real_free_np[j], pred_free_np[j], data_range=1)\n",
    "                            val_ssim_loss += ssim(real_free_np[j], pred_free_np[j], data_range=1, channel_axis=-1)\n",
    "                            # val_psnr_y += psnr_y(real_free_np[j], pred_free_np[j])\n",
    "                            # val_ssim_crop4_loss += ssim_crop4(real_free_np[j], pred_free_np[j])\n",
    "\n",
    "                        pbar_val.update(1)\n",
    "\n",
    "                        # m += 1\n",
    "\n",
    "                # Averaging over the entire validation set\n",
    "                # val_g_loss /= len(val_loader)\n",
    "                val_l1_loss /= len(val_loader)\n",
    "                val_ssim_loss /= len(val_loader.dataset)\n",
    "                # val_ssim_crop4_loss /= len(val_loader.dataset)\n",
    "                val_lpips_loss /= len(val_loader)\n",
    "                val_psnr_mask /= len(val_loader.dataset)\n",
    "                # val_psnr_y /= len(val_loader.dataset)\n",
    "\n",
    "                print(f\"Validation after Epoch [{epoch + 1}/{NUM_EPOCHS}] \"\n",
    "                    f\"Val G Loss: 0, Val L1 Loss: {val_l1_loss:.4f}, \"\n",
    "                    f\"PSNR_MASK: {val_psnr_mask:.4f}, PSNR_Y: {val_psnr_y:.4f}, \"\n",
    "                    f\"SSIM: {val_ssim_loss:.4f}, SSIM_crop4: {val_ssim_crop4_loss:.4f}, \"\n",
    "                    f\"LPIPS: {val_lpips_loss:.4f},\"\n",
    "                    f\"epoch{e}\")\n",
    "                e = e+5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n",
      "Loading model from: c:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "Setting up [LPIPS] perceptual loss: trunk [alex], v[0.1], spatial [off]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\AppData\\Local\\Temp\\ipykernel_190916\\1381373566.py:103: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(f'C:/Users/Admin/Desktop/AI_RAMiT/FFAT_RAMiT/SBSR{e}.pth', map_location='cuda')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: c:\\Users\\Admin\\anaconda3\\envs\\ramit\\lib\\site-packages\\lpips\\weights\\v0.1\\alex.pth\n",
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:34<00:00,  1.40s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0403, Val L1 Loss: 0.0403, PSNR_MASK: 25.3361, SSIM: 0.8982, LPIPS: 0.0759\n",
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0405, Val L1 Loss: 0.0405, PSNR_MASK: 25.7090, SSIM: 0.9022, LPIPS: 0.0661\n",
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0390, Val L1 Loss: 0.0390, PSNR_MASK: 25.5072, SSIM: 0.9015, LPIPS: 0.0680\n",
      "20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0349, Val L1 Loss: 0.0349, PSNR_MASK: 26.7092, SSIM: 0.9063, LPIPS: 0.0617\n",
      "25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0329, Val L1 Loss: 0.0329, PSNR_MASK: 27.0414, SSIM: 0.9077, LPIPS: 0.0587\n",
      "30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0390, Val L1 Loss: 0.0390, PSNR_MASK: 26.7239, SSIM: 0.9045, LPIPS: 0.0614\n",
      "35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0324, Val L1 Loss: 0.0324, PSNR_MASK: 27.4939, SSIM: 0.9107, LPIPS: 0.0544\n",
      "40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0374, Val L1 Loss: 0.0374, PSNR_MASK: 26.7938, SSIM: 0.9097, LPIPS: 0.0550\n",
      "45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0317, Val L1 Loss: 0.0317, PSNR_MASK: 27.8583, SSIM: 0.9133, LPIPS: 0.0496\n",
      "50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0313, Val L1 Loss: 0.0313, PSNR_MASK: 26.9664, SSIM: 0.9106, LPIPS: 0.0555\n",
      "55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0305, Val L1 Loss: 0.0305, PSNR_MASK: 27.7958, SSIM: 0.9149, LPIPS: 0.0485\n",
      "60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0289, Val L1 Loss: 0.0289, PSNR_MASK: 28.4671, SSIM: 0.9156, LPIPS: 0.0454\n",
      "65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0289, Val L1 Loss: 0.0289, PSNR_MASK: 28.2025, SSIM: 0.9153, LPIPS: 0.0454\n",
      "70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0287, Val L1 Loss: 0.0287, PSNR_MASK: 28.5729, SSIM: 0.9173, LPIPS: 0.0419\n",
      "75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0313, Val L1 Loss: 0.0313, PSNR_MASK: 27.3557, SSIM: 0.9133, LPIPS: 0.0488\n",
      "80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0277, Val L1 Loss: 0.0277, PSNR_MASK: 28.8044, SSIM: 0.9165, LPIPS: 0.0429\n",
      "85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0305, Val L1 Loss: 0.0305, PSNR_MASK: 28.4372, SSIM: 0.9135, LPIPS: 0.0460\n",
      "90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0299, Val L1 Loss: 0.0299, PSNR_MASK: 28.0715, SSIM: 0.9125, LPIPS: 0.0462\n",
      "95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0290, Val L1 Loss: 0.0290, PSNR_MASK: 28.6417, SSIM: 0.9161, LPIPS: 0.0421\n",
      "100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0279, Val L1 Loss: 0.0279, PSNR_MASK: 28.5613, SSIM: 0.9169, LPIPS: 0.0432\n",
      "105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0282, Val L1 Loss: 0.0282, PSNR_MASK: 28.2986, SSIM: 0.9173, LPIPS: 0.0443\n",
      "110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0298, Val L1 Loss: 0.0298, PSNR_MASK: 28.3607, SSIM: 0.9162, LPIPS: 0.0420\n",
      "115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0292, Val L1 Loss: 0.0292, PSNR_MASK: 28.5548, SSIM: 0.9152, LPIPS: 0.0451\n",
      "120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0270, Val L1 Loss: 0.0270, PSNR_MASK: 29.1714, SSIM: 0.9178, LPIPS: 0.0402\n",
      "125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0298, Val L1 Loss: 0.0298, PSNR_MASK: 27.9690, SSIM: 0.9150, LPIPS: 0.0446\n",
      "130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0272, Val L1 Loss: 0.0272, PSNR_MASK: 29.1384, SSIM: 0.9164, LPIPS: 0.0410\n",
      "135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0265, Val L1 Loss: 0.0265, PSNR_MASK: 29.1879, SSIM: 0.9180, LPIPS: 0.0389\n",
      "140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0297, Val L1 Loss: 0.0297, PSNR_MASK: 28.2990, SSIM: 0.9124, LPIPS: 0.0461\n",
      "145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0297, Val L1 Loss: 0.0297, PSNR_MASK: 28.6804, SSIM: 0.9144, LPIPS: 0.0431\n",
      "150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0277, Val L1 Loss: 0.0277, PSNR_MASK: 28.9804, SSIM: 0.9151, LPIPS: 0.0422\n",
      "155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0292, Val L1 Loss: 0.0292, PSNR_MASK: 28.7369, SSIM: 0.9143, LPIPS: 0.0430\n",
      "160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0276, Val L1 Loss: 0.0276, PSNR_MASK: 29.0670, SSIM: 0.9137, LPIPS: 0.0435\n",
      "165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0293, Val L1 Loss: 0.0293, PSNR_MASK: 28.9100, SSIM: 0.9135, LPIPS: 0.0427\n",
      "170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0284, Val L1 Loss: 0.0284, PSNR_MASK: 29.0205, SSIM: 0.9157, LPIPS: 0.0419\n",
      "175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0281, Val L1 Loss: 0.0281, PSNR_MASK: 29.0665, SSIM: 0.9156, LPIPS: 0.0404\n",
      "180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0262, Val L1 Loss: 0.0262, PSNR_MASK: 29.3221, SSIM: 0.9185, LPIPS: 0.0392\n",
      "185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.33s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0271, Val L1 Loss: 0.0271, PSNR_MASK: 29.1960, SSIM: 0.9142, LPIPS: 0.0421\n",
      "190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0294, Val L1 Loss: 0.0294, PSNR_MASK: 28.6782, SSIM: 0.9114, LPIPS: 0.0447\n",
      "195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0258, Val L1 Loss: 0.0258, PSNR_MASK: 29.3138, SSIM: 0.9171, LPIPS: 0.0403\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0270, Val L1 Loss: 0.0270, PSNR_MASK: 29.2280, SSIM: 0.9111, LPIPS: 0.0435\n",
      "205\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0270, Val L1 Loss: 0.0270, PSNR_MASK: 29.2179, SSIM: 0.9143, LPIPS: 0.0409\n",
      "210\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:30<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0274, Val L1 Loss: 0.0274, PSNR_MASK: 29.2465, SSIM: 0.9158, LPIPS: 0.0399\n",
      "215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0275, Val L1 Loss: 0.0275, PSNR_MASK: 29.0877, SSIM: 0.9141, LPIPS: 0.0402\n",
      "220\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0279, Val L1 Loss: 0.0279, PSNR_MASK: 29.0515, SSIM: 0.9145, LPIPS: 0.0408\n",
      "225\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0263, Val L1 Loss: 0.0263, PSNR_MASK: 29.2310, SSIM: 0.9169, LPIPS: 0.0397\n",
      "230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0275, Val L1 Loss: 0.0275, PSNR_MASK: 29.2636, SSIM: 0.9139, LPIPS: 0.0409\n",
      "235\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0287, Val L1 Loss: 0.0287, PSNR_MASK: 29.0339, SSIM: 0.9153, LPIPS: 0.0406\n",
      "240\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0281, Val L1 Loss: 0.0281, PSNR_MASK: 29.1140, SSIM: 0.9147, LPIPS: 0.0409\n",
      "245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0268, Val L1 Loss: 0.0268, PSNR_MASK: 29.2740, SSIM: 0.9159, LPIPS: 0.0393\n",
      "250\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0289, Val L1 Loss: 0.0289, PSNR_MASK: 28.9125, SSIM: 0.9130, LPIPS: 0.0413\n",
      "255\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0315, Val L1 Loss: 0.0315, PSNR_MASK: 28.1567, SSIM: 0.9118, LPIPS: 0.0429\n",
      "260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0281, Val L1 Loss: 0.0281, PSNR_MASK: 29.1784, SSIM: 0.9137, LPIPS: 0.0398\n",
      "265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0283, Val L1 Loss: 0.0283, PSNR_MASK: 29.1559, SSIM: 0.9142, LPIPS: 0.0406\n",
      "270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0268, Val L1 Loss: 0.0268, PSNR_MASK: 29.2610, SSIM: 0.9137, LPIPS: 0.0397\n",
      "275\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0268, Val L1 Loss: 0.0268, PSNR_MASK: 29.2414, SSIM: 0.9142, LPIPS: 0.0398\n",
      "280\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0275, Val L1 Loss: 0.0275, PSNR_MASK: 29.0505, SSIM: 0.9119, LPIPS: 0.0412\n",
      "285\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0280, Val L1 Loss: 0.0280, PSNR_MASK: 29.1070, SSIM: 0.9152, LPIPS: 0.0392\n",
      "290\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0273, Val L1 Loss: 0.0273, PSNR_MASK: 29.1988, SSIM: 0.9150, LPIPS: 0.0392\n",
      "295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0269, Val L1 Loss: 0.0269, PSNR_MASK: 29.1975, SSIM: 0.9156, LPIPS: 0.0393\n",
      "300\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0281, Val L1 Loss: 0.0281, PSNR_MASK: 29.0674, SSIM: 0.9159, LPIPS: 0.0393\n",
      "305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0274, Val L1 Loss: 0.0274, PSNR_MASK: 29.2541, SSIM: 0.9156, LPIPS: 0.0387\n",
      "310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0273, Val L1 Loss: 0.0273, PSNR_MASK: 29.1059, SSIM: 0.9131, LPIPS: 0.0402\n",
      "315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0279, Val L1 Loss: 0.0279, PSNR_MASK: 29.0639, SSIM: 0.9116, LPIPS: 0.0401\n",
      "320\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0273, Val L1 Loss: 0.0273, PSNR_MASK: 29.1850, SSIM: 0.9167, LPIPS: 0.0383\n",
      "325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0284, Val L1 Loss: 0.0284, PSNR_MASK: 29.0256, SSIM: 0.9149, LPIPS: 0.0391\n",
      "330\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0273, Val L1 Loss: 0.0273, PSNR_MASK: 29.1657, SSIM: 0.9168, LPIPS: 0.0382\n",
      "335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0270, Val L1 Loss: 0.0270, PSNR_MASK: 29.1563, SSIM: 0.9151, LPIPS: 0.0391\n",
      "340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0269, Val L1 Loss: 0.0269, PSNR_MASK: 29.2642, SSIM: 0.9155, LPIPS: 0.0384\n",
      "345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0278, Val L1 Loss: 0.0278, PSNR_MASK: 29.1366, SSIM: 0.9136, LPIPS: 0.0396\n",
      "350\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0275, Val L1 Loss: 0.0275, PSNR_MASK: 29.1338, SSIM: 0.9149, LPIPS: 0.0389\n",
      "355\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0277, Val L1 Loss: 0.0277, PSNR_MASK: 29.1358, SSIM: 0.9140, LPIPS: 0.0389\n",
      "360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0275, Val L1 Loss: 0.0275, PSNR_MASK: 29.1708, SSIM: 0.9137, LPIPS: 0.0390\n",
      "365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0274, Val L1 Loss: 0.0274, PSNR_MASK: 29.1209, SSIM: 0.9126, LPIPS: 0.0401\n",
      "370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0276, Val L1 Loss: 0.0276, PSNR_MASK: 29.1967, SSIM: 0.9158, LPIPS: 0.0384\n",
      "375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0273, Val L1 Loss: 0.0273, PSNR_MASK: 29.1685, SSIM: 0.9144, LPIPS: 0.0388\n",
      "380\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0277, Val L1 Loss: 0.0277, PSNR_MASK: 29.0398, SSIM: 0.9131, LPIPS: 0.0394\n",
      "385\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0276, Val L1 Loss: 0.0276, PSNR_MASK: 29.1028, SSIM: 0.9144, LPIPS: 0.0386\n",
      "390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0276, Val L1 Loss: 0.0276, PSNR_MASK: 29.1090, SSIM: 0.9144, LPIPS: 0.0390\n",
      "395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0280, Val L1 Loss: 0.0280, PSNR_MASK: 29.0408, SSIM: 0.9130, LPIPS: 0.0389\n",
      "400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0276, Val L1 Loss: 0.0276, PSNR_MASK: 29.0966, SSIM: 0.9130, LPIPS: 0.0390\n",
      "405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0277, Val L1 Loss: 0.0277, PSNR_MASK: 29.0700, SSIM: 0.9131, LPIPS: 0.0390\n",
      "410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0278, Val L1 Loss: 0.0278, PSNR_MASK: 29.1101, SSIM: 0.9136, LPIPS: 0.0390\n",
      "415\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0279, Val L1 Loss: 0.0279, PSNR_MASK: 29.0901, SSIM: 0.9140, LPIPS: 0.0388\n",
      "420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0278, Val L1 Loss: 0.0278, PSNR_MASK: 29.0810, SSIM: 0.9142, LPIPS: 0.0388\n",
      "425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0279, Val L1 Loss: 0.0279, PSNR_MASK: 29.0340, SSIM: 0.9127, LPIPS: 0.0392\n",
      "430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0278, Val L1 Loss: 0.0278, PSNR_MASK: 29.0356, SSIM: 0.9133, LPIPS: 0.0390\n",
      "435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0276, Val L1 Loss: 0.0276, PSNR_MASK: 29.0890, SSIM: 0.9136, LPIPS: 0.0387\n",
      "440\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0279, Val L1 Loss: 0.0279, PSNR_MASK: 29.0021, SSIM: 0.9124, LPIPS: 0.0391\n",
      "445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0276, Val L1 Loss: 0.0276, PSNR_MASK: 29.0824, SSIM: 0.9130, LPIPS: 0.0389\n",
      "450\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0278, Val L1 Loss: 0.0278, PSNR_MASK: 29.0371, SSIM: 0.9132, LPIPS: 0.0389\n",
      "455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0278, Val L1 Loss: 0.0278, PSNR_MASK: 29.0427, SSIM: 0.9128, LPIPS: 0.0391\n",
      "460\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0280, Val L1 Loss: 0.0280, PSNR_MASK: 28.9985, SSIM: 0.9127, LPIPS: 0.0391\n",
      "465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0277, Val L1 Loss: 0.0277, PSNR_MASK: 29.0430, SSIM: 0.9122, LPIPS: 0.0391\n",
      "470\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0279, Val L1 Loss: 0.0279, PSNR_MASK: 29.0200, SSIM: 0.9120, LPIPS: 0.0392\n",
      "475\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0278, Val L1 Loss: 0.0278, PSNR_MASK: 29.0486, SSIM: 0.9128, LPIPS: 0.0389\n",
      "480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0278, Val L1 Loss: 0.0278, PSNR_MASK: 29.0335, SSIM: 0.9123, LPIPS: 0.0391\n",
      "485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0278, Val L1 Loss: 0.0278, PSNR_MASK: 29.0323, SSIM: 0.9125, LPIPS: 0.0391\n",
      "490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0278, Val L1 Loss: 0.0278, PSNR_MASK: 29.0316, SSIM: 0.9125, LPIPS: 0.0391\n",
      "495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0278, Val L1 Loss: 0.0278, PSNR_MASK: 29.0314, SSIM: 0.9125, LPIPS: 0.0391\n",
      "500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validation: 100%|██████████| 68/68 [01:29<00:00,  1.32s/batch]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Val G Loss: 0.0279, Val L1 Loss: 0.0279, PSNR_MASK: 29.0268, SSIM: 0.9125, LPIPS: 0.0390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Validation code\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from tqdm import tqdm\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
    "from lpips import LPIPS\n",
    "import torchvision.models as models\n",
    "\n",
    "# Constants\n",
    "LEARNING_RATE = 0.001\n",
    "NUM_EPOCHS = 500\n",
    "SAVE_INTERVAL = 5\n",
    "START_EPOCH = 0  # Start from epoch 80\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#G1.load_state_dict(torch.load(r\"C:\\Users\\nasef\\Desktop\\SBSR\\Models\\SBSR32\\SBSR30.pth\"))\n",
    "\n",
    "\n",
    "optimizer_G = optim.Adam(G1.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999))\n",
    "\n",
    "# Add the cosine annealing scheduler\n",
    "scheduler_G = optim.lr_scheduler.CosineAnnealingLR(optimizer_G, T_max=NUM_EPOCHS - START_EPOCH, eta_min=0.000001)\n",
    "\n",
    "# Loss functions\n",
    "l1_loss = nn.L1Loss()\n",
    "mse_loss = nn.MSELoss()\n",
    "lpips_loss_fn = LPIPS(net='alex').to(DEVICE)  # Using AlexNet for LPIPS loss\n",
    "\n",
    "# Define the VGG loss\n",
    "class VGGLoss(nn.Module):\n",
    "    def __init__(self, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "        super(VGGLoss, self).__init__()\n",
    "        vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features.to(device).eval()  # Move VGG layers to device and set to eval mode\n",
    "        self.vgg_layers = nn.Sequential(*list(vgg)[:36]).to(device)\n",
    "        self.device = device\n",
    "        for param in self.vgg_layers.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x, y):\n",
    "        x = x.to(self.device)\n",
    "        y = y.to(self.device)\n",
    "        x_vgg = self.vgg_layers(x)\n",
    "        y_vgg = self.vgg_layers(y)\n",
    "        loss = nn.functional.l1_loss(x_vgg, y_vgg)\n",
    "        return loss\n",
    "\n",
    "# Define Focal Frequency Loss\n",
    "class FocalFrequencyLoss(nn.Module):\n",
    "    def __init__(self, alpha=1.0, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
    "        super(FocalFrequencyLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.device = device\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input = input.to(self.device)\n",
    "        target = target.to(self.device)\n",
    "        input_fft = torch.fft.fft2(input)\n",
    "        target_fft = torch.fft.fft2(target)\n",
    "        diff = input_fft - target_fft\n",
    "        abs_diff = torch.abs(diff)\n",
    "        loss = torch.pow(abs_diff, self.alpha)\n",
    "        return torch.mean(loss)\n",
    "\n",
    "# Loss weights\n",
    "lambda_vgg = 0.01\n",
    "lambda_ff = 0.1\n",
    "\n",
    "# Combined loss\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, lambda_vgg, lambda_ff, device=DEVICE):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.lambda_vgg = lambda_vgg\n",
    "        self.lambda_ff = lambda_ff\n",
    "        self.vgg_loss = VGGLoss(device=device)\n",
    "        self.ff_loss = FocalFrequencyLoss(device=device)\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        l1 = l1_loss(input, target)\n",
    "        vgg = self.vgg_loss(input, target)\n",
    "        ff = self.ff_loss(input, target)\n",
    "        return l1 + self.lambda_vgg * vgg + self.lambda_ff * ff\n",
    "\n",
    "# Example usage\n",
    "combined_loss = CombinedLoss(lambda_vgg=lambda_vgg, lambda_ff=lambda_ff, device=DEVICE)\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "lpips_loss_fn = LPIPS(net='alex').to(DEVICE)  # Using AlexNet for LPIPS loss\n",
    "G1 = RAMiT_FFAT(target_mode='light_realdn',img_norm=False, mv_ver = 3)\n",
    "# Validation and Checkpoints\n",
    "G1.eval()\n",
    "e = 5\n",
    "while e <= 500:\n",
    "    G1 = RAMiT_FFAT(target_mode='light_realdn',img_norm=False, mv_ver = 3)\n",
    "    print(e)\n",
    "    from collections import OrderedDict\n",
    "    G1 = G1.to('cuda')\n",
    "    # Load checkpoint and create new state_dict with 'module.' prefix if needed\n",
    "    checkpoint = torch.load(f'C:/Users/Admin/Desktop/AI_RAMiT/FFAT_RAMiT/SBSR{e}.pth', map_location='cuda')\n",
    "    state_dict = checkpoint['state_dict'] if 'state_dict' in checkpoint else checkpoint\n",
    "    new_state_dict = OrderedDict()\n",
    "\n",
    "    # Add prefix 'module.' if needed\n",
    "    new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
    "\n",
    "    # Apply DataParallel and move to GPU\n",
    "    G1.load_state_dict(new_state_dict)\n",
    "    G1 = torch.nn.DataParallel(G1)\n",
    "    with torch.no_grad():\n",
    "        val_g_loss = 0.0\n",
    "        val_l1_loss = 0.0\n",
    "        val_ssim_loss = 0.0\n",
    "        val_lpips_loss = 0.0\n",
    "        val_psnr_mask = 0.0\n",
    "        m = 0\n",
    "\n",
    "        with tqdm(total=len(val_loader), desc=f'Validation', unit='batch') as pbar_val:\n",
    "            for i, (real_shadow, real_free) in enumerate(val_loader):\n",
    "                real_shadow = real_shadow.to(DEVICE)\n",
    "                real_free = real_free.to(DEVICE)\n",
    "\n",
    "                pred_free = G1(real_shadow)\n",
    "                # Generator loss\n",
    "                g1_loss = l1_loss(pred_free, real_free)\n",
    "                g_loss = g1_loss\n",
    "\n",
    "                val_g_loss += g_loss.item()\n",
    "                val_l1_loss += g1_loss.item()\n",
    "\n",
    "                # Calculate SSIM and LPIPS\n",
    "                pred_free_np = pred_free.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "                real_free_np = real_free.cpu().numpy().transpose(0, 2, 3, 1)\n",
    "\n",
    "                lpips_val = lpips_loss_fn(pred_free, real_free).mean().item()  # Average LPIPS over batch\n",
    "                val_lpips_loss += lpips_val\n",
    "\n",
    "                # Calculate PSNR for final shadow-free images\n",
    "                for j in range(real_free_np.shape[0]):\n",
    "                    val_psnr_mask += psnr(real_free_np[j], pred_free_np[j], data_range=1)\n",
    "                    val_ssim_loss += ssim(real_free_np[j], pred_free_np[j], data_range=1, channel_axis=-1)\n",
    "\n",
    "                pbar_val.update(1)\n",
    "                # save_image(pred_free, os.path.join(sample_dir, f'pred_free_epoch_{m}.png'))\n",
    "                m += 1\n",
    "\n",
    "        # Averaging over the entire validation set\n",
    "        val_g_loss /= len(val_loader)\n",
    "        val_l1_loss /= len(val_loader)\n",
    "        val_ssim_loss /= len(val_loader.dataset)\n",
    "        val_lpips_loss /= len(val_loader)\n",
    "        val_psnr_mask /= len(val_loader.dataset)\n",
    "\n",
    "        print(f\"Validation \"\n",
    "            f\"Val G Loss: {val_g_loss:.4f}, Val L1 Loss: {val_l1_loss:.4f}, \"\n",
    "            f\"PSNR_MASK: {val_psnr_mask:.4f}, SSIM: {val_ssim_loss:.4f}, LPIPS: {val_lpips_loss:.4f}\")\n",
    "        e = e+5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "EZfDgQc07rAV"
   },
   "outputs": [],
   "source": [
    "# from google.colab import runtime\n",
    "# runtime.unassign()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "dD_orSeWrrc0"
   },
   "outputs": [],
   "source": [
    "# top_output, top_inputs, top_targets, low_output, low_inputs, low_targets, tev_out, tev_inputs, tev_targets, output_l, targets_l, inputs_l = train(model, train_loader, 10)\n",
    "# # val_loss, e_out, total_psnr = evaluate(model, val_loader)\n",
    "# # val_ppl = math.exp(val_loss)\n",
    "# elapsed = time.time()\n",
    "# print('-' * 89)\n",
    "# print(f'time: {elapsed:5.2f}s ')#| psnr{total_psnr:5.2f} |'\n",
    "#             # f'valid loss {val_loss:5.2f}')\n",
    "# print('-' * 89)\n",
    "\n",
    "# import numpy as np\n",
    "\n",
    "# # scheduler.step()\n",
    "#  # load best model states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "PMm8jS-e0hGb"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torchvision import transforms\n",
    "# import matplotlib.pyplot as plt\n",
    "# from PIL import Image\n",
    "\n",
    "# # Assuming 'tensor' is your input tensor with shape [2, 3, 256, 256]\n",
    "# tensor_list = top_output  # Example tensor, replace with your actual tensor\n",
    "# ind = 0\n",
    "# # Denormalize the tensor\n",
    "# for tensor in tensor_list:\n",
    "#   tensor = tensor * 255  # Scale the tensor values to [0, 255]\n",
    "#   tensor = tensor.clamp(0, 255)  # Ensure the values are within [0, 255]\n",
    "#   tensor = tensor.byte()  # Convert to byte type\n",
    "#   tensor_list[ind] = tensor\n",
    "#   ind += 1\n",
    "\n",
    "# # Convert the tensor to a PIL image and display both images\n",
    "\n",
    "# to_pil_image = transforms.ToPILImage()\n",
    "\n",
    "# for tensor in tensor_list:\n",
    "#   for i in range(tensor.shape[0]):\n",
    "#     image = to_pil_image(tensor[0])\n",
    "#     plt.imshow(image)\n",
    "#     plt.axis('off')  # Hide the axes\n",
    "#     plt.show()\n",
    "# # for i in range(tensor.shape[0]):\n",
    "# #     image = to_pil_image(tensor[i])\n",
    "# #     plt.imshow(image)\n",
    "# #     plt.axis('off')  # Hide the axes\n",
    "# #     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "N3WkKuUJh_TL"
   },
   "outputs": [],
   "source": [
    "# tensor = top_inputs  # Example tensor, replace with your actual tensor\n",
    "\n",
    "# # Denormalize the tensor\n",
    "# tensor = tensor * 255  # Scale the tensor values to [0, 255]\n",
    "# tensor = tensor.clamp(0, 255)  # Ensure the values are within [0, 255]\n",
    "# tensor = tensor.byte()  # Convert to byte type\n",
    "\n",
    "# # Convert the tensor to a PIL image and display both images\n",
    "# to_pil_image = transforms.ToPILImage()\n",
    "\n",
    "# for i in range(tensor.shape[0]):\n",
    "#     image = to_pil_image(tensor[i])\n",
    "#     plt.imshow(image)\n",
    "#     plt.axis('off')  # Hide the axes\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "zEaAb6w0iJnY"
   },
   "outputs": [],
   "source": [
    "# tensor = top_targets  # Example tensor, replace with your actual tensor\n",
    "\n",
    "# # Denormalize the tensor\n",
    "# tensor = tensor * 255  # Scale the tensor values to [0, 255]\n",
    "# tensor = tensor.clamp(0, 255)  # Ensure the values are within [0, 255]\n",
    "# tensor = tensor.byte()  # Convert to byte type\n",
    "\n",
    "# # Convert the tensor to a PIL image and display both images\n",
    "# to_pil_image = transforms.ToPILImage()\n",
    "\n",
    "# for i in range(tensor.shape[0]):\n",
    "#     image = to_pil_image(tensor[i])\n",
    "#     plt.imshow(image)\n",
    "#     plt.axis('off')  # Hide the axes\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "1tGKPM2Qmu-U"
   },
   "outputs": [],
   "source": [
    "# # import torch\n",
    "# # from torchvision import transforms\n",
    "# # import matplotlib.pyplot as plt\n",
    "# # from PIL import Image\n",
    "\n",
    "# # Assuming 'tensor' is your input tensor with shape [1, 3, 256, 256]\n",
    "# # tensor = e_out  # Example tensor, replace with your actual tensor\n",
    "\n",
    "# # # Remove the batch dimension\n",
    "# # tensor = tensor.squeeze(0)  # Now the shape is [3, 256, 256]\n",
    "\n",
    "# # # Convert the tensor to a PIL image\n",
    "# # to_pil_image = transforms.ToPILImage()\n",
    "# # image = to_pil_image(tensor)\n",
    "\n",
    "# # # Display the image\n",
    "# # plt.imshow(image)\n",
    "# # plt.axis('off')  # Hide the axes\n",
    "# # plt.show()\n",
    "\n",
    "\n",
    "# import torch\n",
    "# from torchvision import transforms\n",
    "# import matplotlib.pyplot as plt\n",
    "# from PIL import Image\n",
    "\n",
    "# # Assuming 'tensor' is your input tensor with shape [2, 3, 256, 256]\n",
    "# tensor = e_out  # Example tensor, replace with your actual tensor\n",
    "\n",
    "# # Denormalize the tensor\n",
    "# tensor = tensor * 255  # Scale the tensor values to [0, 255]\n",
    "# tensor = tensor.clamp(0, 255)  # Ensure the values are within [0, 255]\n",
    "# tensor = tensor.byte()  # Convert to byte type\n",
    "\n",
    "# # Convert the tensor to a PIL image and display both images\n",
    "# to_pil_image = transforms.ToPILImage()\n",
    "\n",
    "# for i in range(tensor.shape[0]):\n",
    "#     image = to_pil_image(tensor[0])\n",
    "#     plt.imshow(image)\n",
    "#     plt.axis('off')  # Hide the axes\n",
    "#     plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "FKTfTBRTTz4T",
    "DYsCKqC6bg5c",
    "typm68nYiiGa",
    "yItTYYBeUIfS",
    "iyo1v8YvoqO1",
    "kJ5hGAuX9Lr3"
   ],
   "gpuType": "L4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "ramit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
