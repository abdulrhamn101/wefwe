{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! git clone https://github.com/rami0205/RAMiT.git"
      ],
      "metadata": {
        "id": "KXfes4o2s4Mr",
        "outputId": "4005a6b6-1537-4461-80eb-c14dc188d2ba",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'RAMiT' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyo1v8YvoqO1"
      },
      "source": [
        "## FFAT + RAMiT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "H8f_8E37e9uA"
      },
      "outputs": [],
      "source": [
        "# # RAMiT (Reciprocal Attention Mixing Transformer)\n",
        "\n",
        "# from pdb import set_trace as stx\n",
        "# import numbers\n",
        "# from saicinpainting.training.modules.ffc0 import FFC_BN_ACT\n",
        "# from skimage import io\n",
        "# from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# from torch import nn, Tensor\n",
        "# from einops import rearrange\n",
        "# import torch.nn.functional as F\n",
        "# import torch\n",
        "# from torchvision.transforms import functional as TF\n",
        "# import math\n",
        "# from RAMiT.my_model.common.mean_std import mean_std\n",
        "# from RAMiT.utils.etc_utils import denormalize\n",
        "# from timm.models.fx_features import register_notrace_function\n",
        "# from timm.models.layers import trunc_normal_, to_2tuple\n",
        "# from torch.nn.modules.batchnorm import _BatchNorm\n",
        "# def make_model(args, opts, pe):\n",
        "#     model = RAMiT(target_mode=args.target_mode,\n",
        "#                   img_norm=args.img_norm,\n",
        "#                   in_chans=opts['in_chans'],\n",
        "#                   dim = opts['dim'],\n",
        "#                   depths = opts['depths'],\n",
        "#                   num_heads = opts['num_heads'],\n",
        "#                   head_dim = opts['head_dim'],\n",
        "#                   chsa_head_ratio = opts['chsa_head_ratio'],\n",
        "#                   window_size = opts['window_size'],\n",
        "#                   hidden_ratio = opts['hidden_ratio'],\n",
        "#                   qkv_bias = opts['qkv_bias'],\n",
        "#                   mv_ver = opts['mv_ver'],\n",
        "#                   exp_factor=opts['exp_factor'],\n",
        "#                   expand_groups=opts['expand_groups'],\n",
        "#                   act_layer = opts['act_layer'],\n",
        "#                   norm_layer = opts['norm_layer'],\n",
        "#                   tail_mv = opts['tail_mv'],\n",
        "#                   attn_drop = opts['attn_drop'],\n",
        "#                   proj_drop = opts['proj_drop'],\n",
        "#                   drop_path = opts['drop_path'],\n",
        "#                   helper = opts['helper'],\n",
        "#                   mv_act = opts['mv_act']\n",
        "#                  )\n",
        "#     num_params = sum([p.numel() for n,p in model.named_parameters()])\n",
        "#     print(format(num_params, ','))\n",
        "#     return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# class ShallowModule(nn.Module):\n",
        "#     def __init__(self, in_chans, out_chans, kernel_size=3, stride=1):\n",
        "#         super(ShallowModule, self).__init__()\n",
        "\n",
        "#         self.in_chans = in_chans\n",
        "#         self.out_chans = out_chans\n",
        "#         self.kernel_size = kernel_size\n",
        "#         self.stride = stride\n",
        "\n",
        "#         self.conv = nn.Conv2d(in_chans, out_chans, kernel_size, stride, kernel_size//2)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.conv(x)\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         return resolutions[0]*resolutions[1] * self.kernel_size*self.kernel_size * self.in_chans * self.out_chans\n",
        "\n",
        "# class QKVProjection(nn.Module):\n",
        "#     def __init__(self, dim, num_head, qkv_bias=True):\n",
        "#         super(QKVProjection, self).__init__()\n",
        "#         self.dim = dim\n",
        "#         self.num_head = num_head\n",
        "\n",
        "#         self.qkv = nn.Conv2d(dim, 3*dim, 1, bias=qkv_bias)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         B, C, H, W = x.size()\n",
        "#         qkv = self.qkv(x)\n",
        "#         qkv = rearrange(qkv, 'b (l c) h w -> b l c h w', l=self.num_head)\n",
        "#         return qkv\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         return resolutions[0]*resolutions[1] * 1*1 * self.dim * 3*self.dim\n",
        "\n",
        "# def get_relative_position_index(win_h, win_w):\n",
        "#     # get pair-wise relative position index for each token inside the window\n",
        "#     coords = torch.stack(torch.meshgrid([torch.arange(win_h), torch.arange(win_w)]))  # 2, Wh, Ww\n",
        "#     coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
        "#     relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww (xaxis matrix & yaxis matrix)\n",
        "#     relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
        "#     relative_coords[:, :, 0] += win_h - 1  # shift to start from 0\n",
        "#     relative_coords[:, :, 1] += win_w - 1\n",
        "#     relative_coords[:, :, 0] *= 2 * win_w - 1\n",
        "#     return relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
        "\n",
        "# class SpatialSelfAttention(nn.Module):\n",
        "#     def __init__(self, dim, num_head, total_head, window_size=8, shift=0, attn_drop=0.0, proj_drop=0.0, helper=True):\n",
        "#         super(SpatialSelfAttention, self).__init__()\n",
        "#         self.dim = dim\n",
        "#         self.num_head = num_head\n",
        "#         self.total_head = total_head\n",
        "#         self.window_size = window_size\n",
        "#         self.window_area = window_size**2\n",
        "#         self.shift = shift\n",
        "#         self.helper = helper\n",
        "\n",
        "#         self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_head, 1, 1))), requires_grad=True)\n",
        "\n",
        "#         # define a parameter table of relative position bias, shape: 2*Wh-1 * 2*Ww-1, nH\n",
        "#         self.relative_position_bias_table = nn.Parameter(torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_head))\n",
        "\n",
        "#         # get pair-wise relative position index for each token inside the window\n",
        "#         self.register_buffer(\"relative_position_index\", get_relative_position_index(window_size, window_size))\n",
        "\n",
        "#         self.attn_drop = nn.Dropout(attn_drop)\n",
        "\n",
        "#         self.proj = nn.Conv2d(dim*num_head, dim*num_head, 1)\n",
        "#         self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "#     def _get_rel_pos_bias(self) -> torch.Tensor:\n",
        "#         relative_position_bias = self.relative_position_bias_table[\n",
        "#             self.relative_position_index.view(-1)].view(self.window_area, self.window_area, -1)  # Wh*Ww,Wh*Ww,nH\n",
        "#         relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
        "#         return relative_position_bias.unsqueeze(0)\n",
        "\n",
        "#     def forward(self, qkv, ch=None):\n",
        "#         B, L, C, H, W = qkv.size()\n",
        "#         # window shift\n",
        "#         if self.shift > 0:\n",
        "#             qkv = torch.roll(qkv, shifts=(-self.shift, -self.shift), dims=(-2,-1))\n",
        "\n",
        "#         # window partition\n",
        "#         q,k,v = rearrange(qkv, 'b l c (h wh) (w ww) -> (b h w) l (wh ww) c',\n",
        "#                           wh=self.window_size, ww=self.window_size).chunk(3, dim=-1) # [B_, L1, hw, C/L] respectively\n",
        "#         if ch is not None and self.helper: # [B, C, H, W]\n",
        "#             if self.shift > 0:\n",
        "#                 ch = torch.roll(ch, shifts=(-self.shift, -self.shift), dims=(-2,-1))\n",
        "#             ch = rearrange(ch, 'b (l c) (h wh) (w ww) -> (b h w) l (wh ww) c',\n",
        "#                            l=self.total_head-self.num_head, wh=self.window_size, ww=self.window_size) # [B_, L1, hw, C/L]\n",
        "#             ch = torch.mean(ch, dim=1, keepdim=True) # head squeeze [B_, 1, hw, C/L]\n",
        "#             v = v*ch # [B_, L1, hw, C/L]\n",
        "\n",
        "#         attn = F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(2,-1) # [B_, L1, hw, hw]\n",
        "#         logit_scale = torch.clamp(self.logit_scale, max=math.log(1. / 0.01)).exp()\n",
        "#         attn = attn * logit_scale\n",
        "\n",
        "#         attn = attn + self._get_rel_pos_bias()\n",
        "#         attn = self.attn_drop(F.softmax(attn, dim=-1))\n",
        "\n",
        "#         x = attn @ v # [B_, L1, hw, C/L]\n",
        "\n",
        "#         # window unpartition + head merge\n",
        "#         x = window_unpartition(x, (H,W), self.window_size) # [B, L1*C/L, H, W]\n",
        "#         x = self.proj_drop(self.proj(x))\n",
        "\n",
        "#         # window reverse shift\n",
        "#         if self.shift > 0:\n",
        "#             x = torch.roll(x, shifts=(self.shift, self.shift), dims=(-2,-1))\n",
        "\n",
        "#         return x\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         H,W = resolutions\n",
        "#         num_wins = H//self.window_size * W//self.window_size\n",
        "#         flops = self.num_head * H*W * self.dim if self.helper else 0 # v = v*ch\n",
        "#         flops += num_wins * self.num_head * self.window_area * self.dim * self.window_area # attn = Q@K^T\n",
        "#         flops += num_wins * self.num_head * self.window_area * self.window_area * self.dim # attn@V\n",
        "#         flops += H*W * 1*1 * self.num_head*self.dim * self.num_head*self.dim # self.proj\n",
        "#         return flops\n",
        "\n",
        "# @register_notrace_function\n",
        "# def window_unpartition(x, resolutions, window_size):\n",
        "#     return rearrange(x, '(b h w) l (wh ww) c -> b (l c) (h wh) (w ww)',\n",
        "#                      h=resolutions[0]//window_size, w=resolutions[1]//window_size, wh=window_size)\n",
        "\n",
        "# class ChannelSelfAttention(nn.Module):\n",
        "#     def __init__(self, dim, num_head, total_head, attn_drop=0.0, proj_drop=0.0, helper=True):\n",
        "#         super(ChannelSelfAttention, self).__init__()\n",
        "#         self.dim = dim\n",
        "#         self.num_head = num_head\n",
        "#         self.total_head = total_head\n",
        "#         self.helper = helper\n",
        "\n",
        "#         self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_head, 1, 1))), requires_grad=True)\n",
        "\n",
        "#         self.attn_drop = nn.Dropout(attn_drop)\n",
        "\n",
        "#         self.proj = nn.Conv2d(dim*num_head, dim*num_head, 1)\n",
        "#         self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "#     def forward(self, qkv, sp=None):\n",
        "#         B, L, C, H, W = qkv.size()\n",
        "\n",
        "#         q,k,v = rearrange(qkv, 'b l c h w -> b l c (h w)').chunk(3, dim=-2) # [B, L2, C/L, HW]\n",
        "#         if sp is not None and self.helper:\n",
        "#             sp = torch.mean(sp, dim=1, keepdim=True) # channel squeeze # [B, 1, H, W]\n",
        "#             sp = rearrange(sp, 'b (l c) h w -> b l c (h w)', l=1) # [B, 1, 1, HW]\n",
        "#             v = v*sp # [B, L2, C/L, HW]\n",
        "\n",
        "#         attn = F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(2,-1) # [B, L2, C/L, C/L]\n",
        "#         logit_scale = torch.clamp(self.logit_scale, max=math.log(1. / 0.01)).exp()\n",
        "#         attn = attn * logit_scale\n",
        "\n",
        "#         attn = F.softmax(attn, dim=-1)\n",
        "#         attn = self.attn_drop(attn)\n",
        "\n",
        "#         x = attn @ v # [B, L2, C/L, HW]\n",
        "\n",
        "#         # head merge\n",
        "#         x = rearrange(x, 'b l c (h w) -> b (l c) h w', h=H) # [B, L2*C/L, H, W]\n",
        "#         x = self.proj_drop(self.proj(x)) # [B, L2*C/L, H, W]\n",
        "\n",
        "#         return x\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         H,W = resolutions\n",
        "#         flops = self.num_head * self.dim * H*W if self.helper else 0 # v = v*sp\n",
        "#         flops += self.num_head * self.dim * H*W * self.dim # attn = Q@K^T\n",
        "#         flops += self.num_head * self.dim * self.dim * H*W # attn@V\n",
        "#         flops += H*W * 1*1 * self.num_head*self.dim * self.num_head*self.dim # self.proj\n",
        "#         return flops\n",
        "\n",
        "# class ReshapeLayerNorm(nn.Module):\n",
        "#     def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
        "#         super(ReshapeLayerNorm, self).__init__()\n",
        "\n",
        "#         self.dim = dim\n",
        "#         self.norm = norm_layer(dim)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         B, C, H, W = x.size()\n",
        "#         x = rearrange(x, 'b c h w -> b (h w) c')\n",
        "#         x = self.norm(x)\n",
        "#         x = rearrange(x, 'b (h w) c -> b c h w', h=H)\n",
        "#         return x\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         H,W = resolutions\n",
        "#         flops = 0\n",
        "#         flops += H*W * self.dim\n",
        "#         return flops\n",
        "\n",
        "# class MobiVari1(nn.Module): # MobileNet v1 Variants\n",
        "#     def __init__(self, dim, kernel_size, stride, act=nn.LeakyReLU, out_dim=None):\n",
        "#         super(MobiVari1, self).__init__()\n",
        "#         self.dim = dim\n",
        "#         self.kernel_size = kernel_size\n",
        "#         self.out_dim = out_dim or dim\n",
        "\n",
        "#         self.dw_conv = nn.Conv2d(dim, dim, kernel_size, stride, kernel_size//2, groups=dim)\n",
        "#         self.pw_conv = nn.Conv2d(dim, self.out_dim, 1, 1, 0)\n",
        "#         self.act = act()\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = self.act(self.pw_conv(self.act(self.dw_conv(x))+x))\n",
        "#         return out + x if self.dim==self.out_dim else out\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         H,W = resolutions\n",
        "#         flops = H*W * self.kernel_size*self.kernel_size * self.dim  +  H*W * 1*1 * self.dim * self.out_dim # self.dw_conv + self.pw_conv\n",
        "#         return flops\n",
        "\n",
        "# class MobiVari2(MobiVari1): # MobileNet v2 Variants\n",
        "#     def __init__(self, dim, kernel_size, stride, act=nn.LeakyReLU, out_dim=None, exp_factor=1.2, expand_groups=4):\n",
        "#         super(MobiVari2, self).__init__(dim, kernel_size, stride, act, out_dim)\n",
        "#         self.expand_groups = expand_groups\n",
        "#         expand_dim = int(dim*exp_factor)\n",
        "#         expand_dim = expand_dim+(expand_groups-expand_dim%expand_groups)\n",
        "#         self.expand_dim = expand_dim\n",
        "\n",
        "#         self.exp_conv = nn.Conv2d(dim, self.expand_dim, 1, 1, 0, groups=expand_groups)\n",
        "#         self.dw_conv = nn.Conv2d(expand_dim, expand_dim, kernel_size, stride, kernel_size//2, groups=expand_dim)\n",
        "#         self.pw_conv = nn.Conv2d(expand_dim, self.out_dim, 1, 1, 0)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x1 = self.act(self.exp_conv(x))\n",
        "#         out = self.pw_conv(self.act(self.dw_conv(x1)+x1))\n",
        "#         return out + x if self.dim==self.out_dim else out\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         H,W = resolutions\n",
        "#         flops = H*W * 1*1 * (self.dim//self.expand_groups) * self.expand_dim # self.exp_conv\n",
        "#         flops += H*W * self.kernel_size*self.kernel_size * self.expand_dim # self.dw_conv\n",
        "#         flops += H*W * 1*1 * self.expand_dim * self.out_dim # self.pw_conv\n",
        "#         return flops\n",
        "\n",
        "# class FeedForward(nn.Module):\n",
        "#     def __init__(self, dim, hidden_ratio, act_layer=nn.GELU, bias=True, drop=0.0):\n",
        "#         super(FeedForward, self).__init__()\n",
        "\n",
        "#         self.dim = dim\n",
        "#         self.hidden_ratio = hidden_ratio\n",
        "\n",
        "#         self.hidden = nn.Conv2d(dim, int(dim*hidden_ratio), 1, bias=bias)\n",
        "#         self.drop1 = nn.Dropout(drop)\n",
        "#         self.out = nn.Conv2d(int(dim*hidden_ratio), dim, 1, bias=bias)\n",
        "#         self.drop2 = nn.Dropout(drop)\n",
        "#         self.act = act_layer()\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.drop2(self.out(self.drop1(self.act(self.hidden(x)))))\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         H,W = resolutions\n",
        "#         flops = 2 * H*W * 1*1 * self.dim * self.dim*self.hidden_ratio # self.hidden + self.out\n",
        "#         return flops\n",
        "\n",
        "# class NoLayer(nn.Identity):\n",
        "#     def __init__(self):\n",
        "#         super(NoLayer, self).__init__()\n",
        "#     def flops(self, resolutions):\n",
        "#         return 0\n",
        "#     def forward(self, x, **kwargs):\n",
        "#         return x.flatten(1,2)\n",
        "\n",
        "# class DRAMiTransformer(nn.Module): # Reciprocal Attention Transformer Block\n",
        "#     def __init__(self, dim, num_head, chsa_head_ratio, FFAT, window_size=8, shift=0, head_dim=None, qkv_bias=True, mv_ver=1,\n",
        "#                  hidden_ratio=2.0, act_layer=nn.GELU, norm_layer=ReshapeLayerNorm, attn_drop=0.0, proj_drop=0.0, drop_path=0.0, helper=True,\n",
        "#                  mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4,\n",
        "#                  num_blocks = [4,6,6,8], heads = [1,2,4,8], ffn_expansion_factor = 2.66, bias = False, LayerNorm_type = 'WithBias'):\n",
        "#         super(DRAMiTransformer, self).__init__()\n",
        "\n",
        "#         self.dim = dim\n",
        "#         self.num_head = num_head\n",
        "#         self.window_size = window_size\n",
        "#         self.chsa_head = int(num_head*chsa_head_ratio)\n",
        "#         self.shift = shift\n",
        "#         self.helper = helper\n",
        "#         self.FFAT = FFAT\n",
        "\n",
        "#         self.qkv_proj = QKVProjection(dim, num_head, qkv_bias=qkv_bias)\n",
        "#         self.sp_attn = SpatialSelfAttention(dim//num_head, num_head-self.chsa_head, num_head,\n",
        "#                                             window_size, shift, attn_drop, proj_drop, helper) if num_head-self.chsa_head != 0 else NoLayer()\n",
        "#         self.ch_attn = ChannelSelfAttention(dim//num_head, self.chsa_head, num_head, attn_drop, proj_drop, helper) if self.chsa_head != 0 else NoLayer()\n",
        "\n",
        "\n",
        "#         #FFAT\n",
        "#         if self.FFAT:\n",
        "#           self.encoder_level1 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
        "#         #FFAT\n",
        "\n",
        "\n",
        "#         if mv_ver==3:\n",
        "#             self.mobivari = MobiVari1(dim, 3, 1, act=mv_act)\n",
        "#         elif mv_ver==2:\n",
        "#             self.mobivari = MobiVari2(dim, 3, 1, act=mv_act, out_dim=None, exp_factor=exp_factor, expand_groups=expand_groups)\n",
        "#             # NAFBlock---------------------------------------------\n",
        "#         elif mv_ver==1:\n",
        "#             self.mobivari = NAFBlock(c=self.dim)\n",
        "#             # -------------------------------------------------------\n",
        "\n",
        "#         self.norm1 = norm_layer(dim)\n",
        "\n",
        "#         self.ffn = FeedForward(dim, hidden_ratio, act_layer=act_layer)\n",
        "#         self.norm2 = norm_layer(dim)\n",
        "\n",
        "#         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "#     def forward(self, x, sp_=None, ch_=None):\n",
        "#         B, C, H, W = x.size()\n",
        "\n",
        "#         # QKV projection + head split\n",
        "#         qkv = self.qkv_proj(x) # [B, L, C, H, W]\n",
        "\n",
        "#         # SP-SA / CH-SA\n",
        "#         sp = self.sp_attn(qkv[:,:self.num_head-self.chsa_head], ch=ch_) # [B, L1*C/L, H, W]\n",
        "#         ch = self.ch_attn(qkv[:,self.num_head-self.chsa_head:], sp=sp_) # [B, L2*C/L, H, W]\n",
        "\n",
        "\n",
        "#         #FFAT\n",
        "#         if self.FFAT:\n",
        "#           FFAT_block = self.encoder_level1(torch.cat([sp, ch], dim=1))\n",
        "#           attn0 = self.mobivari(FFAT_block)\n",
        "#         #FFAT\n",
        "\n",
        "\n",
        "#         else:\n",
        "#           attn0 = self.mobivari(torch.cat([sp, ch], dim=1)) # merge [B, C, H, W]\n",
        "#         attn = self.drop_path(self.norm1(attn0)) + x # LN, skip connection [B, C, H, W]\n",
        "\n",
        "#         # FFN\n",
        "#         out = self.drop_path(self.norm2(self.ffn(attn))) + attn # FFN, LN, skip connection [B, C, H, W]\n",
        "\n",
        "#         return out, sp, ch, attn0\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         flops = self.qkv_proj.flops(resolutions)\n",
        "#         flops += self.sp_attn.flops(resolutions)\n",
        "#         flops += self.ch_attn.flops(resolutions)\n",
        "#         flops += self.mobivari.flops(resolutions)\n",
        "#         flops += self.norm1.flops(resolutions)\n",
        "#         flops += self.ffn.flops(resolutions)\n",
        "#         flops += self.norm2.flops(resolutions)\n",
        "#         params = sum([p.numel() for n,p in self.named_parameters()])\n",
        "#         return flops\n",
        "\n",
        "# class EncoderStage(nn.Module):\n",
        "#     def __init__(self, depth, dim, num_head, chsa_head_ratio, FFAT, window_size=8, head_dim=None,\n",
        "#                  qkv_bias=True, mv_ver=1, hidden_ratio=2.0, act_layer=nn.GELU, norm_layer=ReshapeLayerNorm,\n",
        "#                  attn_drop=0.0, proj_drop=0.0, drop_path=0.0, helper=True, mv_act=nn.LeakyReLU,):\n",
        "#         super(EncoderStage, self).__init__()\n",
        "\n",
        "#         self.depth = depth\n",
        "#         self.dim = dim\n",
        "#         self.num_head = num_head\n",
        "#         self.window_size = window_size\n",
        "#         shift = window_size//2\n",
        "\n",
        "#         self.blocks = nn.ModuleList()\n",
        "#         for d in range(depth):\n",
        "#             self.blocks.add_module(f'block{d}', DRAMiTransformer(dim, num_head, chsa_head_ratio, FFAT,\n",
        "#                                                                  window_size, 0 if d%2==0 else shift, head_dim, qkv_bias,\n",
        "#                                                                  mv_ver, hidden_ratio, act_layer, norm_layer,\n",
        "#                                                                  attn_drop, proj_drop, drop_path, helper, mv_act,\n",
        "#                                                                  ))\n",
        "#     def forward(self, x):\n",
        "#         sp, ch = None, None\n",
        "#         for i, blk in enumerate(self.blocks):\n",
        "#             x, sp, ch, attn = blk(x, sp, ch)\n",
        "#             sp, ch = (None, None) if (sp.size(1)==0 or ch.size(1)==0) else (sp, ch) # pure SPSA or CHSA\n",
        "#         return x, attn\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         flops = 0\n",
        "#         for blk in self.blocks:\n",
        "#             flops += blk.flops(resolutions)\n",
        "#         return flops\n",
        "\n",
        "# class Downsizing(nn.Module):\n",
        "#     \"\"\" Patch Merging Layer.\n",
        "\n",
        "#     Args:\n",
        "#         dim (int): Number of input dimension (channels).\n",
        "#         downsample_dim (int, optional): Number of output dimension (channels) (dim if not set).  Default: None\n",
        "#         norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, dim, downsample_dim=None, norm_layer=ReshapeLayerNorm, mv_ver=1, mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4):\n",
        "#         super(Downsizing, self).__init__()\n",
        "#         self.dim = dim\n",
        "#         self.downsample_dim = downsample_dim or dim\n",
        "#         self.norm = norm_layer(4*dim)\n",
        "#         if mv_ver==3:\n",
        "#             self.reduction = MobiVari1(4*dim, 3, 1, act=mv_act, out_dim=self.downsample_dim)\n",
        "#         elif mv_ver==2:\n",
        "#             self.reduction = MobiVari2(4*dim, 3, 1, act=mv_act, out_dim=self.downsample_dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
        "\n",
        "#             # NAFBlock in not accepted\n",
        "#             # NAFBlock---------------------------------------------\n",
        "#         elif mv_ver==1:\n",
        "#             self.mobivari = NAFBlock(c=self.dim)\n",
        "#             # -------------------------------------------------------\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         B, C, H, W = x.size()\n",
        "\n",
        "#         # Concat 2x2\n",
        "#         x0 = x[:, :, 0::2, 0::2]  # [B, C, H/2, W/2], top-left\n",
        "#         x1 = x[:, :, 0::2, 1::2]  # [B, C, H/2, W/2], top-right\n",
        "#         x2 = x[:, :, 1::2, 0::2]  # [B, C, H/2, W/2], bottom-left\n",
        "#         x3 = x[:, :, 1::2, 1::2]  # [B, C, H/2, W/2], bottom-right\n",
        "#         x = torch.cat([x0, x1, x2, x3], dim=1)  # [B, 4C, H/2, W/2]\n",
        "#         return self.reduction(self.norm(x)) # [B, C, H/2, W/2]\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         H,W = resolutions\n",
        "#         flops = self.norm.flops((H//2,W//2)) + self.reduction.flops((H//2,W//2))\n",
        "#         return flops\n",
        "\n",
        "# class Bottleneck(nn.Module):\n",
        "#     def __init__(self, dim, num_stages, act_layer=nn.GELU, norm_layer=ReshapeLayerNorm, mv_ver=1, mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4):\n",
        "#         super(Bottleneck, self).__init__()\n",
        "\n",
        "#         self.dim = dim\n",
        "#         self.cat_dim = sum([dim//(4**x) for x in range(num_stages)])\n",
        "#         if mv_ver==1:\n",
        "#             self.mobivari = MobiVari1(self.cat_dim, 3, 1, act=mv_act, out_dim=dim)\n",
        "#         elif mv_ver==2:\n",
        "#             self.mobivari = MobiVari2(self.cat_dim, 3, 1, act=mv_act, out_dim=dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
        "\n",
        "#             # NAFBlock---------------------------------------------\n",
        "#         elif mv_ver==3:\n",
        "#             self.mobivari = NAFBlock(c=self.dim)\n",
        "#             # -------------------------------------------------------\n",
        "#         self.act = act_layer()\n",
        "#         self.norm = norm_layer(dim)\n",
        "\n",
        "#     def shallow_down(self, x, r):\n",
        "#         for _ in range(r):\n",
        "#             x = F.max_pool2d(x, 2)\n",
        "#         return F.leaky_relu(x)\n",
        "\n",
        "#     def forward(self, x_list):\n",
        "#         xs = x_list[0]\n",
        "#         new_x = []\n",
        "#         for i in range(len(x_list[1:])):\n",
        "#             x_ = x_list[i+1]+self.shallow_down(xs, i)\n",
        "#             new_x.append(F.pixel_shuffle(x_, 2**i))\n",
        "#         new_x = self.norm(self.mobivari(torch.cat(new_x, dim=1)))\n",
        "#         return new_x\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         H,W = resolutions\n",
        "#         flops = 0\n",
        "#         # self.shallow_down (iterative max-pool)\n",
        "#         flops += (H//2)*(W//2) * 2*2 * self.dim # shallow-down into stage2 output (iter1)\n",
        "#         flops += (H//2)*(W//2) * 2*2 * self.dim # shallow-down into stage3 output (iter1)\n",
        "#         flops += (H//4)*(W//4) * 2*2 * self.dim # shallow-down into stage3 output (iter2)\n",
        "#         flops += self.mobivari.flops((H,W))\n",
        "#         flops += self.norm.flops((H,W))\n",
        "#         return flops\n",
        "\n",
        "# class HRAMi(nn.Module):\n",
        "#     def __init__(self, dim, kernel_size=3, stride=1, mv_ver=1, mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4):\n",
        "#         super(HRAMi, self).__init__()\n",
        "\n",
        "#         self.dim = dim\n",
        "#         self.kernel_size = kernel_size\n",
        "\n",
        "#         if mv_ver==1:\n",
        "#             self.mobivari = MobiVari1(dim+dim//4+dim//16+dim, kernel_size, stride, act=mv_act, out_dim=dim)\n",
        "#         elif mv_ver==2:\n",
        "#             self.mobivari = MobiVari2(dim+dim//4+dim//16+dim, kernel_size, stride, act=mv_act, out_dim=dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
        "#             # NAFBlock---------------------------------------------\n",
        "#         elif mv_ver==3:\n",
        "#             self.mobivari = NAFBlock(c=self.dim)\n",
        "#             # -------------------------------------------------------\n",
        "\n",
        "#     def forward(self, attn_list):\n",
        "#         for i, attn in enumerate(attn_list[:-1]):\n",
        "#             attn = F.pixel_shuffle(attn, 2**i)\n",
        "#             x = attn if i==0 else torch.cat([x, attn], dim=1)\n",
        "#         x = torch.cat([x, attn_list[-1]], dim=1)\n",
        "#         x = self.mobivari(x)\n",
        "#         return x\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         return self.mobivari.flops(resolutions)\n",
        "\n",
        "# class Reconstruction(nn.Module):\n",
        "#     def __init__(self, target_mode, dim, kernel_size=3, stride=1, num_mv=2, mv_ver=1, mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4):\n",
        "#         super(Reconstruction, self).__init__()\n",
        "\n",
        "#         self.upscale = upscale = int(target_mode[-1]) if target_mode in ['light_x2', 'light_x3', 'light_x4'] else 1\n",
        "#         self.target_mode = target_mode\n",
        "#         self.dim = dim\n",
        "#         self.out_chans = 3 if 'gray' not in target_mode else 1\n",
        "#         self.kernel_size = kernel_size\n",
        "#         self.stride = stride\n",
        "#         self.num_mv = num_mv\n",
        "\n",
        "#         self.mobivari = nn.ModuleList()\n",
        "#         for i in range(num_mv):\n",
        "#             if mv_ver==3:\n",
        "#                 self.mobivari.add_module(f'mobivari{i}', MobiVari1(dim, kernel_size, stride, mv_act))\n",
        "#             elif mv_ver==2:\n",
        "#                 self.mobivari.add_module(f'mobivari{i}', MobiVari2(dim, kernel_size, stride, mv_act, None, exp_factor, expand_groups))\n",
        "#                 # NAFBlock---------------------------------------------\n",
        "#             elif mv_ver==1:\n",
        "#                  self.mobivari.add_module(f'mobivari{i}',NAFBlock(c=self.dim))\n",
        "#                 # -------------------------------------------------------\n",
        "\n",
        "\n",
        "#         self.conv = nn.Conv2d(dim, self.out_chans*(upscale**2), kernel_size, stride, kernel_size//2)\n",
        "#         self.shuffle = nn.PixelShuffle(upscale) if upscale!=1 else nn.Identity()\n",
        "#         self.final_conv = nn.Conv2d(self.out_chans, self.out_chans, kernel_size, stride, kernel_size//2)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         for mobivari in self.mobivari:\n",
        "#             x = mobivari(x)\n",
        "#         return self.final_conv(self.shuffle(self.conv(x)))\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         H,W = resolutions\n",
        "#         flops = 0\n",
        "#         for mobivari in self.mobivari: # self.mobivari\n",
        "#             flops += mobivari.flops((H,W))\n",
        "#         flops += H*W * self.kernel_size*self.kernel_size * self.dim * self.out_chans*(self.upscale**2) # self.conv\n",
        "#         flops += H*W * self.kernel_size*self.kernel_size * self.out_chans * self.out_chans # self.final_conv\n",
        "#         return flops\n",
        "\n",
        "# class RAMiT_FFAT(nn.Module):\n",
        "#     def __init__(self, in_chans=3, dim=64, depths=(1,4,4,6), num_heads=(4,4,4,4), head_dim=None, chsa_head_ratio=0.25, #depths = (6,4,4,6)\n",
        "#                  window_size=8, hidden_ratio=2.0, qkv_bias=True, mv_ver=1, exp_factor=1.2, expand_groups=4,\n",
        "#                  act_layer=nn.GELU, norm_layer=ReshapeLayerNorm, tail_mv=2, weight_init='', target_mode='light_x2', img_norm=True,\n",
        "#                  attn_drop=0.0, proj_drop=0.0, drop_path=0.0, helper=True, mv_act=nn.LeakyReLU,\n",
        "#                  num_blocks = [1,4,6,6,8],\n",
        "#                  heads = [1,2,4,8],\n",
        "#                  ffn_expansion_factor = 2.66,\n",
        "#                  bias = False,\n",
        "#                  LayerNorm_type = 'WithBias',   ## Other option 'BiasFree'\n",
        "#                  FFAT = False, **kwargs):\n",
        "#         super(RAMiT_FFAT, self).__init__()\n",
        "#         self.cat_dim = sum([dim//(4**x) for x in range(len(depths)-1)]) # added to use the torchinfor.summary\n",
        "#         self.unit = 2**(len(depths)-2)*window_size\n",
        "\n",
        "#         self.in_chans = in_chans\n",
        "#         self.dim = dim\n",
        "#         self.depths = depths\n",
        "#         self.num_heads = num_heads\n",
        "#         self.head_dim = head_dim\n",
        "#         self.window_size = window_size\n",
        "#         self.hidden_ratio = hidden_ratio\n",
        "#         self.qkv_bias = qkv_bias\n",
        "#         self.act_layer = act_layer\n",
        "#         norm_layer = ReshapeLayerNorm if norm_layer == 'ReshapeLayerNorm' else norm_layer\n",
        "#         self.norm_layer = norm_layer = ReshapeLayerNorm if norm_layer == 'ReshapeLayerNorm' else norm_layer\n",
        "#         self.tail_mv = tail_mv\n",
        "\n",
        "#         self.scale = int(target_mode[-1]) if target_mode in ['light_x2', 'light_x3', 'light_x4'] else 1\n",
        "#         self.mean, self.std = mean_std(self.scale, target_mode)\n",
        "#         self.target_mode = target_mode\n",
        "#         self.img_norm = img_norm\n",
        "\n",
        "#         dpr = [x.item() for x in torch.linspace(0, drop_path, sum(depths))]  # stochastic depth decay rule\n",
        "\n",
        "#         self.shallow = ShallowModule(in_chans, dim, 3, 1)\n",
        "#         self.stage1 = EncoderStage(depths[0], dim, num_heads[0], chsa_head_ratio, FFAT, window_size, head_dim, qkv_bias, mv_ver,\n",
        "#                                    hidden_ratio, act_layer, norm_layer, attn_drop, proj_drop, drop_path, helper, mv_act)\n",
        "\n",
        "\n",
        "#         # FFAT\n",
        "#         self.encoder_level1 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
        "\n",
        "#         self.encoder_level2 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
        "\n",
        "#         self.encoder_level3 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
        "#         # FFAT\n",
        "\n",
        "\n",
        "#         self.down1 = Downsizing(dim, dim, norm_layer, mv_ver, mv_act)\n",
        "#         self.stage2 = EncoderStage(depths[0], dim, num_heads[1], chsa_head_ratio, FFAT, window_size, head_dim, qkv_bias, mv_ver,\n",
        "#                                    hidden_ratio, act_layer, norm_layer, attn_drop, proj_drop, drop_path, helper, mv_act)\n",
        "#         self.down2 = Downsizing(dim, dim, norm_layer, mv_ver, mv_act)\n",
        "#         self.stage3 = EncoderStage(depths[0], dim, num_heads[2], chsa_head_ratio, FFAT, window_size, head_dim, qkv_bias, mv_ver,\n",
        "#                                    hidden_ratio, act_layer, norm_layer, attn_drop, proj_drop, drop_path, helper, mv_act)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#         if mv_ver==3:\n",
        "#             self.mobivari = MobiVari1(self.cat_dim, 3, 1, act=mv_act, out_dim=dim)\n",
        "#         elif mv_ver==2:\n",
        "#             self.mobivari = MobiVari2(self.cat_dim, 3, 1, act=mv_act, out_dim=dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
        "#             # NAFBlock not Accepted\n",
        "#             # NAFBlock---------------------------------------------\n",
        "#         elif mv_ver==1:\n",
        "#             self.mobivari = NAFBlock(c=self.dim)\n",
        "#             # -------------------------------------------------------\n",
        "#         self.act = act_layer()\n",
        "#         self.norm = norm_layer(dim)\n",
        "#         # self.bottleneck = Bottleneck(dim, len(depths)-1, act_layer, norm_layer, mv_ver, mv_act)\n",
        "\n",
        "#         self.stage4 = EncoderStage(depths[0], dim, num_heads[3], chsa_head_ratio, FFAT, window_size, head_dim, qkv_bias, mv_ver,\n",
        "#                                    hidden_ratio, act_layer, norm_layer, attn_drop, proj_drop, drop_path, helper, mv_act)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#         if mv_ver==3:\n",
        "#             self.mobivari_mix = MobiVari1(dim+dim//4+dim//16+dim, 3, 1, act=mv_act, out_dim=dim)\n",
        "#         elif mv_ver==2:\n",
        "#             self.mobivari_mix = MobiVari2(dim+dim//4+dim//16+dim, 3, 1, act=mv_act, out_dim=dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
        "#             # NAFBlock not accepted\n",
        "#              # NAFBlock---------------------------------------------\n",
        "#         elif mv_ver==1:\n",
        "#             self.mobivari = NAFBlock(c=self.dim)\n",
        "#             # -------------------------------------------------------\n",
        "#         # self.attn_mix = HRAMi(dim, 3, 1, mv_ver, mv_act)\n",
        "\n",
        "\n",
        "#         self.to_target = Reconstruction(target_mode, dim, 3, 1, tail_mv, mv_ver, mv_act, exp_factor, expand_groups)\n",
        "\n",
        "#         self.apply(self._init_weights)\n",
        "\n",
        "#     def forward_size_norm(self, x):\n",
        "#         _,_,h,w = x.size()\n",
        "#         padh = self.unit-(h%self.unit) if h%self.unit!=0 else 0\n",
        "#         padw = self.unit-(w%self.unit) if w%self.unit!=0 else 0\n",
        "#         x = TF.pad(x, (0,0,padw,padh))\n",
        "#         return TF.normalize(x, self.mean, self.std) if self.img_norm else x\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         _,_,H_ori,W_ori = x.size()\n",
        "#         x = self.forward_size_norm(x)\n",
        "#         B, C, H, W = x.size()\n",
        "#         x_lq = x if self.scale == 1 else None # due to residual image (except for only SR)\n",
        "\n",
        "#         shallow = self.shallow(x)\n",
        "#         o1_, attn1 = self.stage1(shallow) # [B, C, H, W]\n",
        "\n",
        "\n",
        "#         # FFAT\n",
        "#         o1_ = self.encoder_level1(o1_)\n",
        "#         # FFAT\n",
        "\n",
        "\n",
        "#         o1 = self.down1(o1_) # [B, C, H//2, W//2]\n",
        "\n",
        "#         o2_, attn2 = self.stage2(o1) # [B, C, H//2, W//2]\n",
        "\n",
        "\n",
        "#         # FFAT\n",
        "#         o2_ = self.encoder_level2(o2_)\n",
        "#         # FFAT\n",
        "\n",
        "\n",
        "#         o2 = self.down2(o2_) # [B, C, H//4, W//4]\n",
        "\n",
        "#         o3_, attn3 = self.stage3(o2) # [B, C, H//4, W//4]\n",
        "\n",
        "\n",
        "#         # FFAT\n",
        "#         o3_ = self.encoder_level3(o3_)\n",
        "#         # FFAT\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Bottleneck\n",
        "#         def shallow_down( x, r):\n",
        "#           for _ in range(r):\n",
        "#               x = F.max_pool2d(x, 2)\n",
        "#           return F.leaky_relu(x)\n",
        "#         x_list = [shallow, o1_, o2_, o3_]\n",
        "#         xs = x_list[0]\n",
        "#         new_x = []\n",
        "#         for i in range(len(x_list[1:])):\n",
        "#             x_ = x_list[i+1]+shallow_down(xs, i)\n",
        "#             new_x.append(F.pixel_shuffle(x_, 2**i))\n",
        "#         ob = self.norm(self.mobivari(torch.cat(new_x, dim=1)))\n",
        "#         # ob = self.bottleneck([shallow, o1_, o2_, o3_]) # [B, C, H, W]\n",
        "\n",
        "\n",
        "\n",
        "#         o4, attn4 = self.stage4(ob) # [B, C, H, W]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # HRAMiT\n",
        "#         attn_list = [attn1, attn2, attn3, attn4]\n",
        "#         for i, attn in enumerate(attn_list[:-1]):\n",
        "#             attn = F.pixel_shuffle(attn, 2**i)\n",
        "#             x = attn if i==0 else torch.cat([x, attn], dim=1)\n",
        "#         x = torch.cat([x, attn_list[-1]], dim=1)\n",
        "#         mix = self.mobivari_mix(x)\n",
        "#         # mix = self.attn_mix([attn1, attn2, attn3, attn4]) # [B, C, H, W]\n",
        "\n",
        "\n",
        "\n",
        "#         o4 = o4 * mix # [B, C, H, W]\n",
        "\n",
        "#         out = self.to_target(o4+shallow) # global skip connection\n",
        "#         out = out + x_lq if self.scale == 1 else out # residual output (except for only SR)\n",
        "\n",
        "#         out = denormalize(out, self.mean, self.std) if self.img_norm else out\n",
        "#         out = out[:, :, :H_ori*self.scale, :W_ori*self.scale]\n",
        "#         return out\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         H_ori,W_ori = resolutions\n",
        "#         padh = self.unit-(H_ori%self.unit) if H_ori%self.unit!=0 else 0\n",
        "#         padw = self.unit-(W_ori%self.unit) if W_ori%self.unit!=0 else 0\n",
        "#         H,W = H_ori+padh,W_ori+padw\n",
        "#         flops = 0\n",
        "#         flops += self.shallow.flops((H,W))\n",
        "#         flops += self.stage1.flops((H,W))\n",
        "#         flops += self.down1.flops((H,W))\n",
        "#         flops += self.stage2.flops((H//2,W//2))\n",
        "#         flops += self.down2.flops((H//2,W//2))\n",
        "#         flops += self.stage3.flops((H//4,W//4))\n",
        "#         flops += self.bottleneck.flops((H,W))\n",
        "#         flops += self.stage4.flops((H,W))\n",
        "#         flops += self.attn_mix.flops((H,W))\n",
        "#         flops += self.dim * H*W # o4 = o4*mix\n",
        "#         flops += self.to_target.flops((H,W))\n",
        "#         return flops\n",
        "\n",
        "#     def _init_weights(self, m):\n",
        "#         # Swin V2 manner\n",
        "#         if isinstance(m, nn.Linear):\n",
        "#             trunc_normal_(m.weight, std=.02)\n",
        "#             if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "#                 nn.init.constant_(m.bias, 0)\n",
        "#         elif isinstance(m, nn.LayerNorm):\n",
        "#             nn.init.constant_(m.bias, 0)\n",
        "#             nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "#     @torch.jit.ignore\n",
        "#     def no_weight_decay(self):\n",
        "#         nwd = set()\n",
        "#         for n, _ in self.named_parameters():\n",
        "#             if 'relative_position_bias_table' in n:\n",
        "#                 nwd.add(n)\n",
        "#         return nwd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # ----------------------------Shadow_R-------------------------\n",
        "#     # @torch.no_grad()\n",
        "#     # def _init_weights(module_list, scale=1, bias_fill=0, **kwargs):\n",
        "#     #   \"\"\"Initialize network weights.\n",
        "\n",
        "#     #   Args:\n",
        "#     #       module_list (list[nn.Module] | nn.Module): Modules to be initialized.\n",
        "#     #       scale (float): Scale initialized weights, especially for residual\n",
        "#     #           blocks. Default: 1.\n",
        "#     #       bias_fill (float): The value to fill bias. Default: 0\n",
        "#     #       kwargs (dict): Other arguments for initialization function.\n",
        "#     #   \"\"\"\n",
        "#     #   if not isinstance(module_list, list):\n",
        "#     #       module_list = [module_list]\n",
        "#     #   for module in module_list:\n",
        "#     #       for m in module.modules():\n",
        "#     #           if isinstance(m, nn.Conv2d):\n",
        "#     #               nn.init.kaiming_normal_(m.weight, **kwargs)\n",
        "#     #               m.weight.data *= scale\n",
        "#     #               if m.bias is not None:\n",
        "#     #                   m.bias.data.fill_(bias_fill)\n",
        "#     #           elif isinstance(m, nn.Linear):\n",
        "#     #               nn.init.kaiming_normal_(m.weight, **kwargs)\n",
        "#     #               m.weight.data *= scale\n",
        "#     #               if m.bias is not None:\n",
        "#     #                   m.bias.data.fill_(bias_fill)\n",
        "#     #           elif isinstance(m, _BatchNorm):\n",
        "#     #               nn.init.constant_(m.weight, 1)\n",
        "#     #               if m.bias is not None:\n",
        "#     #                   m.bias.data.fill_(bias_fill)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "-5R9WbRqs1jX",
        "outputId": "9f8ada8c-bcd4-41e5-a02c-2a9d45c010da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/timm/models/fx_features.py:4: FutureWarning: Importing from timm.models.fx_features is deprecated, please import via timm.models\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.models\", FutureWarning)\n",
            "/usr/local/lib/python3.10/dist-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
            "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n"
          ]
        }
      ],
      "source": [
        "# RAMiT (Reciprocal Attention Mixing Transformer)\n",
        "\n",
        "from pdb import set_trace as stx\n",
        "import numbers\n",
        "from skimage import io\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "from torch import nn, Tensor\n",
        "from einops import rearrange\n",
        "import torch.nn.functional as F\n",
        "import torch\n",
        "from torchvision.transforms import functional as TF\n",
        "import math\n",
        "from RAMiT.my_model.common.mean_std import mean_std\n",
        "from RAMiT.utils.etc_utils import denormalize\n",
        "from timm.models.fx_features import register_notrace_function\n",
        "from timm.models.layers import trunc_normal_, to_2tuple\n",
        "from torch.nn.modules.batchnorm import _BatchNorm\n",
        "def make_model(args, opts, pe):\n",
        "    model = RAMiT(target_mode=args.target_mode,\n",
        "                  img_norm=args.img_norm,\n",
        "                  in_chans=opts['in_chans'],\n",
        "                  dim = opts['dim'],\n",
        "                  depths = opts['depths'],\n",
        "                  num_heads = opts['num_heads'],\n",
        "                  head_dim = opts['head_dim'],\n",
        "                  chsa_head_ratio = opts['chsa_head_ratio'],\n",
        "                  window_size = opts['window_size'],\n",
        "                  hidden_ratio = opts['hidden_ratio'],\n",
        "                  qkv_bias = opts['qkv_bias'],\n",
        "                  mv_ver = opts['mv_ver'],\n",
        "                  exp_factor=opts['exp_factor'],\n",
        "                  expand_groups=opts['expand_groups'],\n",
        "                  act_layer = opts['act_layer'],\n",
        "                  norm_layer = opts['norm_layer'],\n",
        "                  tail_mv = opts['tail_mv'],\n",
        "                  attn_drop = opts['attn_drop'],\n",
        "                  proj_drop = opts['proj_drop'],\n",
        "                  drop_path = opts['drop_path'],\n",
        "                  helper = opts['helper'],\n",
        "                  mv_act = opts['mv_act']\n",
        "                 )\n",
        "    num_params = sum([p.numel() for n,p in model.named_parameters()])\n",
        "    print(format(num_params, ','))\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# FFSANet------------------------------------------------\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.fft\n",
        "\n",
        "class BasicConv(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, kernel_size, stride, bias=True, norm=False, relu=True, transpose=False):\n",
        "        super(BasicConv, self).__init__()\n",
        "        if bias and norm:\n",
        "            bias = False\n",
        "\n",
        "        padding = kernel_size // 2\n",
        "        layers = list()\n",
        "        if transpose:\n",
        "            padding = kernel_size // 2 -1\n",
        "            layers.append(nn.ConvTranspose2d(in_channel, out_channel, kernel_size, padding=padding, stride=stride, bias=bias))\n",
        "        else:\n",
        "            layers.append(\n",
        "                nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding, stride=stride, bias=bias))\n",
        "        if norm:\n",
        "            layers.append(nn.BatchNorm2d(out_channel))\n",
        "        if relu:\n",
        "            layers.append(nn.GELU())\n",
        "        self.main = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.main(x)\n",
        "\n",
        "class ResBlock(nn.Module):\n",
        "    def __init__(self, in_channel, out_channel, filter=False):\n",
        "        super(ResBlock, self).__init__()\n",
        "        self.conv1 = BasicConv(in_channel, out_channel, kernel_size=3, stride=1, relu=True)\n",
        "        self.conv2 = BasicConv(out_channel, out_channel, kernel_size=3, stride=1, relu=False)\n",
        "        if filter:\n",
        "            self.cubic_11 = cubic_attention(in_channel//2, group=1, kernel=11)\n",
        "            self.cubic_7 = cubic_attention(in_channel//2, group=1, kernel=7)\n",
        "        self.filter = filter\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.conv1(x)\n",
        "        if self.filter:\n",
        "            out = torch.chunk(out, 2, dim=1)\n",
        "            out_11 = self.cubic_11(out[0])\n",
        "            out_7 = self.cubic_7(out[1])\n",
        "            out = torch.cat((out_11, out_7), dim=1)\n",
        "        out = self.conv2(out)\n",
        "        return out + x\n",
        "\n",
        "# class cubic_attention(nn.Module):\n",
        "#     def __init__(self, dim, group, kernel) -> None:\n",
        "#         super().__init__()\n",
        "\n",
        "#         self.H_spatial_att = spatial_strip_att(dim, group=group, kernel=kernel)\n",
        "#         self.W_spatial_att = spatial_strip_att(dim, group=group, kernel=kernel, H=False)\n",
        "#         self.gamma = nn.Parameter(torch.zeros(dim,1,1))\n",
        "#         self.beta = nn.Parameter(torch.ones(dim,1,1))\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         # Apply Fourier Transform to the input\n",
        "#         x_fft = torch.fft.fft2(x)\n",
        "\n",
        "#         # Apply attention mechanisms in the frequency domain\n",
        "#         out = self.H_spatial_att(x_fft)\n",
        "#         out = self.W_spatial_att(out)\n",
        "\n",
        "#         # Apply Inverse Fourier Transform to get back to the spatial domain\n",
        "#         out_ifft = torch.fft.ifft2(out).real\n",
        "\n",
        "#         return self.gamma * out_ifft + x * self.beta\n",
        "\n",
        "class cubic_attention(nn.Module):\n",
        "    def __init__(self, dim, group, kernel) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.H_spatial_att = spatial_strip_att(dim, group=group, kernel=kernel)\n",
        "        self.W_spatial_att = spatial_strip_att(dim, group=group, kernel=kernel, H=False)\n",
        "        self.gamma = nn.Parameter(torch.zeros(dim,1,1))\n",
        "        self.beta = nn.Parameter(torch.ones(dim,1,1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply Fourier Transform to the input\n",
        "        x_fft = torch.fft.fft2(x)\n",
        "\n",
        "        # Separate real and imaginary parts\n",
        "        real_part = x_fft.real\n",
        "        imag_part = x_fft.imag\n",
        "\n",
        "        # Apply attention mechanisms to real and imaginary parts separately\n",
        "        out_real = self.H_spatial_att(real_part)\n",
        "        out_real = self.W_spatial_att(out_real)\n",
        "        out_imag = self.H_spatial_att(imag_part)\n",
        "        out_imag = self.W_spatial_att(out_imag)\n",
        "\n",
        "        # Combine real and imaginary parts back into a complex tensor\n",
        "        out = torch.complex(out_real, out_imag)\n",
        "\n",
        "        # Apply Inverse Fourier Transform to get back to the spatial domain\n",
        "        out_ifft = torch.fft.ifft2(out).real\n",
        "\n",
        "        return self.gamma * out_ifft + x * self.beta\n",
        "\n",
        "\n",
        "class spatial_strip_att(nn.Module):\n",
        "    def __init__(self, dim, kernel=5, group=2, H=True) -> None:\n",
        "        super().__init__()\n",
        "\n",
        "        self.k = kernel\n",
        "        pad = kernel // 2\n",
        "        self.kernel = (1, kernel) if H else (kernel, 1)\n",
        "        self.padding = (kernel//2, 1) if H else (1, kernel//2)\n",
        "\n",
        "        self.group = group\n",
        "        self.pad = nn.ReflectionPad2d((pad, pad, 0, 0)) if H else nn.ReflectionPad2d((0, 0, pad, pad))\n",
        "        self.conv = nn.Conv2d(dim, group*kernel, kernel_size=1, stride=1, bias=False)\n",
        "        self.ap = nn.AdaptiveAvgPool2d((1, 1))\n",
        "        self.filter_act = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        filter = self.ap(x)\n",
        "        filter = self.conv(filter)\n",
        "        n, c, h, w = x.shape\n",
        "\n",
        "        # Perform operations in the frequency domain\n",
        "        x = F.unfold(self.pad(x), kernel_size=self.kernel).reshape(n, self.group, c//self.group, self.k, h*w)\n",
        "        n, c1, p, q = filter.shape\n",
        "        filter = filter.reshape(n, c1//self.k, self.k, p*q).unsqueeze(2)\n",
        "        filter = self.filter_act(filter)\n",
        "        out = torch.sum(x * filter, dim=3).reshape(n, c, h, w)\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "# FFSANet----------------------------------------------------------\n",
        "\n",
        "class ShallowModule(nn.Module):\n",
        "    def __init__(self, in_chans, out_chans, kernel_size=3, stride=1):\n",
        "        super(ShallowModule, self).__init__()\n",
        "\n",
        "        self.in_chans = in_chans\n",
        "        self.out_chans = out_chans\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "\n",
        "        self.conv = nn.Conv2d(in_chans, out_chans, kernel_size, stride, kernel_size//2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x)\n",
        "\n",
        "    def flops(self, resolutions):\n",
        "        return resolutions[0]*resolutions[1] * self.kernel_size*self.kernel_size * self.in_chans * self.out_chans\n",
        "\n",
        "class QKVProjection(nn.Module):\n",
        "    def __init__(self, dim, num_head, qkv_bias=True):\n",
        "        super(QKVProjection, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.num_head = num_head\n",
        "\n",
        "        self.qkv = nn.Conv2d(dim, 3*dim, 1, bias=qkv_bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.size()\n",
        "        qkv = self.qkv(x)\n",
        "        qkv = rearrange(qkv, 'b (l c) h w -> b l c h w', l=self.num_head)\n",
        "        return qkv\n",
        "\n",
        "    def flops(self, resolutions):\n",
        "        return resolutions[0]*resolutions[1] * 1*1 * self.dim * 3*self.dim\n",
        "\n",
        "def get_relative_position_index(win_h, win_w):\n",
        "    # get pair-wise relative position index for each token inside the window\n",
        "    coords = torch.stack(torch.meshgrid([torch.arange(win_h), torch.arange(win_w)]))  # 2, Wh, Ww\n",
        "    coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
        "    relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww (xaxis matrix & yaxis matrix)\n",
        "    relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
        "    relative_coords[:, :, 0] += win_h - 1  # shift to start from 0\n",
        "    relative_coords[:, :, 1] += win_w - 1\n",
        "    relative_coords[:, :, 0] *= 2 * win_w - 1\n",
        "    return relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
        "\n",
        "class SpatialSelfAttention(nn.Module):\n",
        "    def __init__(self, dim, num_head, total_head, window_size=8, shift=0, attn_drop=0.0, proj_drop=0.0, helper=True):\n",
        "        super(SpatialSelfAttention, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.num_head = num_head\n",
        "        self.total_head = total_head\n",
        "        self.window_size = window_size\n",
        "        self.window_area = window_size**2\n",
        "        self.shift = shift\n",
        "        self.helper = helper\n",
        "\n",
        "        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_head, 1, 1))), requires_grad=True)\n",
        "\n",
        "        # define a parameter table of relative position bias, shape: 2*Wh-1 * 2*Ww-1, nH\n",
        "        self.relative_position_bias_table = nn.Parameter(torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_head))\n",
        "\n",
        "        # get pair-wise relative position index for each token inside the window\n",
        "        self.register_buffer(\"relative_position_index\", get_relative_position_index(window_size, window_size))\n",
        "\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "\n",
        "        self.proj = nn.Conv2d(dim*num_head, dim*num_head, 1)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def _get_rel_pos_bias(self) -> torch.Tensor:\n",
        "        relative_position_bias = self.relative_position_bias_table[\n",
        "            self.relative_position_index.view(-1)].view(self.window_area, self.window_area, -1)  # Wh*Ww,Wh*Ww,nH\n",
        "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
        "        return relative_position_bias.unsqueeze(0)\n",
        "\n",
        "    def forward(self, qkv, ch=None):\n",
        "        B, L, C, H, W = qkv.size()\n",
        "        # window shift\n",
        "        if self.shift > 0:\n",
        "            qkv = torch.roll(qkv, shifts=(-self.shift, -self.shift), dims=(-2,-1))\n",
        "\n",
        "        # window partition\n",
        "        q,k,v = rearrange(qkv, 'b l c (h wh) (w ww) -> (b h w) l (wh ww) c',\n",
        "                          wh=self.window_size, ww=self.window_size).chunk(3, dim=-1) # [B_, L1, hw, C/L] respectively\n",
        "        if ch is not None and self.helper: # [B, C, H, W]\n",
        "            if self.shift > 0:\n",
        "                ch = torch.roll(ch, shifts=(-self.shift, -self.shift), dims=(-2,-1))\n",
        "            ch = rearrange(ch, 'b (l c) (h wh) (w ww) -> (b h w) l (wh ww) c',\n",
        "                           l=self.total_head-self.num_head, wh=self.window_size, ww=self.window_size) # [B_, L1, hw, C/L]\n",
        "            ch = torch.mean(ch, dim=1, keepdim=True) # head squeeze [B_, 1, hw, C/L]\n",
        "            v = v*ch # [B_, L1, hw, C/L]\n",
        "\n",
        "        attn = F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(2,-1) # [B_, L1, hw, hw]\n",
        "        logit_scale = torch.clamp(self.logit_scale, max=math.log(1. / 0.01)).exp()\n",
        "        attn = attn * logit_scale\n",
        "\n",
        "        attn = attn + self._get_rel_pos_bias()\n",
        "        attn = self.attn_drop(F.softmax(attn, dim=-1))\n",
        "\n",
        "        x = attn @ v # [B_, L1, hw, C/L]\n",
        "\n",
        "        # window unpartition + head merge\n",
        "        x = window_unpartition(x, (H,W), self.window_size) # [B, L1*C/L, H, W]\n",
        "        x = self.proj_drop(self.proj(x))\n",
        "\n",
        "        # window reverse shift\n",
        "        if self.shift > 0:\n",
        "            x = torch.roll(x, shifts=(self.shift, self.shift), dims=(-2,-1))\n",
        "\n",
        "        return x\n",
        "\n",
        "    def flops(self, resolutions):\n",
        "        H,W = resolutions\n",
        "        num_wins = H//self.window_size * W//self.window_size\n",
        "        flops = self.num_head * H*W * self.dim if self.helper else 0 # v = v*ch\n",
        "        flops += num_wins * self.num_head * self.window_area * self.dim * self.window_area # attn = Q@K^T\n",
        "        flops += num_wins * self.num_head * self.window_area * self.window_area * self.dim # attn@V\n",
        "        flops += H*W * 1*1 * self.num_head*self.dim * self.num_head*self.dim # self.proj\n",
        "        return flops\n",
        "\n",
        "@register_notrace_function\n",
        "def window_unpartition(x, resolutions, window_size):\n",
        "    return rearrange(x, '(b h w) l (wh ww) c -> b (l c) (h wh) (w ww)',\n",
        "                     h=resolutions[0]//window_size, w=resolutions[1]//window_size, wh=window_size)\n",
        "\n",
        "class ChannelSelfAttention(nn.Module):\n",
        "    def __init__(self, dim, num_head, total_head, attn_drop=0.0, proj_drop=0.0, helper=True):\n",
        "        super(ChannelSelfAttention, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.num_head = num_head\n",
        "        self.total_head = total_head\n",
        "        self.helper = helper\n",
        "\n",
        "        self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_head, 1, 1))), requires_grad=True)\n",
        "\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "\n",
        "        self.proj = nn.Conv2d(dim*num_head, dim*num_head, 1)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, qkv, sp=None):\n",
        "        B, L, C, H, W = qkv.size()\n",
        "\n",
        "        q,k,v = rearrange(qkv, 'b l c h w -> b l c (h w)').chunk(3, dim=-2) # [B, L2, C/L, HW]\n",
        "        if sp is not None and self.helper:\n",
        "            sp = torch.mean(sp, dim=1, keepdim=True) # channel squeeze # [B, 1, H, W]\n",
        "            sp = rearrange(sp, 'b (l c) h w -> b l c (h w)', l=1) # [B, 1, 1, HW]\n",
        "            v = v*sp # [B, L2, C/L, HW]\n",
        "\n",
        "        attn = F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(2,-1) # [B, L2, C/L, C/L]\n",
        "        logit_scale = torch.clamp(self.logit_scale, max=math.log(1. / 0.01)).exp()\n",
        "        attn = attn * logit_scale\n",
        "\n",
        "        attn = F.softmax(attn, dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = attn @ v # [B, L2, C/L, HW]\n",
        "\n",
        "        # head merge\n",
        "        x = rearrange(x, 'b l c (h w) -> b (l c) h w', h=H) # [B, L2*C/L, H, W]\n",
        "        x = self.proj_drop(self.proj(x)) # [B, L2*C/L, H, W]\n",
        "\n",
        "        return x\n",
        "\n",
        "    def flops(self, resolutions):\n",
        "        H,W = resolutions\n",
        "        flops = self.num_head * self.dim * H*W if self.helper else 0 # v = v*sp\n",
        "        flops += self.num_head * self.dim * H*W * self.dim # attn = Q@K^T\n",
        "        flops += self.num_head * self.dim * self.dim * H*W # attn@V\n",
        "        flops += H*W * 1*1 * self.num_head*self.dim * self.num_head*self.dim # self.proj\n",
        "        return flops\n",
        "\n",
        "class ReshapeLayerNorm(nn.Module):\n",
        "    def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
        "        super(ReshapeLayerNorm, self).__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.norm = norm_layer(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.size()\n",
        "        x = rearrange(x, 'b c h w -> b (h w) c')\n",
        "        x = self.norm(x)\n",
        "        x = rearrange(x, 'b (h w) c -> b c h w', h=H)\n",
        "        return x\n",
        "\n",
        "    def flops(self, resolutions):\n",
        "        H,W = resolutions\n",
        "        flops = 0\n",
        "        flops += H*W * self.dim\n",
        "        return flops\n",
        "\n",
        "class MobiVari1(nn.Module): # MobileNet v1 Variants\n",
        "    def __init__(self, dim, kernel_size, stride, act=nn.LeakyReLU, out_dim=None):\n",
        "        super(MobiVari1, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.kernel_size = kernel_size\n",
        "        self.out_dim = out_dim or dim\n",
        "\n",
        "        self.dw_conv = nn.Conv2d(dim, dim, kernel_size, stride, kernel_size//2, groups=dim)\n",
        "        self.pw_conv = nn.Conv2d(dim, self.out_dim, 1, 1, 0)\n",
        "        self.act = act()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.act(self.pw_conv(self.act(self.dw_conv(x))+x))\n",
        "        return out + x if self.dim==self.out_dim else out\n",
        "\n",
        "    def flops(self, resolutions):\n",
        "        H,W = resolutions\n",
        "        flops = H*W * self.kernel_size*self.kernel_size * self.dim  +  H*W * 1*1 * self.dim * self.out_dim # self.dw_conv + self.pw_conv\n",
        "        return flops\n",
        "\n",
        "class MobiVari2(MobiVari1): # MobileNet v2 Variants\n",
        "    def __init__(self, dim, kernel_size, stride, act=nn.LeakyReLU, out_dim=None, exp_factor=1.2, expand_groups=4):\n",
        "        super(MobiVari2, self).__init__(dim, kernel_size, stride, act, out_dim)\n",
        "        self.expand_groups = expand_groups\n",
        "        expand_dim = int(dim*exp_factor)\n",
        "        expand_dim = expand_dim+(expand_groups-expand_dim%expand_groups)\n",
        "        self.expand_dim = expand_dim\n",
        "\n",
        "        self.exp_conv = nn.Conv2d(dim, self.expand_dim, 1, 1, 0, groups=expand_groups)\n",
        "        self.dw_conv = nn.Conv2d(expand_dim, expand_dim, kernel_size, stride, kernel_size//2, groups=expand_dim)\n",
        "        self.pw_conv = nn.Conv2d(expand_dim, self.out_dim, 1, 1, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x1 = self.act(self.exp_conv(x))\n",
        "        out = self.pw_conv(self.act(self.dw_conv(x1)+x1))\n",
        "        return out + x if self.dim==self.out_dim else out\n",
        "\n",
        "    def flops(self, resolutions):\n",
        "        H,W = resolutions\n",
        "        flops = H*W * 1*1 * (self.dim//self.expand_groups) * self.expand_dim # self.exp_conv\n",
        "        flops += H*W * self.kernel_size*self.kernel_size * self.expand_dim # self.dw_conv\n",
        "        flops += H*W * 1*1 * self.expand_dim * self.out_dim # self.pw_conv\n",
        "        return flops\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, hidden_ratio, act_layer=nn.GELU, bias=True, drop=0.0):\n",
        "        super(FeedForward, self).__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.hidden_ratio = hidden_ratio\n",
        "\n",
        "        self.hidden = nn.Conv2d(dim, int(dim*hidden_ratio), 1, bias=bias)\n",
        "        self.drop1 = nn.Dropout(drop)\n",
        "        self.out = nn.Conv2d(int(dim*hidden_ratio), dim, 1, bias=bias)\n",
        "        self.drop2 = nn.Dropout(drop)\n",
        "        self.act = act_layer()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.drop2(self.out(self.drop1(self.act(self.hidden(x)))))\n",
        "\n",
        "    def flops(self, resolutions):\n",
        "        H,W = resolutions\n",
        "        flops = 2 * H*W * 1*1 * self.dim * self.dim*self.hidden_ratio # self.hidden + self.out\n",
        "        return flops\n",
        "\n",
        "class NoLayer(nn.Identity):\n",
        "    def __init__(self):\n",
        "        super(NoLayer, self).__init__()\n",
        "    def flops(self, resolutions):\n",
        "        return 0\n",
        "    def forward(self, x, **kwargs):\n",
        "        return x.flatten(1,2)\n",
        "\n",
        "class DRAMiTransformer(nn.Module): # Reciprocal Attention Transformer Block\n",
        "    def __init__(self, dim, num_head, chsa_head_ratio, FFAT, window_size=8, shift=0, head_dim=None, qkv_bias=True, mv_ver=1,\n",
        "                 hidden_ratio=2.0, act_layer=nn.GELU, norm_layer=ReshapeLayerNorm, attn_drop=0.0, proj_drop=0.0, drop_path=0.0, helper=True,\n",
        "                 mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4,\n",
        "                 num_blocks = [4,6,6,8], heads = [1,2,4,8], ffn_expansion_factor = 2.66, bias = False, LayerNorm_type = 'WithBias'):\n",
        "        super(DRAMiTransformer, self).__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.num_head = num_head\n",
        "        self.window_size = window_size\n",
        "        self.chsa_head = int(num_head*chsa_head_ratio)\n",
        "        self.shift = shift\n",
        "        self.helper = helper\n",
        "        self.FFAT = FFAT\n",
        "\n",
        "        self.qkv_proj = QKVProjection(dim, num_head, qkv_bias=qkv_bias)\n",
        "        self.sp_attn = SpatialSelfAttention(dim//num_head, num_head-self.chsa_head, num_head,\n",
        "                                            window_size, shift, attn_drop, proj_drop, helper) if num_head-self.chsa_head != 0 else NoLayer()\n",
        "        self.ch_attn = ChannelSelfAttention(dim//num_head, self.chsa_head, num_head, attn_drop, proj_drop, helper) if self.chsa_head != 0 else NoLayer()\n",
        "\n",
        "\n",
        "        #FFAT\n",
        "        if self.FFAT:\n",
        "          self.encoder_level1 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
        "        #FFAT\n",
        "\n",
        "\n",
        "        if mv_ver==3:\n",
        "            self.mobivari = MobiVari1(dim, 3, 1, act=mv_act)\n",
        "        elif mv_ver==2:\n",
        "            self.mobivari = MobiVari2(dim, 3, 1, act=mv_act, out_dim=None, exp_factor=exp_factor, expand_groups=expand_groups)\n",
        "            # NAFBlock---------------------------------------------\n",
        "        elif mv_ver==1:\n",
        "            self.mobivari = NAFBlock(c=self.dim)\n",
        "            # -------------------------------------------------------\n",
        "\n",
        "        self.norm1 = norm_layer(dim)\n",
        "\n",
        "        self.ffn = FeedForward(dim, hidden_ratio, act_layer=act_layer)\n",
        "        self.norm2 = norm_layer(dim)\n",
        "\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "    def forward(self, x, sp_=None, ch_=None):\n",
        "        B, C, H, W = x.size()\n",
        "\n",
        "        # QKV projection + head split\n",
        "        qkv = self.qkv_proj(x) # [B, L, C, H, W]\n",
        "\n",
        "        # SP-SA / CH-SA\n",
        "        sp = self.sp_attn(qkv[:,:self.num_head-self.chsa_head], ch=ch_) # [B, L1*C/L, H, W]\n",
        "        ch = self.ch_attn(qkv[:,self.num_head-self.chsa_head:], sp=sp_) # [B, L2*C/L, H, W]\n",
        "\n",
        "\n",
        "        #FFAT\n",
        "        if self.FFAT:\n",
        "          FFAT_block = self.encoder_level1(torch.cat([sp, ch], dim=1))\n",
        "          attn0 = self.mobivari(FFAT_block)\n",
        "        #FFAT\n",
        "\n",
        "\n",
        "        else:\n",
        "          attn0 = self.mobivari(torch.cat([sp, ch], dim=1)) # merge [B, C, H, W]\n",
        "        attn = self.drop_path(self.norm1(attn0)) + x # LN, skip connection [B, C, H, W]\n",
        "\n",
        "        # FFN\n",
        "        out = self.drop_path(self.norm2(self.ffn(attn))) + attn # FFN, LN, skip connection [B, C, H, W]\n",
        "\n",
        "        return out, sp, ch, attn0\n",
        "\n",
        "    def flops(self, resolutions):\n",
        "        flops = self.qkv_proj.flops(resolutions)\n",
        "        flops += self.sp_attn.flops(resolutions)\n",
        "        flops += self.ch_attn.flops(resolutions)\n",
        "        flops += self.mobivari.flops(resolutions)\n",
        "        flops += self.norm1.flops(resolutions)\n",
        "        flops += self.ffn.flops(resolutions)\n",
        "        flops += self.norm2.flops(resolutions)\n",
        "        params = sum([p.numel() for n,p in self.named_parameters()])\n",
        "        return flops\n",
        "\n",
        "class EncoderStage(nn.Module):\n",
        "    def __init__(self, depth, dim, num_head, chsa_head_ratio, FFAT, window_size=8, head_dim=None,\n",
        "                 qkv_bias=True, mv_ver=1, hidden_ratio=2.0, act_layer=nn.GELU, norm_layer=ReshapeLayerNorm,\n",
        "                 attn_drop=0.0, proj_drop=0.0, drop_path=0.0, helper=True, mv_act=nn.LeakyReLU,):\n",
        "        super(EncoderStage, self).__init__()\n",
        "\n",
        "        self.depth = depth\n",
        "        self.dim = dim\n",
        "        self.num_head = num_head\n",
        "        self.window_size = window_size\n",
        "        shift = window_size//2\n",
        "\n",
        "        self.blocks = nn.ModuleList()\n",
        "        for d in range(depth):\n",
        "            self.blocks.add_module(f'block{d}', DRAMiTransformer(dim, num_head, chsa_head_ratio, FFAT,\n",
        "                                                                 window_size, 0 if d%2==0 else shift, head_dim, qkv_bias,\n",
        "                                                                 mv_ver, hidden_ratio, act_layer, norm_layer,\n",
        "                                                                 attn_drop, proj_drop, drop_path, helper, mv_act,\n",
        "                                                                 ))\n",
        "    def forward(self, x):\n",
        "        sp, ch = None, None\n",
        "        for i, blk in enumerate(self.blocks):\n",
        "            x, sp, ch, attn = blk(x, sp, ch)\n",
        "            sp, ch = (None, None) if (sp.size(1)==0 or ch.size(1)==0) else (sp, ch) # pure SPSA or CHSA\n",
        "        return x, attn\n",
        "\n",
        "    def flops(self, resolutions):\n",
        "        flops = 0\n",
        "        for blk in self.blocks:\n",
        "            flops += blk.flops(resolutions)\n",
        "        return flops\n",
        "\n",
        "class Downsizing(nn.Module):\n",
        "    \"\"\" Patch Merging Layer.\n",
        "\n",
        "    Args:\n",
        "        dim (int): Number of input dimension (channels).\n",
        "        downsample_dim (int, optional): Number of output dimension (channels) (dim if not set).  Default: None\n",
        "        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, dim, downsample_dim=None, norm_layer=ReshapeLayerNorm, mv_ver=1, mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4):\n",
        "        super(Downsizing, self).__init__()\n",
        "        self.dim = dim\n",
        "        self.downsample_dim = downsample_dim or dim\n",
        "        self.norm = norm_layer(4*dim)\n",
        "        if mv_ver==3:\n",
        "            self.reduction = MobiVari1(4*dim, 3, 1, act=mv_act, out_dim=self.downsample_dim)\n",
        "        elif mv_ver==2:\n",
        "            self.reduction = MobiVari2(4*dim, 3, 1, act=mv_act, out_dim=self.downsample_dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
        "\n",
        "            # NAFBlock in not accepted\n",
        "            # NAFBlock---------------------------------------------\n",
        "        elif mv_ver==1:\n",
        "            self.mobivari = NAFBlock(c=self.dim)\n",
        "            # -------------------------------------------------------\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.size()\n",
        "\n",
        "        # Concat 2x2\n",
        "        x0 = x[:, :, 0::2, 0::2]  # [B, C, H/2, W/2], top-left\n",
        "        x1 = x[:, :, 0::2, 1::2]  # [B, C, H/2, W/2], top-right\n",
        "        x2 = x[:, :, 1::2, 0::2]  # [B, C, H/2, W/2], bottom-left\n",
        "        x3 = x[:, :, 1::2, 1::2]  # [B, C, H/2, W/2], bottom-right\n",
        "        x = torch.cat([x0, x1, x2, x3], dim=1)  # [B, 4C, H/2, W/2]\n",
        "        return self.reduction(self.norm(x)) # [B, C, H/2, W/2]\n",
        "\n",
        "    def flops(self, resolutions):\n",
        "        H,W = resolutions\n",
        "        flops = self.norm.flops((H//2,W//2)) + self.reduction.flops((H//2,W//2))\n",
        "        return flops\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    def __init__(self, dim, num_stages, act_layer=nn.GELU, norm_layer=ReshapeLayerNorm, mv_ver=1, mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4):\n",
        "        super(Bottleneck, self).__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.cat_dim = sum([dim//(4**x) for x in range(num_stages)])\n",
        "        if mv_ver==1:\n",
        "            self.mobivari = MobiVari1(self.cat_dim, 3, 1, act=mv_act, out_dim=dim)\n",
        "        elif mv_ver==2:\n",
        "            self.mobivari = MobiVari2(self.cat_dim, 3, 1, act=mv_act, out_dim=dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
        "\n",
        "            # NAFBlock---------------------------------------------\n",
        "        elif mv_ver==3:\n",
        "            self.mobivari = NAFBlock(c=self.dim)\n",
        "            # -------------------------------------------------------\n",
        "        self.act = act_layer()\n",
        "        self.norm = norm_layer(dim)\n",
        "\n",
        "    def shallow_down(self, x, r):\n",
        "        for _ in range(r):\n",
        "            x = F.max_pool2d(x, 2)\n",
        "        return F.leaky_relu(x)\n",
        "\n",
        "    def forward(self, x_list):\n",
        "        xs = x_list[0]\n",
        "        new_x = []\n",
        "        for i in range(len(x_list[1:])):\n",
        "            x_ = x_list[i+1]+self.shallow_down(xs, i)\n",
        "            new_x.append(F.pixel_shuffle(x_, 2**i))\n",
        "        new_x = self.norm(self.mobivari(torch.cat(new_x, dim=1)))\n",
        "        return new_x\n",
        "\n",
        "    def flops(self, resolutions):\n",
        "        H,W = resolutions\n",
        "        flops = 0\n",
        "        # self.shallow_down (iterative max-pool)\n",
        "        flops += (H//2)*(W//2) * 2*2 * self.dim # shallow-down into stage2 output (iter1)\n",
        "        flops += (H//2)*(W//2) * 2*2 * self.dim # shallow-down into stage3 output (iter1)\n",
        "        flops += (H//4)*(W//4) * 2*2 * self.dim # shallow-down into stage3 output (iter2)\n",
        "        flops += self.mobivari.flops((H,W))\n",
        "        flops += self.norm.flops((H,W))\n",
        "        return flops\n",
        "\n",
        "class HRAMi(nn.Module):\n",
        "    def __init__(self, dim, kernel_size=3, stride=1, mv_ver=1, mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4):\n",
        "        super(HRAMi, self).__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.kernel_size = kernel_size\n",
        "\n",
        "        if mv_ver==1:\n",
        "            self.mobivari = MobiVari1(dim+dim//4+dim//16+dim, kernel_size, stride, act=mv_act, out_dim=dim)\n",
        "        elif mv_ver==2:\n",
        "            self.mobivari = MobiVari2(dim+dim//4+dim//16+dim, kernel_size, stride, act=mv_act, out_dim=dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
        "            # NAFBlock---------------------------------------------\n",
        "        elif mv_ver==3:\n",
        "            self.mobivari = NAFBlock(c=self.dim)\n",
        "            # -------------------------------------------------------\n",
        "\n",
        "    def forward(self, attn_list):\n",
        "        for i, attn in enumerate(attn_list[:-1]):\n",
        "            attn = F.pixel_shuffle(attn, 2**i)\n",
        "            x = attn if i==0 else torch.cat([x, attn], dim=1)\n",
        "        x = torch.cat([x, attn_list[-1]], dim=1)\n",
        "        x = self.mobivari(x)\n",
        "        return x\n",
        "\n",
        "    def flops(self, resolutions):\n",
        "        return self.mobivari.flops(resolutions)\n",
        "\n",
        "class Reconstruction(nn.Module):\n",
        "    def __init__(self, target_mode, dim, kernel_size=3, stride=1, num_mv=2, mv_ver=1, mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4):\n",
        "        super(Reconstruction, self).__init__()\n",
        "\n",
        "        self.upscale = upscale = int(target_mode[-1]) if target_mode in ['light_x2', 'light_x3', 'light_x4'] else 1\n",
        "        self.target_mode = target_mode\n",
        "        self.dim = dim\n",
        "        self.out_chans = 3 if 'gray' not in target_mode else 1\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "        self.num_mv = num_mv\n",
        "\n",
        "        self.mobivari = nn.ModuleList()\n",
        "        for i in range(num_mv):\n",
        "            if mv_ver==3:\n",
        "                self.mobivari.add_module(f'mobivari{i}', MobiVari1(dim, kernel_size, stride, mv_act))\n",
        "            elif mv_ver==2:\n",
        "                self.mobivari.add_module(f'mobivari{i}', MobiVari2(dim, kernel_size, stride, mv_act, None, exp_factor, expand_groups))\n",
        "                # NAFBlock---------------------------------------------\n",
        "            elif mv_ver==1:\n",
        "                 self.mobivari.add_module(f'mobivari{i}',NAFBlock(c=self.dim))\n",
        "                # -------------------------------------------------------\n",
        "\n",
        "\n",
        "        self.conv = nn.Conv2d(dim, self.out_chans*(upscale**2), kernel_size, stride, kernel_size//2)\n",
        "        self.shuffle = nn.PixelShuffle(upscale) if upscale!=1 else nn.Identity()\n",
        "        self.final_conv = nn.Conv2d(self.out_chans, self.out_chans, kernel_size, stride, kernel_size//2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        for mobivari in self.mobivari:\n",
        "            x = mobivari(x)\n",
        "        return self.final_conv(self.shuffle(self.conv(x)))\n",
        "\n",
        "    def flops(self, resolutions):\n",
        "        H,W = resolutions\n",
        "        flops = 0\n",
        "        for mobivari in self.mobivari: # self.mobivari\n",
        "            flops += mobivari.flops((H,W))\n",
        "        flops += H*W * self.kernel_size*self.kernel_size * self.dim * self.out_chans*(self.upscale**2) # self.conv\n",
        "        flops += H*W * self.kernel_size*self.kernel_size * self.out_chans * self.out_chans # self.final_conv\n",
        "        return flops\n",
        "\n",
        "class RAMiT_FFAT(nn.Module):\n",
        "    def __init__(self, in_chans=3, dim=64, depths=(6,4,4,6), num_heads=(4,4,4,4), head_dim=None, chsa_head_ratio=0.25, #depths = (6,4,4,6)\n",
        "                 window_size=8, hidden_ratio=2.0, qkv_bias=True, mv_ver=1, exp_factor=1.2, expand_groups=4,\n",
        "                 act_layer=nn.GELU, norm_layer=ReshapeLayerNorm, tail_mv=2, weight_init='', target_mode='light_x2', img_norm=True,\n",
        "                 attn_drop=0.0, proj_drop=0.0, drop_path=0.0, helper=True, mv_act=nn.LeakyReLU,\n",
        "                 num_blocks = [1,4,6,6,8],\n",
        "                 heads = [1,2,4,8],\n",
        "                 ffn_expansion_factor = 2.66,\n",
        "                 bias = False,\n",
        "                 LayerNorm_type = 'WithBias',   ## Other option 'BiasFree'\n",
        "                 FFAT = False, **kwargs):\n",
        "        super(RAMiT_FFAT, self).__init__()\n",
        "        self.cat_dim = sum([dim//(4**x) for x in range(len(depths)-1)]) # added to use the torchinfor.summary\n",
        "        self.unit = 2**(len(depths)-2)*window_size\n",
        "\n",
        "        self.in_chans = in_chans\n",
        "        self.dim = dim\n",
        "        self.depths = depths\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = head_dim\n",
        "        self.window_size = window_size\n",
        "        self.hidden_ratio = hidden_ratio\n",
        "        self.qkv_bias = qkv_bias\n",
        "        self.act_layer = act_layer\n",
        "        norm_layer = ReshapeLayerNorm if norm_layer == 'ReshapeLayerNorm' else norm_layer\n",
        "        self.norm_layer = norm_layer = ReshapeLayerNorm if norm_layer == 'ReshapeLayerNorm' else norm_layer\n",
        "        self.tail_mv = tail_mv\n",
        "\n",
        "        self.scale = int(target_mode[-1]) if target_mode in ['light_x2', 'light_x3', 'light_x4'] else 1\n",
        "        self.mean, self.std = mean_std(self.scale, target_mode)\n",
        "        self.target_mode = target_mode\n",
        "        self.img_norm = img_norm\n",
        "\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path, sum(depths))]  # stochastic depth decay rule\n",
        "\n",
        "        self.shallow = ShallowModule(in_chans, dim, 3, 1)\n",
        "        self.stage1 = EncoderStage(depths[1], dim, num_heads[0], chsa_head_ratio, FFAT, window_size, head_dim, qkv_bias, mv_ver,\n",
        "                                   hidden_ratio, act_layer, norm_layer, attn_drop, proj_drop, drop_path, helper, mv_act)\n",
        "\n",
        "\n",
        "        # FFSANet\n",
        "        self.cubic_11_1 = cubic_attention(dim//2, group=1, kernel=11)\n",
        "        self.cubic_7_1 = cubic_attention(dim//2, group=1, kernel=7)\n",
        "\n",
        "        self.cubic_11_2 = cubic_attention(dim//2, group=1, kernel=11)\n",
        "        self.cubic_7_2 = cubic_attention(dim//2, group=1, kernel=7)\n",
        "\n",
        "        self.cubic_11_3 = cubic_attention(dim//2, group=1, kernel=11)\n",
        "        self.cubic_7_3 = cubic_attention(dim//2, group=1, kernel=7)\n",
        "        # FFSANet\n",
        "\n",
        "\n",
        "        self.down1 = Downsizing(dim, dim, norm_layer, mv_ver, mv_act)\n",
        "        self.stage2 = EncoderStage(depths[1], dim, num_heads[1], chsa_head_ratio, FFAT, window_size, head_dim, qkv_bias, mv_ver,\n",
        "                                   hidden_ratio, act_layer, norm_layer, attn_drop, proj_drop, drop_path, helper, mv_act)\n",
        "        self.down2 = Downsizing(dim, dim, norm_layer, mv_ver, mv_act)\n",
        "        self.stage3 = EncoderStage(depths[1], dim, num_heads[2], chsa_head_ratio, FFAT, window_size, head_dim, qkv_bias, mv_ver,\n",
        "                                   hidden_ratio, act_layer, norm_layer, attn_drop, proj_drop, drop_path, helper, mv_act)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if mv_ver==3:\n",
        "            self.mobivari = MobiVari1(self.cat_dim, 3, 1, act=mv_act, out_dim=dim)\n",
        "        elif mv_ver==2:\n",
        "            self.mobivari = MobiVari2(self.cat_dim, 3, 1, act=mv_act, out_dim=dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
        "            # NAFBlock not Accepted\n",
        "            # NAFBlock---------------------------------------------\n",
        "        elif mv_ver==1:\n",
        "            self.mobivari = NAFBlock(c=self.dim)\n",
        "            # -------------------------------------------------------\n",
        "        self.act = act_layer()\n",
        "        self.norm = norm_layer(dim)\n",
        "        # self.bottleneck = Bottleneck(dim, len(depths)-1, act_layer, norm_layer, mv_ver, mv_act)\n",
        "\n",
        "        self.stage4 = EncoderStage(depths[1], dim, num_heads[3], chsa_head_ratio, FFAT, window_size, head_dim, qkv_bias, mv_ver,\n",
        "                                   hidden_ratio, act_layer, norm_layer, attn_drop, proj_drop, drop_path, helper, mv_act)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        if mv_ver==3:\n",
        "            self.mobivari_mix = MobiVari1(dim+dim//4+dim//16+dim, 3, 1, act=mv_act, out_dim=dim)\n",
        "        elif mv_ver==2:\n",
        "            self.mobivari_mix = MobiVari2(dim+dim//4+dim//16+dim, 3, 1, act=mv_act, out_dim=dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
        "        # self.attn_mix = HRAMi(dim, 3, 1, mv_ver, mv_act)\n",
        "\n",
        "\n",
        "        self.to_target = Reconstruction(target_mode, dim, 3, 1, tail_mv, mv_ver, mv_act, exp_factor, expand_groups)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def forward_size_norm(self, x):\n",
        "        _,_,h,w = x.size()\n",
        "        padh = self.unit-(h%self.unit) if h%self.unit!=0 else 0\n",
        "        padw = self.unit-(w%self.unit) if w%self.unit!=0 else 0\n",
        "        x = TF.pad(x, (0,0,padw,padh))\n",
        "        return TF.normalize(x, self.mean, self.std) if self.img_norm else x\n",
        "\n",
        "    def forward(self, x):\n",
        "        _,_,H_ori,W_ori = x.size()\n",
        "        x = self.forward_size_norm(x)\n",
        "        B, C, H, W = x.size()\n",
        "        x_lq = x if self.scale == 1 else None # due to residual image (except for only SR)\n",
        "\n",
        "        shallow = self.shallow(x)\n",
        "        o1_, attn1 = self.stage1(shallow) # [B, C, H, W]\n",
        "\n",
        "\n",
        "        # FFAT\n",
        "        out = torch.chunk(o1_, 2, dim=1)\n",
        "        out_11_1 = self.cubic_11_1(out[0])\n",
        "        out_7_1 = self.cubic_7_1(out[1])\n",
        "        o1_ = torch.cat((out_11_1, out_7_1), dim=1)\n",
        "        # FFAT\n",
        "\n",
        "\n",
        "        o1 = self.down1(o1_) # [B, C, H//2, W//2]\n",
        "\n",
        "        o2_, attn2 = self.stage2(o1) # [B, C, H//2, W//2]\n",
        "\n",
        "\n",
        "        # FFAT\n",
        "        out = torch.chunk(o2_, 2, dim=1)\n",
        "        out_11_2 = self.cubic_11_2(out[0])\n",
        "        out_7_2 = self.cubic_7_2(out[1])\n",
        "        o2_ = torch.cat((out_11_2, out_7_2), dim=1)\n",
        "        # FFAT\n",
        "\n",
        "\n",
        "        o2 = self.down2(o2_) # [B, C, H//4, W//4]\n",
        "\n",
        "        o3_, attn3 = self.stage3(o2) # [B, C, H//4, W//4]\n",
        "\n",
        "\n",
        "        # FFAT\n",
        "        out = torch.chunk(o3_, 2, dim=1)\n",
        "        out_11_3 = self.cubic_11_3(out[0])\n",
        "        out_7_3 = self.cubic_7_3(out[1])\n",
        "        o3_ = torch.cat((out_11_3, out_7_3), dim=1)\n",
        "        # FFAT\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Bottleneck\n",
        "        def shallow_down( x, r):\n",
        "          for _ in range(r):\n",
        "              x = F.max_pool2d(x, 2)\n",
        "          return F.leaky_relu(x)\n",
        "        x_list = [shallow, o1_, o2_, o3_]\n",
        "        xs = x_list[0]\n",
        "        new_x = []\n",
        "        for i in range(len(x_list[1:])):\n",
        "            x_ = x_list[i+1]+shallow_down(xs, i)\n",
        "            new_x.append(F.pixel_shuffle(x_, 2**i))\n",
        "        ob = self.norm(self.mobivari(torch.cat(new_x, dim=1)))\n",
        "        # ob = self.bottleneck([shallow, o1_, o2_, o3_]) # [B, C, H, W]\n",
        "\n",
        "\n",
        "\n",
        "        o4, attn4 = self.stage4(ob) # [B, C, H, W]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# HRAMiT\n",
        "        attn_list = [attn1, attn2, attn3, attn4]\n",
        "        for i, attn in enumerate(attn_list[:-1]):\n",
        "            attn = F.pixel_shuffle(attn, 2**i)\n",
        "            x = attn if i==0 else torch.cat([x, attn], dim=1)\n",
        "        x = torch.cat([x, attn_list[-1]], dim=1)\n",
        "        mix = self.mobivari_mix(x)\n",
        "        # mix = self.attn_mix([attn1, attn2, attn3, attn4]) # [B, C, H, W]\n",
        "\n",
        "\n",
        "\n",
        "        o4 = o4 * mix # [B, C, H, W]\n",
        "\n",
        "        out = self.to_target(o4+shallow) # global skip connection\n",
        "        out = out + x_lq if self.scale == 1 else out # residual output (except for only SR)\n",
        "\n",
        "        out = denormalize(out, self.mean, self.std) if self.img_norm else out\n",
        "        out = out[:, :, :H_ori*self.scale, :W_ori*self.scale]\n",
        "        return out\n",
        "\n",
        "    def flops(self, resolutions):\n",
        "        H_ori,W_ori = resolutions\n",
        "        padh = self.unit-(H_ori%self.unit) if H_ori%self.unit!=0 else 0\n",
        "        padw = self.unit-(W_ori%self.unit) if W_ori%self.unit!=0 else 0\n",
        "        H,W = H_ori+padh,W_ori+padw\n",
        "        flops = 0\n",
        "        flops += self.shallow.flops((H,W))\n",
        "        flops += self.stage1.flops((H,W))\n",
        "        flops += self.down1.flops((H,W))\n",
        "        flops += self.stage2.flops((H//2,W//2))\n",
        "        flops += self.down2.flops((H//2,W//2))\n",
        "        flops += self.stage3.flops((H//4,W//4))\n",
        "        flops += self.bottleneck.flops((H,W))\n",
        "        flops += self.stage4.flops((H,W))\n",
        "        flops += self.attn_mix.flops((H,W))\n",
        "        flops += self.dim * H*W # o4 = o4*mix\n",
        "        flops += self.to_target.flops((H,W))\n",
        "        return flops\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        # Swin V2 manner\n",
        "        if isinstance(m, nn.Linear):\n",
        "            trunc_normal_(m.weight, std=.02)\n",
        "            if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "            nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "    @torch.jit.ignore\n",
        "    def no_weight_decay(self):\n",
        "        nwd = set()\n",
        "        for n, _ in self.named_parameters():\n",
        "            if 'relative_position_bias_table' in n:\n",
        "                nwd.add(n)\n",
        "        return nwd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ----------------------------Shadow_R-------------------------\n",
        "    # @torch.no_grad()\n",
        "    # def _init_weights(module_list, scale=1, bias_fill=0, **kwargs):\n",
        "    #   \"\"\"Initialize network weights.\n",
        "\n",
        "    #   Args:\n",
        "    #       module_list (list[nn.Module] | nn.Module): Modules to be initialized.\n",
        "    #       scale (float): Scale initialized weights, especially for residual\n",
        "    #           blocks. Default: 1.\n",
        "    #       bias_fill (float): The value to fill bias. Default: 0\n",
        "    #       kwargs (dict): Other arguments for initialization function.\n",
        "    #   \"\"\"\n",
        "    #   if not isinstance(module_list, list):\n",
        "    #       module_list = [module_list]\n",
        "    #   for module in module_list:\n",
        "    #       for m in module.modules():\n",
        "    #           if isinstance(m, nn.Conv2d):\n",
        "    #               nn.init.kaiming_normal_(m.weight, **kwargs)\n",
        "    #               m.weight.data *= scale\n",
        "    #               if m.bias is not None:\n",
        "    #                   m.bias.data.fill_(bias_fill)\n",
        "    #           elif isinstance(m, nn.Linear):\n",
        "    #               nn.init.kaiming_normal_(m.weight, **kwargs)\n",
        "    #               m.weight.data *= scale\n",
        "    #               if m.bias is not None:\n",
        "    #                   m.bias.data.fill_(bias_fill)\n",
        "    #           elif isinstance(m, _BatchNorm):\n",
        "    #               nn.init.constant_(m.weight, 1)\n",
        "    #               if m.bias is not None:\n",
        "    #                   m.bias.data.fill_(bias_fill)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Fk6xNHiphkkk",
        "outputId": "39f43feb-0798-4b25-a2dd-e610113375cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/functional.py:534: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3595.)\n",
            "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The model has 672,151 trainable parameters\n"
          ]
        }
      ],
      "source": [
        "G1 = RAMiT_FFAT(target_mode='light_realdn',img_norm=False, mv_ver = 3)\n",
        "G1 = nn.DataParallel(G1)\n",
        "G1.to('cuda')\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in G1.parameters() if p.requires_grad)\n",
        "\n",
        "# Create the model\n",
        "\n",
        "# Print the number of parameters\n",
        "print(f'The model has {count_parameters(G1):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "S9po3TgMs1jn"
      },
      "outputs": [],
      "source": [
        "# # Load the checkpoint\n",
        "# checkpoint = torch.load(r'C:\\Users\\Admin\\Desktop\\AI_RAMiT\\FFAT_RAMiT\\SBSR70.pth', map_location='cuda')\n",
        "\n",
        "# # Remove \"module.\" prefix if it exists in the keys\n",
        "# from collections import OrderedDict\n",
        "# new_state_dict = OrderedDict()\n",
        "# for k, v in checkpoint.items():\n",
        "#     name = k[7:] if k.startswith(\"module.\") else k  # remove \"module.\" prefix\n",
        "#     new_state_dict[name] = v\n",
        "\n",
        "# # Load the modified state_dict into the model\n",
        "# G1 = RAMiT_FFAT(target_mode='light_realdn', img_norm=False, mv_ver=3)\n",
        "# G1.load_state_dict(new_state_dict)\n",
        "\n",
        "# # Apply DataParallel and move to GPU\n",
        "# G1 = nn.DataParallel(G1)\n",
        "# G1.to('cuda')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cEs_UewFzrIu",
        "outputId": "c868b38a-14ea-45e9-a5bc-02e62b0cdec1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "=========================================================================================================\n",
              "Layer (type:depth-idx)                                  Output Shape              Param #\n",
              "=========================================================================================================\n",
              "DataParallel                                            [1, 3, 360, 480]          --\n",
              "RAMiT_FFAT: 1-1                                       [1, 3, 360, 480]          --\n",
              "    ShallowModule: 2-1                               [1, 64, 384, 480]         --\n",
              "        Conv2d: 3-1                                 [1, 64, 384, 480]         1,792\n",
              "    EncoderStage: 2-2                                [1, 64, 384, 480]         --\n",
              "        ModuleList: 3-2                             --                        149,660\n",
              "    cubic_attention: 2-3                             [1, 32, 384, 480]         64\n",
              "        spatial_strip_att: 3-3                      [1, 32, 384, 480]         352\n",
              "        spatial_strip_att: 3-4                      [1, 32, 384, 480]         352\n",
              "        spatial_strip_att: 3-5                      [1, 32, 384, 480]         (recursive)\n",
              "        spatial_strip_att: 3-6                      [1, 32, 384, 480]         (recursive)\n",
              "    cubic_attention: 2-4                             [1, 32, 384, 480]         64\n",
              "        spatial_strip_att: 3-7                      [1, 32, 384, 480]         224\n",
              "        spatial_strip_att: 3-8                      [1, 32, 384, 480]         224\n",
              "        spatial_strip_att: 3-9                      [1, 32, 384, 480]         (recursive)\n",
              "        spatial_strip_att: 3-10                     [1, 32, 384, 480]         (recursive)\n",
              "    Downsizing: 2-5                                  [1, 64, 192, 240]         --\n",
              "        ReshapeLayerNorm: 3-11                      [1, 256, 192, 240]        512\n",
              "        MobiVari1: 3-12                             [1, 64, 192, 240]         19,008\n",
              "    EncoderStage: 2-6                                [1, 64, 192, 240]         --\n",
              "        ModuleList: 3-13                            --                        149,660\n",
              "    cubic_attention: 2-7                             [1, 32, 192, 240]         64\n",
              "        spatial_strip_att: 3-14                     [1, 32, 192, 240]         352\n",
              "        spatial_strip_att: 3-15                     [1, 32, 192, 240]         352\n",
              "        spatial_strip_att: 3-16                     [1, 32, 192, 240]         (recursive)\n",
              "        spatial_strip_att: 3-17                     [1, 32, 192, 240]         (recursive)\n",
              "    cubic_attention: 2-8                             [1, 32, 192, 240]         64\n",
              "        spatial_strip_att: 3-18                     [1, 32, 192, 240]         224\n",
              "        spatial_strip_att: 3-19                     [1, 32, 192, 240]         224\n",
              "        spatial_strip_att: 3-20                     [1, 32, 192, 240]         (recursive)\n",
              "        spatial_strip_att: 3-21                     [1, 32, 192, 240]         (recursive)\n",
              "    Downsizing: 2-9                                  [1, 64, 96, 120]          --\n",
              "        ReshapeLayerNorm: 3-22                      [1, 256, 96, 120]         512\n",
              "        MobiVari1: 3-23                             [1, 64, 96, 120]          19,008\n",
              "    EncoderStage: 2-10                               [1, 64, 96, 120]          --\n",
              "        ModuleList: 3-24                            --                        149,660\n",
              "    cubic_attention: 2-11                            [1, 32, 96, 120]          64\n",
              "        spatial_strip_att: 3-25                     [1, 32, 96, 120]          352\n",
              "        spatial_strip_att: 3-26                     [1, 32, 96, 120]          352\n",
              "        spatial_strip_att: 3-27                     [1, 32, 96, 120]          (recursive)\n",
              "        spatial_strip_att: 3-28                     [1, 32, 96, 120]          (recursive)\n",
              "    cubic_attention: 2-12                            [1, 32, 96, 120]          64\n",
              "        spatial_strip_att: 3-29                     [1, 32, 96, 120]          224\n",
              "        spatial_strip_att: 3-30                     [1, 32, 96, 120]          224\n",
              "        spatial_strip_att: 3-31                     [1, 32, 96, 120]          (recursive)\n",
              "        spatial_strip_att: 3-32                     [1, 32, 96, 120]          (recursive)\n",
              "    MobiVari1: 2-13                                  [1, 64, 384, 480]         --\n",
              "        Conv2d: 3-33                                [1, 84, 384, 480]         840\n",
              "        LeakyReLU: 3-34                             [1, 84, 384, 480]         --\n",
              "        Conv2d: 3-35                                [1, 64, 384, 480]         5,440\n",
              "        LeakyReLU: 3-36                             [1, 64, 384, 480]         --\n",
              "    ReshapeLayerNorm: 2-14                           [1, 64, 384, 480]         --\n",
              "        LayerNorm: 3-37                             [1, 184320, 64]           128\n",
              "    EncoderStage: 2-15                               [1, 64, 384, 480]         --\n",
              "        ModuleList: 3-38                            --                        149,660\n",
              "    MobiVari1: 2-16                                  [1, 64, 384, 480]         --\n",
              "        Conv2d: 3-39                                [1, 148, 384, 480]        1,480\n",
              "        LeakyReLU: 3-40                             [1, 148, 384, 480]        --\n",
              "        Conv2d: 3-41                                [1, 64, 384, 480]         9,536\n",
              "        LeakyReLU: 3-42                             [1, 64, 384, 480]         --\n",
              "    Reconstruction: 2-17                             [1, 3, 384, 480]          --\n",
              "        ModuleList: 3-43                            --                        9,600\n",
              "        Conv2d: 3-44                                [1, 3, 384, 480]          1,731\n",
              "        Identity: 3-45                              [1, 3, 384, 480]          --\n",
              "        Conv2d: 3-46                                [1, 3, 384, 480]          84\n",
              "=========================================================================================================\n",
              "Total params: 672,151\n",
              "Trainable params: 672,151\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (G): 68.91\n",
              "=========================================================================================================\n",
              "Input size (MB): 2.07\n",
              "Forward/backward pass size (MB): 10973.68\n",
              "Params size (MB): 2.64\n",
              "Estimated Total Size (MB): 10978.39\n",
              "========================================================================================================="
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "from torchinfo import summary\n",
        "summary(G1, (1, 3, 360, 480))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJ5hGAuX9Lr3"
      },
      "source": [
        "<!-- ## FFAT + RAMiT + NAFNet -->"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xKVDzuCxs1jp"
      },
      "source": [
        "## Error RAMiT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LMopfquO9WCb"
      },
      "outputs": [],
      "source": [
        "# # RAMiT (Reciprocal Attention Mixing Transformer)\n",
        "\n",
        "# from pdb import set_trace as stx\n",
        "# import numbers\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# from torch import nn, Tensor\n",
        "# from einops import rearrange\n",
        "# import torch.nn.functional as F\n",
        "# import torch\n",
        "# from torchvision.transforms import functional as TF\n",
        "# import math\n",
        "# from RAMiT.my_model.common.mean_std import mean_std\n",
        "# from RAMiT.utils.etc_utils import denormalize\n",
        "# from timm.models.fx_features import register_notrace_function\n",
        "# from timm.models.layers import trunc_normal_, to_2tuple\n",
        "# from torch.nn.modules.batchnorm import _BatchNorm\n",
        "# def make_model(args, opts, pe):\n",
        "#     model = RAMiT(target_mode=args.target_mode,\n",
        "#                   img_norm=args.img_norm,\n",
        "#                   in_chans=opts['in_chans'],\n",
        "#                   dim = opts['dim'],\n",
        "#                   depths = opts['depths'],\n",
        "#                   num_heads = opts['num_heads'],\n",
        "#                   head_dim = opts['head_dim'],\n",
        "#                   chsa_head_ratio = opts['chsa_head_ratio'],\n",
        "#                   window_size = opts['window_size'],\n",
        "#                   hidden_ratio = opts['hidden_ratio'],\n",
        "#                   qkv_bias = opts['qkv_bias'],\n",
        "#                   mv_ver = opts['mv_ver'],\n",
        "#                   exp_factor=opts['exp_factor'],\n",
        "#                   expand_groups=opts['expand_groups'],\n",
        "#                   act_layer = opts['act_layer'],\n",
        "#                   norm_layer = opts['norm_layer'],\n",
        "#                   tail_mv = opts['tail_mv'],\n",
        "#                   attn_drop = opts['attn_drop'],\n",
        "#                   proj_drop = opts['proj_drop'],\n",
        "#                   drop_path = opts['drop_path'],\n",
        "#                   helper = opts['helper'],\n",
        "#                   mv_act = opts['mv_act']\n",
        "#                  )\n",
        "#     num_params = sum([p.numel() for n,p in model.named_parameters()])\n",
        "#     print(format(num_params, ','))\n",
        "#     return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # ---------------------------------------------------------------------------Shadow_R---------------------------------------------------------------------------\n",
        "\n",
        "# def to_3d(x):\n",
        "#     return rearrange(x, 'b c h w -> b (h w) c')\n",
        "\n",
        "# def to_4d(x,h,w):\n",
        "#     return rearrange(x, 'b (h w) c -> b c h w',h=h,w=w)\n",
        "\n",
        "# class BiasFree_LayerNorm(nn.Module):\n",
        "#     def __init__(self, normalized_shape):\n",
        "#         super(BiasFree_LayerNorm, self).__init__()\n",
        "#         if isinstance(normalized_shape, numbers.Integral):\n",
        "#             normalized_shape = (normalized_shape,)\n",
        "#         normalized_shape = torch.Size(normalized_shape)\n",
        "\n",
        "#         assert len(normalized_shape) == 1\n",
        "\n",
        "#         self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
        "#         self.normalized_shape = normalized_shape\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         sigma = x.var(-1, keepdim=True, unbiased=False)\n",
        "#         return x / torch.sqrt(sigma+1e-5) * self.weight\n",
        "\n",
        "# class WithBias_LayerNorm(nn.Module):\n",
        "#     def __init__(self, normalized_shape):\n",
        "#         super(WithBias_LayerNorm, self).__init__()\n",
        "#         if isinstance(normalized_shape, numbers.Integral):\n",
        "#             normalized_shape = (normalized_shape,)\n",
        "#         normalized_shape = torch.Size(normalized_shape)\n",
        "\n",
        "#         assert len(normalized_shape) == 1\n",
        "\n",
        "#         self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
        "#         self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
        "#         self.normalized_shape = normalized_shape\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         mu = x.mean(-1, keepdim=True)\n",
        "#         sigma = x.var(-1, keepdim=True, unbiased=False)\n",
        "#         return (x - mu) / torch.sqrt(sigma+1e-5) * self.weight + self.bias\n",
        "\n",
        "\n",
        "# class LayerNorm(nn.Module):\n",
        "#     def __init__(self, dim, LayerNorm_type):\n",
        "#         super(LayerNorm, self).__init__()\n",
        "#         if LayerNorm_type =='BiasFree':\n",
        "#             self.body = BiasFree_LayerNorm(dim)\n",
        "#         else:\n",
        "#             self.body = WithBias_LayerNorm(dim)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         h, w = x.shape[-2:]\n",
        "#         return to_4d(self.body(to_3d(x)), h, w)\n",
        "\n",
        "\n",
        "\n",
        "# ##########################################################################\n",
        "# ## Gated-Dconv Feed-Forward Network (GDFN)\n",
        "# class FeedForwardFFAT(nn.Module):\n",
        "#     def __init__(self, dim, ffn_expansion_factor, bias):\n",
        "#         super(FeedForwardFFAT, self).__init__()\n",
        "\n",
        "#         hidden_features = int(dim*ffn_expansion_factor)\n",
        "\n",
        "#         self.project_in = nn.Conv2d(dim, hidden_features*2, kernel_size=1, bias=bias)\n",
        "\n",
        "#         self.dwconv = nn.Conv2d(hidden_features*2, hidden_features*2, kernel_size=3, stride=1, padding=1, groups=hidden_features*2, bias=bias)\n",
        "\n",
        "#         self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.project_in(x)\n",
        "#         x1, x2 = self.dwconv(x).chunk(2, dim=1)\n",
        "#         x = F.gelu(x1) * x2\n",
        "#         x = self.project_out(x)\n",
        "#         return x\n",
        "\n",
        "\n",
        "\n",
        "# class Attention(nn.Module):\n",
        "#     def __init__(self, dim, num_heads, bias):\n",
        "#         super(Attention, self).__init__()\n",
        "\n",
        "#         self.num_heads = num_heads\n",
        "\n",
        "#         self.to_hidden = nn.Conv2d(dim, dim * 6, kernel_size=1, bias=bias)\n",
        "#         self.to_hidden_dw = nn.Conv2d(dim * 6, dim * 6, kernel_size=3, stride=1, padding=1, groups=dim * 6, bias=bias)\n",
        "\n",
        "#         self.project_out = nn.Conv2d(dim * 2, dim, kernel_size=1, bias=bias)\n",
        "\n",
        "#         self.norm = LayerNorm(dim * 2, LayerNorm_type='WithBias')\n",
        "\n",
        "#         self.patch_size = 2 # 8\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         hidden = self.to_hidden(x)\n",
        "\n",
        "#         q, k, v = self.to_hidden_dw(hidden).chunk(3, dim=1)\n",
        "\n",
        "#         q_patch = rearrange(q, 'b c (h patch1) (w patch2) -> b c h w patch1 patch2', patch1=self.patch_size,\n",
        "#                             patch2=self.patch_size)\n",
        "#         k_patch = rearrange(k, 'b c (h patch1) (w patch2) -> b c h w patch1 patch2', patch1=self.patch_size,\n",
        "#                             patch2=self.patch_size)\n",
        "#         q_fft = torch.fft.rfft2(q_patch.float())\n",
        "#         k_fft = torch.fft.rfft2(k_patch.float())\n",
        "\n",
        "#         out = q_fft * k_fft\n",
        "#         out = torch.fft.irfft2(out, s=(self.patch_size, self.patch_size))\n",
        "#         out = rearrange(out, 'b c h w patch1 patch2 -> b c (h patch1) (w patch2)', patch1=self.patch_size,\n",
        "#                         patch2=self.patch_size)\n",
        "\n",
        "#         out = self.norm(out)\n",
        "\n",
        "#         output = v * out\n",
        "#         output = self.project_out(output)\n",
        "\n",
        "#         return output\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ##########################################################################\n",
        "# class TransformerBlock(nn.Module):\n",
        "#     def __init__(self, dim, num_heads, ffn_expansion_factor, bias, LayerNorm_type):\n",
        "#         super(TransformerBlock, self).__init__()\n",
        "\n",
        "#         self.norm1 = LayerNorm(dim, LayerNorm_type)\n",
        "#         self.attn = Attention(dim, num_heads, bias)\n",
        "#         self.norm2 = LayerNorm(dim, LayerNorm_type)\n",
        "#         self.ffn = FeedForwardFFAT(dim, ffn_expansion_factor, bias)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = x + self.attn(self.norm1(x))\n",
        "#         x = x + self.ffn(self.norm2(x))\n",
        "\n",
        "#         return x\n",
        "# # ---------------------------------------------------------------------------Shadow_R---------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# class ShallowModule(nn.Module):\n",
        "#     def __init__(self, in_chans, out_chans, kernel_size=3, stride=1):\n",
        "#         super(ShallowModule, self).__init__()\n",
        "\n",
        "#         self.in_chans = in_chans\n",
        "#         self.out_chans = out_chans\n",
        "#         self.kernel_size = kernel_size\n",
        "#         self.stride = stride\n",
        "\n",
        "#         self.conv = nn.Conv2d(in_chans, out_chans, kernel_size, stride, kernel_size//2)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.conv(x)\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         return resolutions[0]*resolutions[1] * self.kernel_size*self.kernel_size * self.in_chans * self.out_chans\n",
        "\n",
        "# class QKVProjection(nn.Module):\n",
        "#     def __init__(self, dim, num_head, qkv_bias=True):\n",
        "#         super(QKVProjection, self).__init__()\n",
        "#         self.dim = dim\n",
        "#         self.num_head = num_head\n",
        "\n",
        "#         self.qkv = nn.Conv2d(dim, 3*dim, 1, bias=qkv_bias)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         B, C, H, W = x.size()\n",
        "#         qkv = self.qkv(x)\n",
        "#         qkv = rearrange(qkv, 'b (l c) h w -> b l c h w', l=self.num_head)\n",
        "#         return qkv\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         return resolutions[0]*resolutions[1] * 1*1 * self.dim * 3*self.dim\n",
        "\n",
        "# def get_relative_position_index(win_h, win_w):\n",
        "#     # get pair-wise relative position index for each token inside the window\n",
        "#     coords = torch.stack(torch.meshgrid([torch.arange(win_h), torch.arange(win_w)]))  # 2, Wh, Ww\n",
        "#     coords_flatten = torch.flatten(coords, 1)  # 2, Wh*Ww\n",
        "#     relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]  # 2, Wh*Ww, Wh*Ww (xaxis matrix & yaxis matrix)\n",
        "#     relative_coords = relative_coords.permute(1, 2, 0).contiguous()  # Wh*Ww, Wh*Ww, 2\n",
        "#     relative_coords[:, :, 0] += win_h - 1  # shift to start from 0\n",
        "#     relative_coords[:, :, 1] += win_w - 1\n",
        "#     relative_coords[:, :, 0] *= 2 * win_w - 1\n",
        "#     return relative_coords.sum(-1)  # Wh*Ww, Wh*Ww\n",
        "\n",
        "# class SpatialSelfAttention(nn.Module):\n",
        "#     def __init__(self, dim, num_head, total_head, window_size=8, shift=0, attn_drop=0.0, proj_drop=0.0, helper=True):\n",
        "#         super(SpatialSelfAttention, self).__init__()\n",
        "#         self.dim = dim\n",
        "#         self.num_head = num_head\n",
        "#         self.total_head = total_head\n",
        "#         self.window_size = window_size\n",
        "#         self.window_area = window_size**2\n",
        "#         self.shift = shift\n",
        "#         self.helper = helper\n",
        "\n",
        "#         self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_head, 1, 1))), requires_grad=True)\n",
        "\n",
        "#         # define a parameter table of relative position bias, shape: 2*Wh-1 * 2*Ww-1, nH\n",
        "#         self.relative_position_bias_table = nn.Parameter(torch.zeros((2 * window_size - 1) * (2 * window_size - 1), num_head))\n",
        "\n",
        "#         # get pair-wise relative position index for each token inside the window\n",
        "#         self.register_buffer(\"relative_position_index\", get_relative_position_index(window_size, window_size))\n",
        "\n",
        "#         self.attn_drop = nn.Dropout(attn_drop)\n",
        "\n",
        "#         self.proj = nn.Conv2d(dim*num_head, dim*num_head, 1)\n",
        "#         self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "#     def _get_rel_pos_bias(self) -> torch.Tensor:\n",
        "#         relative_position_bias = self.relative_position_bias_table[\n",
        "#             self.relative_position_index.view(-1)].view(self.window_area, self.window_area, -1)  # Wh*Ww,Wh*Ww,nH\n",
        "#         relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()  # nH, Wh*Ww, Wh*Ww\n",
        "#         return relative_position_bias.unsqueeze(0)\n",
        "\n",
        "#     def forward(self, qkv, ch=None):\n",
        "#         B, L, C, H, W = qkv.size()\n",
        "#         # window shift\n",
        "#         if self.shift > 0:\n",
        "#             qkv = torch.roll(qkv, shifts=(-self.shift, -self.shift), dims=(-2,-1))\n",
        "\n",
        "#         # window partition\n",
        "#         q,k,v = rearrange(qkv, 'b l c (h wh) (w ww) -> (b h w) l (wh ww) c',\n",
        "#                           wh=self.window_size, ww=self.window_size).chunk(3, dim=-1) # [B_, L1, hw, C/L] respectively\n",
        "#         if ch is not None and self.helper: # [B, C, H, W]\n",
        "#             if self.shift > 0:\n",
        "#                 ch = torch.roll(ch, shifts=(-self.shift, -self.shift), dims=(-2,-1))\n",
        "#             ch = rearrange(ch, 'b (l c) (h wh) (w ww) -> (b h w) l (wh ww) c',\n",
        "#                            l=self.total_head-self.num_head, wh=self.window_size, ww=self.window_size) # [B_, L1, hw, C/L]\n",
        "#             ch = torch.mean(ch, dim=1, keepdim=True) # head squeeze [B_, 1, hw, C/L]\n",
        "#             v = v*ch # [B_, L1, hw, C/L]\n",
        "\n",
        "#         attn = F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(2,-1) # [B_, L1, hw, hw]\n",
        "#         logit_scale = torch.clamp(self.logit_scale, max=math.log(1. / 0.01)).exp()\n",
        "#         attn = attn * logit_scale\n",
        "\n",
        "#         attn = attn + self._get_rel_pos_bias()\n",
        "#         attn = self.attn_drop(F.softmax(attn, dim=-1))\n",
        "\n",
        "#         x = attn @ v # [B_, L1, hw, C/L]\n",
        "\n",
        "#         # window unpartition + head merge\n",
        "#         x = window_unpartition(x, (H,W), self.window_size) # [B, L1*C/L, H, W]\n",
        "#         x = self.proj_drop(self.proj(x))\n",
        "\n",
        "#         # window reverse shift\n",
        "#         if self.shift > 0:\n",
        "#             x = torch.roll(x, shifts=(self.shift, self.shift), dims=(-2,-1))\n",
        "\n",
        "#         return x\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         H,W = resolutions\n",
        "#         num_wins = H//self.window_size * W//self.window_size\n",
        "#         flops = self.num_head * H*W * self.dim if self.helper else 0 # v = v*ch\n",
        "#         flops += num_wins * self.num_head * self.window_area * self.dim * self.window_area # attn = Q@K^T\n",
        "#         flops += num_wins * self.num_head * self.window_area * self.window_area * self.dim # attn@V\n",
        "#         flops += H*W * 1*1 * self.num_head*self.dim * self.num_head*self.dim # self.proj\n",
        "#         return flops\n",
        "\n",
        "# @register_notrace_function\n",
        "# def window_unpartition(x, resolutions, window_size):\n",
        "#     return rearrange(x, '(b h w) l (wh ww) c -> b (l c) (h wh) (w ww)',\n",
        "#                      h=resolutions[0]//window_size, w=resolutions[1]//window_size, wh=window_size)\n",
        "\n",
        "# class ChannelSelfAttention(nn.Module):\n",
        "#     def __init__(self, dim, num_head, total_head, attn_drop=0.0, proj_drop=0.0, helper=True):\n",
        "#         super(ChannelSelfAttention, self).__init__()\n",
        "#         self.dim = dim\n",
        "#         self.num_head = num_head\n",
        "#         self.total_head = total_head\n",
        "#         self.helper = helper\n",
        "\n",
        "#         self.logit_scale = nn.Parameter(torch.log(10 * torch.ones((num_head, 1, 1))), requires_grad=True)\n",
        "\n",
        "#         self.attn_drop = nn.Dropout(attn_drop)\n",
        "\n",
        "#         self.proj = nn.Conv2d(dim*num_head, dim*num_head, 1)\n",
        "#         self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "#     def forward(self, qkv, sp=None):\n",
        "#         B, L, C, H, W = qkv.size()\n",
        "\n",
        "#         q,k,v = rearrange(qkv, 'b l c h w -> b l c (h w)').chunk(3, dim=-2) # [B, L2, C/L, HW]\n",
        "#         if sp is not None and self.helper:\n",
        "#             sp = torch.mean(sp, dim=1, keepdim=True) # channel squeeze # [B, 1, H, W]\n",
        "#             sp = rearrange(sp, 'b (l c) h w -> b l c (h w)', l=1) # [B, 1, 1, HW]\n",
        "#             v = v*sp # [B, L2, C/L, HW]\n",
        "\n",
        "#         attn = F.normalize(q, dim=-1) @ F.normalize(k, dim=-1).transpose(2,-1) # [B, L2, C/L, C/L]\n",
        "#         logit_scale = torch.clamp(self.logit_scale, max=math.log(1. / 0.01)).exp()\n",
        "#         attn = attn * logit_scale\n",
        "\n",
        "#         attn = F.softmax(attn, dim=-1)\n",
        "#         attn = self.attn_drop(attn)\n",
        "\n",
        "#         x = attn @ v # [B, L2, C/L, HW]\n",
        "\n",
        "#         # head merge\n",
        "#         x = rearrange(x, 'b l c (h w) -> b (l c) h w', h=H) # [B, L2*C/L, H, W]\n",
        "#         x = self.proj_drop(self.proj(x)) # [B, L2*C/L, H, W]\n",
        "\n",
        "#         return x\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         H,W = resolutions\n",
        "#         flops = self.num_head * self.dim * H*W if self.helper else 0 # v = v*sp\n",
        "#         flops += self.num_head * self.dim * H*W * self.dim # attn = Q@K^T\n",
        "#         flops += self.num_head * self.dim * self.dim * H*W # attn@V\n",
        "#         flops += H*W * 1*1 * self.num_head*self.dim * self.num_head*self.dim # self.proj\n",
        "#         return flops\n",
        "\n",
        "# class ReshapeLayerNorm(nn.Module):\n",
        "#     def __init__(self, dim, norm_layer=nn.LayerNorm):\n",
        "#         super(ReshapeLayerNorm, self).__init__()\n",
        "\n",
        "#         self.dim = dim\n",
        "#         self.norm = norm_layer(dim)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         B, C, H, W = x.size()\n",
        "#         x = rearrange(x, 'b c h w -> b (h w) c')\n",
        "#         x = self.norm(x)\n",
        "#         x = rearrange(x, 'b (h w) c -> b c h w', h=H)\n",
        "#         return x\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         H,W = resolutions\n",
        "#         flops = 0\n",
        "#         flops += H*W * self.dim\n",
        "#         return flops\n",
        "\n",
        "# class MobiVari1(nn.Module): # MobileNet v1 Variants\n",
        "#     def __init__(self, dim, kernel_size, stride, act=nn.LeakyReLU, out_dim=None):\n",
        "#         super(MobiVari1, self).__init__()\n",
        "#         self.dim = dim\n",
        "#         self.kernel_size = kernel_size\n",
        "#         self.out_dim = out_dim or dim\n",
        "\n",
        "#         self.dw_conv = nn.Conv2d(dim, dim, kernel_size, stride, kernel_size//2, groups=dim)\n",
        "#         self.pw_conv = nn.Conv2d(dim, self.out_dim, 1, 1, 0)\n",
        "#         self.act = act()\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         out = self.act(self.pw_conv(self.act(self.dw_conv(x))+x))\n",
        "#         return out + x if self.dim==self.out_dim else out\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         H,W = resolutions\n",
        "#         flops = H*W * self.kernel_size*self.kernel_size * self.dim  +  H*W * 1*1 * self.dim * self.out_dim # self.dw_conv + self.pw_conv\n",
        "#         return flops\n",
        "\n",
        "# class MobiVari2(MobiVari1): # MobileNet v2 Variants\n",
        "#     def __init__(self, dim, kernel_size, stride, act=nn.LeakyReLU, out_dim=None, exp_factor=1.2, expand_groups=4):\n",
        "#         super(MobiVari2, self).__init__(dim, kernel_size, stride, act, out_dim)\n",
        "#         self.expand_groups = expand_groups\n",
        "#         expand_dim = int(dim*exp_factor)\n",
        "#         expand_dim = expand_dim+(expand_groups-expand_dim%expand_groups)\n",
        "#         self.expand_dim = expand_dim\n",
        "\n",
        "#         self.exp_conv = nn.Conv2d(dim, self.expand_dim, 1, 1, 0, groups=expand_groups)\n",
        "#         self.dw_conv = nn.Conv2d(expand_dim, expand_dim, kernel_size, stride, kernel_size//2, groups=expand_dim)\n",
        "#         self.pw_conv = nn.Conv2d(expand_dim, self.out_dim, 1, 1, 0)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x1 = self.act(self.exp_conv(x))\n",
        "#         out = self.pw_conv(self.act(self.dw_conv(x1)+x1))\n",
        "#         return out + x if self.dim==self.out_dim else out\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         H,W = resolutions\n",
        "#         flops = H*W * 1*1 * (self.dim//self.expand_groups) * self.expand_dim # self.exp_conv\n",
        "#         flops += H*W * self.kernel_size*self.kernel_size * self.expand_dim # self.dw_conv\n",
        "#         flops += H*W * 1*1 * self.expand_dim * self.out_dim # self.pw_conv\n",
        "#         return flops\n",
        "\n",
        "# class FeedForward(nn.Module):\n",
        "#     def __init__(self, dim, hidden_ratio, act_layer=nn.GELU, bias=True, drop=0.0):\n",
        "#         super(FeedForward, self).__init__()\n",
        "\n",
        "#         self.dim = dim\n",
        "#         self.hidden_ratio = hidden_ratio\n",
        "\n",
        "#         self.hidden = nn.Conv2d(dim, int(dim*hidden_ratio), 1, bias=bias)\n",
        "#         self.drop1 = nn.Dropout(drop)\n",
        "#         self.out = nn.Conv2d(int(dim*hidden_ratio), dim, 1, bias=bias)\n",
        "#         self.drop2 = nn.Dropout(drop)\n",
        "#         self.act = act_layer()\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         return self.drop2(self.out(self.drop1(self.act(self.hidden(x)))))\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         H,W = resolutions\n",
        "#         flops = 2 * H*W * 1*1 * self.dim * self.dim*self.hidden_ratio # self.hidden + self.out\n",
        "#         return flops\n",
        "\n",
        "# class NoLayer(nn.Identity):\n",
        "#     def __init__(self):\n",
        "#         super(NoLayer, self).__init__()\n",
        "#     def flops(self, resolutions):\n",
        "#         return 0\n",
        "#     def forward(self, x, **kwargs):\n",
        "#         return x.flatten(1,2)\n",
        "\n",
        "# class DRAMiTransformer(nn.Module): # Reciprocal Attention Transformer Block\n",
        "#     def __init__(self, dim, num_head, chsa_head_ratio, FFAT, window_size=8, shift=0, head_dim=None, qkv_bias=True, mv_ver=1,\n",
        "#                  hidden_ratio=2.0, act_layer=nn.GELU, norm_layer=ReshapeLayerNorm, attn_drop=0.0, proj_drop=0.0, drop_path=0.0, helper=True,\n",
        "#                  mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4,\n",
        "#                  num_blocks = [4,6,6,8], heads = [1,2,4,8], ffn_expansion_factor = 2.66, bias = False, LayerNorm_type = 'WithBias'):\n",
        "#         super(DRAMiTransformer, self).__init__()\n",
        "\n",
        "#         self.dim = dim\n",
        "#         self.num_head = num_head\n",
        "#         self.window_size = window_size\n",
        "#         self.chsa_head = int(num_head*chsa_head_ratio)\n",
        "#         self.shift = shift\n",
        "#         self.helper = helper\n",
        "#         self.FFAT = FFAT\n",
        "\n",
        "#         self.qkv_proj = QKVProjection(dim, num_head, qkv_bias=qkv_bias)\n",
        "#         self.sp_attn = SpatialSelfAttention(dim//num_head, num_head-self.chsa_head, num_head,\n",
        "#                                             window_size, shift, attn_drop, proj_drop, helper) if num_head-self.chsa_head != 0 else NoLayer()\n",
        "#         self.ch_attn = ChannelSelfAttention(dim//num_head, self.chsa_head, num_head, attn_drop, proj_drop, helper) if self.chsa_head != 0 else NoLayer()\n",
        "\n",
        "\n",
        "#         #FFAT\n",
        "#         if self.FFAT:\n",
        "#           self.encoder_level1 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
        "#         #FFAT\n",
        "\n",
        "\n",
        "#         if mv_ver==1:\n",
        "#             self.mobivari = MobiVari1(dim, 3, 1, act=mv_act)\n",
        "#         elif mv_ver==2:\n",
        "#             self.mobivari = MobiVari2(dim, 3, 1, act=mv_act, out_dim=None, exp_factor=exp_factor, expand_groups=expand_groups)\n",
        "\n",
        "#             # NAFBlock---------------------------------------------\n",
        "#         elif mv_ver==3:\n",
        "#             self.mobivari = NAFBlock(c=self.dim)\n",
        "#             # -------------------------------------------------------\n",
        "\n",
        "#         self.norm1 = norm_layer(dim)\n",
        "\n",
        "#         self.ffn = FeedForward(dim, hidden_ratio, act_layer=act_layer)\n",
        "#         self.norm2 = norm_layer(dim)\n",
        "\n",
        "#         self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "\n",
        "#     def forward(self, x, sp_=None, ch_=None):\n",
        "#         B, C, H, W = x.size()\n",
        "\n",
        "#         # QKV projection + head split\n",
        "#         qkv = self.qkv_proj(x) # [B, L, C, H, W]\n",
        "\n",
        "#         # SP-SA / CH-SA\n",
        "#         sp = self.sp_attn(qkv[:,:self.num_head-self.chsa_head], ch=ch_) # [B, L1*C/L, H, W]\n",
        "#         ch = self.ch_attn(qkv[:,self.num_head-self.chsa_head:], sp=sp_) # [B, L2*C/L, H, W]\n",
        "\n",
        "\n",
        "#         #FFAT\n",
        "#         if self.FFAT:\n",
        "#           FFAT_block = self.encoder_level1(torch.cat([sp, ch], dim=1))\n",
        "#           attn0 = self.mobivari(FFAT_block)\n",
        "#         #FFAT\n",
        "\n",
        "\n",
        "#         else:\n",
        "#           attn0 = self.mobivari(torch.cat([sp, ch], dim=1)) # merge [B, C, H, W]\n",
        "#         attn = self.drop_path(self.norm1(attn0)) + x # LN, skip connection [B, C, H, W]\n",
        "\n",
        "#         # FFN\n",
        "#         out = self.drop_path(self.norm2(self.ffn(attn))) + attn # FFN, LN, skip connection [B, C, H, W]\n",
        "\n",
        "#         return out, sp, ch, attn0\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         flops = self.qkv_proj.flops(resolutions)\n",
        "#         flops += self.sp_attn.flops(resolutions)\n",
        "#         flops += self.ch_attn.flops(resolutions)\n",
        "#         flops += self.mobivari.flops(resolutions)\n",
        "#         flops += self.norm1.flops(resolutions)\n",
        "#         flops += self.ffn.flops(resolutions)\n",
        "#         flops += self.norm2.flops(resolutions)\n",
        "#         params = sum([p.numel() for n,p in self.named_parameters()])\n",
        "#         return flops\n",
        "\n",
        "# class EncoderStage(nn.Module):\n",
        "#     def __init__(self, depth, dim, num_head, chsa_head_ratio, FFAT, window_size=8, head_dim=None,\n",
        "#                  qkv_bias=True, mv_ver=1, hidden_ratio=2.0, act_layer=nn.GELU, norm_layer=ReshapeLayerNorm,\n",
        "#                  attn_drop=0.0, proj_drop=0.0, drop_path=0.0, helper=True, mv_act=nn.LeakyReLU,):\n",
        "#         super(EncoderStage, self).__init__()\n",
        "\n",
        "#         self.depth = depth\n",
        "#         self.dim = dim\n",
        "#         self.num_head = num_head\n",
        "#         self.window_size = window_size\n",
        "#         shift = window_size//2\n",
        "\n",
        "#         self.blocks = nn.ModuleList()\n",
        "#         for d in range(depth):\n",
        "#             self.blocks.add_module(f'block{d}', DRAMiTransformer(dim, num_head, chsa_head_ratio, FFAT,\n",
        "#                                                                  window_size, 0 if d%2==0 else shift, head_dim, qkv_bias,\n",
        "#                                                                  mv_ver, hidden_ratio, act_layer, norm_layer,\n",
        "#                                                                  attn_drop, proj_drop, drop_path, helper, mv_act,\n",
        "#                                                                  ))\n",
        "#     def forward(self, x):\n",
        "#         sp, ch = None, None\n",
        "#         for i, blk in enumerate(self.blocks):\n",
        "#             x, sp, ch, attn = blk(x, sp, ch)\n",
        "#             sp, ch = (None, None) if (sp.size(1)==0 or ch.size(1)==0) else (sp, ch) # pure SPSA or CHSA\n",
        "#         return x, attn\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         flops = 0\n",
        "#         for blk in self.blocks:\n",
        "#             flops += blk.flops(resolutions)\n",
        "#         return flops\n",
        "\n",
        "# class Downsizing(nn.Module):\n",
        "#     \"\"\" Patch Merging Layer.\n",
        "\n",
        "#     Args:\n",
        "#         dim (int): Number of input dimension (channels).\n",
        "#         downsample_dim (int, optional): Number of output dimension (channels) (dim if not set).  Default: None\n",
        "#         norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n",
        "#     \"\"\"\n",
        "\n",
        "#     def __init__(self, dim, downsample_dim=None, norm_layer=ReshapeLayerNorm, mv_ver=1, mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4):\n",
        "#         super(Downsizing, self).__init__()\n",
        "#         self.dim = dim\n",
        "#         self.downsample_dim = downsample_dim or dim\n",
        "#         self.norm = norm_layer(4*dim)\n",
        "#         if mv_ver==1:\n",
        "#             self.reduction = MobiVari1(4*dim, 3, 1, act=mv_act, out_dim=self.downsample_dim)\n",
        "#         elif mv_ver==2:\n",
        "#             self.reduction = MobiVari2(4*dim, 3, 1, act=mv_act, out_dim=self.downsample_dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
        "\n",
        "#             # NAFBlock---------------------------------------------\n",
        "#         elif mv_ver==3:\n",
        "#             self.reduction = NAFBlock(c=4*dim)\n",
        "#             # -------------------------------------------------------\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         B, C, H, W = x.size()\n",
        "\n",
        "#         # Concat 2x2\n",
        "#         x0 = x[:, :, 0::2, 0::2]  # [B, C, H/2, W/2], top-left\n",
        "#         x1 = x[:, :, 0::2, 1::2]  # [B, C, H/2, W/2], top-right\n",
        "#         x2 = x[:, :, 1::2, 0::2]  # [B, C, H/2, W/2], bottom-left\n",
        "#         x3 = x[:, :, 1::2, 1::2]  # [B, C, H/2, W/2], bottom-right\n",
        "#         x = torch.cat([x0, x1, x2, x3], dim=1)  # [B, 4C, H/2, W/2]\n",
        "#         return self.reduction(self.norm(x)) # [B, C, H/2, W/2]\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         H,W = resolutions\n",
        "#         flops = self.norm.flops((H//2,W//2)) + self.reduction.flops((H//2,W//2))\n",
        "#         return flops\n",
        "\n",
        "# class Bottleneck(nn.Module):\n",
        "#     def __init__(self, dim, num_stages, act_layer=nn.GELU, norm_layer=ReshapeLayerNorm, mv_ver=1, mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4):\n",
        "#         super(Bottleneck, self).__init__()\n",
        "\n",
        "#         self.dim = dim\n",
        "#         self.cat_dim = sum([dim//(4**x) for x in range(num_stages)])\n",
        "#         if mv_ver==1:\n",
        "#             self.mobivari = MobiVari1(self.cat_dim, 3, 1, act=mv_act, out_dim=dim)\n",
        "#         elif mv_ver==2:\n",
        "#             self.mobivari = MobiVari2(self.cat_dim, 3, 1, act=mv_act, out_dim=dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
        "\n",
        "#             # NAFBlock---------------------------------------------\n",
        "#         elif mv_ver==3:\n",
        "#             self.mobivari = NAFBlock(c=self.dim)\n",
        "#             # -------------------------------------------------------\n",
        "#         self.act = act_layer()\n",
        "#         self.norm = norm_layer(dim)\n",
        "\n",
        "#     def shallow_down(self, x, r):\n",
        "#         for _ in range(r):\n",
        "#             x = F.max_pool2d(x, 2)\n",
        "#         return F.leaky_relu(x)\n",
        "\n",
        "#     def forward(self, x_list):\n",
        "#         xs = x_list[0]\n",
        "#         new_x = []\n",
        "#         for i in range(len(x_list[1:])):\n",
        "#             x_ = x_list[i+1]+self.shallow_down(xs, i)\n",
        "#             new_x.append(F.pixel_shuffle(x_, 2**i))\n",
        "#         new_x = self.norm(self.mobivari(torch.cat(new_x, dim=1)))\n",
        "#         return new_x\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         H,W = resolutions\n",
        "#         flops = 0\n",
        "#         # self.shallow_down (iterative max-pool)\n",
        "#         flops += (H//2)*(W//2) * 2*2 * self.dim # shallow-down into stage2 output (iter1)\n",
        "#         flops += (H//2)*(W//2) * 2*2 * self.dim # shallow-down into stage3 output (iter1)\n",
        "#         flops += (H//4)*(W//4) * 2*2 * self.dim # shallow-down into stage3 output (iter2)\n",
        "#         flops += self.mobivari.flops((H,W))\n",
        "#         flops += self.norm.flops((H,W))\n",
        "#         return flops\n",
        "\n",
        "# class HRAMi(nn.Module):\n",
        "#     def __init__(self, dim, kernel_size=3, stride=1, mv_ver=1, mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4):\n",
        "#         super(HRAMi, self).__init__()\n",
        "\n",
        "#         self.dim = dim\n",
        "#         self.kernel_size = kernel_size\n",
        "\n",
        "#         if mv_ver==1:\n",
        "#             self.mobivari = MobiVari1(dim+dim//4+dim//16+dim, kernel_size, stride, act=mv_act, out_dim=dim)\n",
        "#         elif mv_ver==2:\n",
        "#             self.mobivari = MobiVari2(dim+dim//4+dim//16+dim, kernel_size, stride, act=mv_act, out_dim=dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
        "#             # NAFBlock---------------------------------------------\n",
        "#         elif mv_ver==3:\n",
        "#             self.mobivari = NAFBlock(c=self.dim)\n",
        "#             # -------------------------------------------------------\n",
        "\n",
        "#     def forward(self, attn_list):\n",
        "#         for i, attn in enumerate(attn_list[:-1]):\n",
        "#             attn = F.pixel_shuffle(attn, 2**i)\n",
        "#             x = attn if i==0 else torch.cat([x, attn], dim=1)\n",
        "#         x = torch.cat([x, attn_list[-1]], dim=1)\n",
        "#         x = self.mobivari(x)\n",
        "#         return x\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         return self.mobivari.flops(resolutions)\n",
        "\n",
        "# class Reconstruction(nn.Module):\n",
        "#     def __init__(self, target_mode, dim, kernel_size=3, stride=1, num_mv=2, mv_ver=1, mv_act=nn.LeakyReLU, exp_factor=1.2, expand_groups=4):\n",
        "#         super(Reconstruction, self).__init__()\n",
        "\n",
        "#         self.upscale = upscale = int(target_mode[-1]) if target_mode in ['light_x2', 'light_x3', 'light_x4'] else 1\n",
        "#         self.target_mode = target_mode\n",
        "#         self.dim = dim\n",
        "#         self.out_chans = 3 if 'gray' not in target_mode else 1\n",
        "#         self.kernel_size = kernel_size\n",
        "#         self.stride = stride\n",
        "#         self.num_mv = num_mv\n",
        "\n",
        "#         self.mobivari = nn.ModuleList()\n",
        "#         for i in range(num_mv):\n",
        "#             if mv_ver==1:\n",
        "#                 self.mobivari.add_module(f'mobivari{i}', MobiVari1(dim, kernel_size, stride, mv_act))\n",
        "#             elif mv_ver==2:\n",
        "#                 self.mobivari.add_module(f'mobivari{i}', MobiVari2(dim, kernel_size, stride, mv_act, None, exp_factor, expand_groups))\n",
        "#                 # NAFBlock---------------------------------------------\n",
        "#             elif mv_ver==3:\n",
        "#                 self.mobivari.add_module(f'mobivari{i}', NAFBlock(c=self.dim))\n",
        "#                 # -------------------------------------------------------\n",
        "\n",
        "\n",
        "#         self.conv = nn.Conv2d(dim, self.out_chans*(upscale**2), kernel_size, stride, kernel_size//2)\n",
        "#         self.shuffle = nn.PixelShuffle(upscale) if upscale!=1 else nn.Identity()\n",
        "#         self.final_conv = nn.Conv2d(self.out_chans, self.out_chans, kernel_size, stride, kernel_size//2)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         for mobivari in self.mobivari:\n",
        "#             x = mobivari(x)\n",
        "#         return self.final_conv(self.shuffle(self.conv(x)))\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         H,W = resolutions\n",
        "#         flops = 0\n",
        "#         for mobivari in self.mobivari: # self.mobivari\n",
        "#             flops += mobivari.flops((H,W))\n",
        "#         flops += H*W * self.kernel_size*self.kernel_size * self.dim * self.out_chans*(self.upscale**2) # self.conv\n",
        "#         flops += H*W * self.kernel_size*self.kernel_size * self.out_chans * self.out_chans # self.final_conv\n",
        "#         return flops\n",
        "\n",
        "# class RAMiT_FFAT(nn.Module):\n",
        "#     def __init__(self, in_chans=3, dim=64, depths=(1,6,4,4,6), num_heads=(4,4,4,4), head_dim=None, chsa_head_ratio=0.25,\n",
        "#                  window_size=8, hidden_ratio=2.0, qkv_bias=True, mv_ver=1, exp_factor=1.2, expand_groups=4,\n",
        "#                  act_layer=nn.GELU, norm_layer=ReshapeLayerNorm, tail_mv=2, weight_init='', target_mode='light_x2', img_norm=True,\n",
        "#                  attn_drop=0.0, proj_drop=0.0, drop_path=0.0, helper=True, mv_act=nn.LeakyReLU,\n",
        "#                  num_blocks = [1,4,6,6,8],\n",
        "#                  heads = [1,2,4,8],\n",
        "#                  ffn_expansion_factor = 2.66,\n",
        "#                  bias = False,\n",
        "#                  LayerNorm_type = 'WithBias',   ## Other option 'BiasFree'\n",
        "#                  FFAT = False, **kwargs):\n",
        "#         super(RAMiT_FFAT, self).__init__()\n",
        "#         self.cat_dim = sum([dim//(4**x) for x in range(len(depths)-1)]) # added to use the torchinfor.summary\n",
        "#         self.unit = 2**(len(depths)-2)*window_size\n",
        "\n",
        "#         self.in_chans = in_chans\n",
        "#         self.dim = dim\n",
        "#         self.depths = depths\n",
        "#         self.num_heads = num_heads\n",
        "#         self.head_dim = head_dim\n",
        "#         self.window_size = window_size\n",
        "#         self.hidden_ratio = hidden_ratio\n",
        "#         self.qkv_bias = qkv_bias\n",
        "#         self.act_layer = act_layer\n",
        "#         norm_layer = ReshapeLayerNorm if norm_layer == 'ReshapeLayerNorm' else norm_layer\n",
        "#         self.norm_layer = norm_layer = ReshapeLayerNorm if norm_layer == 'ReshapeLayerNorm' else norm_layer\n",
        "#         self.tail_mv = tail_mv\n",
        "\n",
        "#         self.scale = int(target_mode[-1]) if target_mode in ['light_x2', 'light_x3', 'light_x4'] else 1\n",
        "#         self.mean, self.std = mean_std(self.scale, target_mode)\n",
        "#         self.target_mode = target_mode\n",
        "#         self.img_norm = img_norm\n",
        "\n",
        "#         dpr = [x.item() for x in torch.linspace(0, drop_path, sum(depths))]  # stochastic depth decay rule\n",
        "\n",
        "#         self.shallow = ShallowModule(in_chans, dim, 3, 1)\n",
        "#         self.stage1 = EncoderStage(depths[0], dim, num_heads[0], chsa_head_ratio, FFAT, window_size, head_dim, qkv_bias, mv_ver,\n",
        "#                                    hidden_ratio, act_layer, norm_layer, attn_drop, proj_drop, drop_path, helper, mv_act)\n",
        "\n",
        "\n",
        "#         # FFAT\n",
        "#         self.encoder_level1 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
        "\n",
        "#         self.encoder_level2 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
        "\n",
        "#         self.encoder_level3 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
        "#         # FFAT\n",
        "\n",
        "\n",
        "#         self.down1 = Downsizing(dim, dim, norm_layer, mv_ver, mv_act)\n",
        "#         self.stage2 = EncoderStage(depths[0], dim, num_heads[1], chsa_head_ratio, FFAT, window_size, head_dim, qkv_bias, mv_ver,\n",
        "#                                    hidden_ratio, act_layer, norm_layer, attn_drop, proj_drop, drop_path, helper, mv_act)\n",
        "#         self.down2 = Downsizing(dim, dim, norm_layer, mv_ver, mv_act)\n",
        "#         self.stage3 = EncoderStage(depths[0], dim, num_heads[2], chsa_head_ratio, FFAT, window_size, head_dim, qkv_bias, mv_ver,\n",
        "#                                    hidden_ratio, act_layer, norm_layer, attn_drop, proj_drop, drop_path, helper, mv_act)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#         if mv_ver==1:\n",
        "#             self.mobivari_b = MobiVari1(self.cat_dim, 3, 1, act=mv_act, out_dim=dim)\n",
        "#         elif mv_ver==2:\n",
        "#             self.mobivari_b = MobiVari2(self.cat_dim, 3, 1, act=mv_act, out_dim=dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
        "#             # NAFBlock---------------------------------------------\n",
        "#         elif mv_ver==3:\n",
        "#             self.mobivari_b = NAFBlock(c=self.dim, out_channel=dim)\n",
        "#             # -------------------------------------------------------\n",
        "#         self.act = act_layer()\n",
        "#         self.norm = norm_layer(dim)\n",
        "#         # self.bottleneck = Bottleneck(dim, len(depths)-1, act_layer, norm_layer, mv_ver, mv_act)\n",
        "\n",
        "#         self.stage4 = EncoderStage(depths[0], dim, num_heads[3], chsa_head_ratio, FFAT, window_size, head_dim, qkv_bias, mv_ver,\n",
        "#                                    hidden_ratio, act_layer, norm_layer, attn_drop, proj_drop, drop_path, helper, mv_act)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#         if mv_ver==1:\n",
        "#             self.mobivari_mix = MobiVari1(dim+dim//4+dim//16+dim, 3, 1, act=mv_act, out_dim=dim)\n",
        "#         elif mv_ver==2:\n",
        "#             self.mobivari_mix = MobiVari2(dim+dim//4+dim//16+dim, 3, 1, act=mv_act, out_dim=dim, exp_factor=exp_factor, expand_groups=expand_groups)\n",
        "#              # NAFBlock---------------------------------------------\n",
        "#         elif mv_ver==3:\n",
        "#             self.mobivari_mix = NAFBlock(c=dim+dim//4+dim//16+dim, out_channel=dim)\n",
        "#             # -------------------------------------------------------\n",
        "#         # self.attn_mix = HRAMi(dim, 3, 1, mv_ver, mv_act)\n",
        "\n",
        "\n",
        "#         self.to_target = Reconstruction(target_mode, dim, 3, 1, tail_mv, mv_ver, mv_act, exp_factor, expand_groups)\n",
        "\n",
        "#         self.apply(self._init_weights)\n",
        "\n",
        "#     def forward_size_norm(self, x):\n",
        "#         _,_,h,w = x.size()\n",
        "#         padh = self.unit-(h%self.unit) if h%self.unit!=0 else 0\n",
        "#         padw = self.unit-(w%self.unit) if w%self.unit!=0 else 0\n",
        "#         x = TF.pad(x, (0,0,padw,padh))\n",
        "#         return TF.normalize(x, self.mean, self.std) if self.img_norm else x\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         _,_,H_ori,W_ori = x.size()\n",
        "#         x = self.forward_size_norm(x)\n",
        "#         B, C, H, W = x.size()\n",
        "#         x_lq = x if self.scale == 1 else None # due to residual image (except for only SR)\n",
        "\n",
        "#         shallow = self.shallow(x)\n",
        "#         o1_, attn1 = self.stage1(shallow) # [B, C, H, W]\n",
        "\n",
        "\n",
        "#         # FFAT\n",
        "#         o1_ = self.encoder_level1(o1_)\n",
        "#         # FFAT\n",
        "\n",
        "\n",
        "#         o1 = self.down1(o1_) # [B, C, H//2, W//2]\n",
        "\n",
        "#         o2_, attn2 = self.stage2(o1) # [B, C, H//2, W//2]\n",
        "\n",
        "\n",
        "#         # FFAT\n",
        "#         o2_ = self.encoder_level2(o2_)\n",
        "#         # FFAT\n",
        "\n",
        "\n",
        "#         o2 = self.down2(o2_) # [B, C, H//4, W//4]\n",
        "\n",
        "#         o3_, attn3 = self.stage3(o2) # [B, C, H//4, W//4]\n",
        "\n",
        "\n",
        "#         # FFAT\n",
        "#         o3_ = self.encoder_level3(o3_)\n",
        "#         # FFAT\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # Bottleneck\n",
        "#         def shallow_down( x, r):\n",
        "#           for _ in range(r):\n",
        "#               x = F.max_pool2d(x, 2)\n",
        "#           return F.leaky_relu(x)\n",
        "#         x_list = [shallow, o1_, o2_, o3_]\n",
        "#         xs = x_list[0]\n",
        "#         new_x = []\n",
        "#         for i in range(len(x_list[1:])):\n",
        "#             x_ = x_list[i+1]+shallow_down(xs, i)\n",
        "#             new_x.append(F.pixel_shuffle(x_, 2**i))\n",
        "#         ob = self.norm(self.mobivari_b(torch.cat(new_x, dim=1)))\n",
        "#         # ob = self.bottleneck([shallow, o1_, o2_, o3_]) # [B, C, H, W]\n",
        "\n",
        "\n",
        "\n",
        "#         o4, attn4 = self.stage4(ob) # [B, C, H, W]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # HRAMiT\n",
        "#         attn_list = [attn1, attn2, attn3, attn4]\n",
        "#         for i, attn in enumerate(attn_list[:-1]):\n",
        "#             attn = F.pixel_shuffle(attn, 2**i)\n",
        "#             x = attn if i==0 else torch.cat([x, attn], dim=1)\n",
        "#         x = torch.cat([x, attn_list[-1]], dim=1)\n",
        "#         mix = self.mobivari_mix(x)\n",
        "#         # mix = self.attn_mix([attn1, attn2, attn3, attn4]) # [B, C, H, W]\n",
        "\n",
        "\n",
        "\n",
        "#         o4 = o4 * mix # [B, C, H, W]\n",
        "\n",
        "#         out = self.to_target(o4+shallow) # global skip connection\n",
        "#         out = out + x_lq if self.scale == 1 else out # residual output (except for only SR)\n",
        "\n",
        "#         out = denormalize(out, self.mean, self.std) if self.img_norm else out\n",
        "#         out = out[:, :, :H_ori*self.scale, :W_ori*self.scale]\n",
        "#         return out\n",
        "\n",
        "#     def flops(self, resolutions):\n",
        "#         H_ori,W_ori = resolutions\n",
        "#         padh = self.unit-(H_ori%self.unit) if H_ori%self.unit!=0 else 0\n",
        "#         padw = self.unit-(W_ori%self.unit) if W_ori%self.unit!=0 else 0\n",
        "#         H,W = H_ori+padh,W_ori+padw\n",
        "#         flops = 0\n",
        "#         flops += self.shallow.flops((H,W))\n",
        "#         flops += self.stage1.flops((H,W))\n",
        "#         flops += self.down1.flops((H,W))\n",
        "#         flops += self.stage2.flops((H//2,W//2))\n",
        "#         flops += self.down2.flops((H//2,W//2))\n",
        "#         flops += self.stage3.flops((H//4,W//4))\n",
        "#         flops += self.bottleneck.flops((H,W))\n",
        "#         flops += self.stage4.flops((H,W))\n",
        "#         flops += self.attn_mix.flops((H,W))\n",
        "#         flops += self.dim * H*W # o4 = o4*mix\n",
        "#         flops += self.to_target.flops((H,W))\n",
        "#         return flops\n",
        "\n",
        "#     def _init_weights(self, m):\n",
        "#         # Swin V2 manner\n",
        "#         if isinstance(m, nn.Linear):\n",
        "#             trunc_normal_(m.weight, std=.02)\n",
        "#             if isinstance(m, nn.Linear) and m.bias is not None:\n",
        "#                 nn.init.constant_(m.bias, 0)\n",
        "#         elif isinstance(m, nn.LayerNorm):\n",
        "#             nn.init.constant_(m.bias, 0)\n",
        "#             nn.init.constant_(m.weight, 1.0)\n",
        "\n",
        "#     @torch.jit.ignore\n",
        "#     def no_weight_decay(self):\n",
        "#         nwd = set()\n",
        "#         for n, _ in self.named_parameters():\n",
        "#             if 'relative_position_bias_table' in n:\n",
        "#                 nwd.add(n)\n",
        "#         return nwd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# # ----------------------------Shadow_R-------------------------\n",
        "#     # @torch.no_grad()\n",
        "#     # def _init_weights(module_list, scale=1, bias_fill=0, **kwargs):\n",
        "#     #   \"\"\"Initialize network weights.\n",
        "\n",
        "#     #   Args:\n",
        "#     #       module_list (list[nn.Module] | nn.Module): Modules to be initialized.\n",
        "#     #       scale (float): Scale initialized weights, especially for residual\n",
        "#     #           blocks. Default: 1.\n",
        "#     #       bias_fill (float): The value to fill bias. Default: 0\n",
        "#     #       kwargs (dict): Other arguments for initialization function.\n",
        "#     #   \"\"\"\n",
        "#     #   if not isinstance(module_list, list):\n",
        "#     #       module_list = [module_list]\n",
        "#     #   for module in module_list:\n",
        "#     #       for m in module.modules():\n",
        "#     #           if isinstance(m, nn.Conv2d):\n",
        "#     #               nn.init.kaiming_normal_(m.weight, **kwargs)\n",
        "#     #               m.weight.data *= scale\n",
        "#     #               if m.bias is not None:\n",
        "#     #                   m.bias.data.fill_(bias_fill)\n",
        "#     #           elif isinstance(m, nn.Linear):\n",
        "#     #               nn.init.kaiming_normal_(m.weight, **kwargs)\n",
        "#     #               m.weight.data *= scale\n",
        "#     #               if m.bias is not None:\n",
        "#     #                   m.bias.data.fill_(bias_fill)\n",
        "#     #           elif isinstance(m, _BatchNorm):\n",
        "#     #               nn.init.constant_(m.weight, 1)\n",
        "#     #               if m.bias is not None:\n",
        "#     #                   m.bias.data.fill_(bias_fill)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "8XqRCHSS9cY1"
      },
      "outputs": [],
      "source": [
        "# model = RAMiT_FFAT(target_mode='light_realdn',img_norm=False, mv_ver=1)\n",
        "# model.to('cuda')\n",
        "# def count_parameters(model):\n",
        "#     return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "# # Create the model\n",
        "\n",
        "# # Print the number of parameters\n",
        "# print(f'The model has {count_parameters(model):,} trainable parameters')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "T1h2p0sx9fSX"
      },
      "outputs": [],
      "source": [
        "# from torchinfo import summary\n",
        "# summary(model, (1, 3, 256, 256))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zzTpesFq0AP"
      },
      "source": [
        "<!-- ## Data loader -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "chSRMdZxOO4V"
      },
      "outputs": [],
      "source": [
        "# from PIL import Image\n",
        "# from torchvision import transforms\n",
        "# from torch.utils.data import Dataset, DataLoader\n",
        "# import os\n",
        "# import numpy as np\n",
        "# import torch\n",
        "\n",
        "\n",
        "\n",
        "# class ImageRestorationDataset(Dataset):\n",
        "#     def __init__(self, root_dir, transform=None, num_samples=None):\n",
        "#         self.root_dir = root_dir\n",
        "#         self.transform = transform\n",
        "#         self.input_dir = os.path.join(root_dir, 'crop_inputs')\n",
        "#         self.target_dir = os.path.join(root_dir, 'crop_targets')\n",
        "#         self.input_filenames = [f for f in sorted(os.listdir(self.input_dir)) if os.path.isfile(os.path.join(self.input_dir, f))]\n",
        "#         self.target_filenames = [f for f in sorted(os.listdir(self.target_dir)) if os.path.isfile(os.path.join(self.target_dir, f))]\n",
        "\n",
        "#         if num_samples is not None:\n",
        "#             self.input_filenames = self.input_filenames[:num_samples]\n",
        "#             self.target_filenames = self.target_filenames[:num_samples]\n",
        "\n",
        "#     def __len__(self):\n",
        "#         return len(self.input_filenames)\n",
        "\n",
        "#     def __getitem__(self, idx):\n",
        "#         input_path = os.path.join(self.input_dir, self.input_filenames[idx])\n",
        "#         target_path = os.path.join(self.target_dir, self.target_filenames[idx])\n",
        "#         input_image = Image.open(input_path).convert('RGB')\n",
        "#         target_image = Image.open(target_path).convert('RGB')\n",
        "\n",
        "\n",
        "#         if self.transform:\n",
        "#             input_image = self.transform(input_image)\n",
        "#             target_image = self.transform(target_image)\n",
        "\n",
        "#             # input_imaget = np.array(input_image, dtype=np.uint8)\n",
        "#             # target_image = np.array(target_image, dtype=np.uint8)\n",
        "#             # input_imaget = input_imaget.astype(np.float16)\n",
        "#             # target_image = target_image.astype(np.float16)\n",
        "#             # input_imaget = input_imaget.transpose(2, 1, 0)\n",
        "#             # target_image = target_image.transpose(2, 1, 0)\n",
        "#             # input_imaget = torch.from_numpy(input_imaget).type(torch.float16)\n",
        "#             # target_image = torch.from_numpy(target_image).type(torch.float16)\n",
        "\n",
        "#         # print(type(self.input_filenames))\n",
        "#         return input_image, target_image\n",
        "\n",
        "# # Define the transformations\n",
        "# transform = transforms.Compose([\n",
        "#     # transforms.Resize((128, 128)),  # Example resize\n",
        "#     transforms.ToTensor(),  # Convert to PyTorch tensors\n",
        "#     transforms.Normalize(mean=(0.0000, 0.0000, 0.0000), std=(1.0000, 1.0000, 1.0000))\n",
        "\n",
        "# ])\n",
        "\n",
        "# # Load the dataset\n",
        "# train_dataset = ImageRestorationDataset(root_dir='/content/drive/MyDrive/istd_dataset/train', transform = transform, num_samples= 8400)\n",
        "# val_dataset = ImageRestorationDataset(root_dir='/content/drive/MyDrive/istd_dataset/val',transform = transform, num_samples= 1680)\n",
        "# # print(type(train_dataset))\n",
        "# # Create DataLoaders\n",
        "# train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
        "# val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False)\n",
        "\n",
        "# # Example: Iterate through the dataset\n",
        "# # for inputs, targets in train_loader:\n",
        "#     # print(f\"Batch of input images has shape: {inputs.shape}\")\n",
        "#     # print(f\"Batch of target images has shape: {targets.shape}\")\n",
        "#     # Further processing...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZjjOEhts1jw"
      },
      "source": [
        "## DATA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "EE4SLCh4-DwS",
        "outputId": "d7960159-ae27-42fb-983b-ac6c2a2610c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 315
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\Admin\\\\Desktop\\\\shadow_removal_results\\\\ISTD\\\\train\\\\targets+'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 79\u001b[0m\n\u001b[1;32m     76\u001b[0m val_lq_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mAdmin\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mshadow_removal_results\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mISTD\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mval\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124minputs\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Replace with your dataset path\u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mShadowRemovalDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_gt_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlq_dir\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_lq_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtransform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maugment\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m ShadowRemovalDataset(gt_dir\u001b[38;5;241m=\u001b[39mval_gt_dir, lq_dir\u001b[38;5;241m=\u001b[39mval_lq_dir, transform\u001b[38;5;241m=\u001b[39mtransform, augment\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     82\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;66;03m# Set the batch size\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[14], line 17\u001b[0m, in \u001b[0;36mShadowRemovalDataset.__init__\u001b[0;34m(self, gt_dir, lq_dir, transform, augment)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maugment \u001b[38;5;241m=\u001b[39m augment\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Filter out directories and non-image files\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgt_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgt_dir\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(gt_dir, f)) \u001b[38;5;129;01mand\u001b[39;00m f\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpeg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbmp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtiff\u001b[39m\u001b[38;5;124m'\u001b[39m))])\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlq_images \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([f \u001b[38;5;28;01mfor\u001b[39;00m f \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39mlistdir(lq_dir) \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(lq_dir, f)) \u001b[38;5;129;01mand\u001b[39;00m f\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith((\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpng\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mjpeg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbmp\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtiff\u001b[39m\u001b[38;5;124m'\u001b[39m))])\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\Admin\\\\Desktop\\\\shadow_removal_results\\\\ISTD\\\\train\\\\targets+'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import functional as TF\n",
        "import random\n",
        "from PIL import Image\n",
        "\n",
        "class ShadowRemovalDataset(Dataset):\n",
        "    def __init__(self, gt_dir, lq_dir, transform=None, augment=False):\n",
        "        self.gt_dir = gt_dir\n",
        "        self.lq_dir = lq_dir\n",
        "        self.transform = transform\n",
        "        self.augment = augment\n",
        "\n",
        "        # Filter out directories and non-image files\n",
        "        self.gt_images = sorted([f for f in os.listdir(gt_dir) if os.path.isfile(os.path.join(gt_dir, f)) and f.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'tiff'))])\n",
        "        self.lq_images = sorted([f for f in os.listdir(lq_dir) if os.path.isfile(os.path.join(lq_dir, f)) and f.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'tiff'))])\n",
        "        # self.gt_images = self.gt_images[:80]\n",
        "        # self.lq_images = self.gt_images[:80]\n",
        "    def __len__(self):\n",
        "        return len(self.gt_images)\n",
        "\n",
        "    def augment_images(self, gt_image, lq_image):\n",
        "        # Random rotation\n",
        "        angle = random.choice([0, 90, 180, 270])\n",
        "        gt_image = TF.rotate(gt_image, angle)\n",
        "        lq_image = TF.rotate(lq_image, angle)\n",
        "\n",
        "        # Random horizontal flipping\n",
        "        if random.random() > 0.5:\n",
        "            gt_image = TF.hflip(gt_image)\n",
        "            lq_image = TF.hflip(lq_image)\n",
        "\n",
        "        # Random vertical flipping\n",
        "        if random.random() > 0.5:\n",
        "            gt_image = TF.vflip(gt_image)\n",
        "            lq_image = TF.vflip(lq_image)\n",
        "\n",
        "        return gt_image, lq_image\n",
        "\n",
        "    def random_crop(self, gt_image, lq_image):\n",
        "        i, j, h, w = transforms.RandomCrop.get_params(\n",
        "            #Random crop size\n",
        "            gt_image, output_size=(360, 480))\n",
        "        gt_image = TF.crop(gt_image, i, j, h, w)\n",
        "        lq_image = TF.crop(lq_image, i, j, h, w)\n",
        "        return gt_image, lq_image\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        gt_img_path = os.path.join(self.gt_dir, self.gt_images[idx])\n",
        "        lq_img_path = os.path.join(self.lq_dir, self.lq_images[idx])\n",
        "\n",
        "        gt_image = Image.open(gt_img_path).convert(\"RGB\")\n",
        "        lq_image = Image.open(lq_img_path).convert(\"RGB\")\n",
        "\n",
        "        if self.augment:\n",
        "            #gt_image, lq_image = self.augment_images(gt_image, lq_image)\n",
        "            gt_image, lq_image = self.random_crop(gt_image, lq_image)\n",
        "\n",
        "        if self.transform:\n",
        "            gt_image = self.transform(gt_image)\n",
        "            lq_image = self.transform(lq_image)\n",
        "\n",
        "        return lq_image, gt_image\n",
        "\n",
        "# Load the dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor()\n",
        "])\n",
        "\n",
        "# Define paths to your dataset in Google Drive\n",
        "train_gt_dir = r'C:\\Users\\Admin\\Desktop\\shadow_removal_results\\ISTD\\train\\targets+'\n",
        "train_lq_dir = r'C:\\Users\\Admin\\Desktop\\shadow_removal_results\\ISTD\\train\\inputs'\n",
        "val_gt_dir = r'C:\\Users\\Admin\\Desktop\\shadow_removal_results\\ISTD\\val\\targets+'\n",
        "val_lq_dir = r'C:\\Users\\Admin\\Desktop\\shadow_removal_results\\ISTD\\val\\inputs'\n",
        "\n",
        "# Replace with your dataset path\n",
        "train_dataset = ShadowRemovalDataset(gt_dir=train_gt_dir, lq_dir=train_lq_dir, transform=transform, augment=True)\n",
        "val_dataset = ShadowRemovalDataset(gt_dir=val_gt_dir, lq_dir=val_lq_dir, transform=transform, augment=False)\n",
        "\n",
        "batch_size = 2 # Set the batch size\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, pin_memory=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, pin_memory=True)\n",
        "\n",
        "# Check the number of files (samples) in the datasets\n",
        "print(f\"Number of training samples: {len(train_dataset)}\")\n",
        "print(f\"Number of validation samples: {len(val_dataset)}\")\n",
        "\n",
        "# Check the number of batches in the DataLoaders\n",
        "print(f\"Number of training batches: {len(train_loader)}\")\n",
        "print(f\"Number of validation batches: {len(val_loader)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-1GpNypQq5gR"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wHBpsePEyAEZ"
      },
      "outputs": [],
      "source": [
        "# import time\n",
        "# from tqdm import tqdm\n",
        "# from torchmetrics.functional import structural_similarity_index_measure as ssim\n",
        "# from lpips import LPIPS\n",
        "\n",
        "# # transfrom the data from tensors to numpys to calculate the PSNR\n",
        "# def tensor_to_numpy(tensor):\n",
        "#     tensor = tensor.permute(1, 2, 0).cpu().numpy()  # Change from (C, H, W) to (H, W, C)\n",
        "#     tensor = (tensor * 255).astype(np.uint8)  # Optional: Scale to [0, 255] and convert to uint8\n",
        "#     return tensor\n",
        "# # ----------------------------------------------------\n",
        "\n",
        "# # uste the transfromed data to calculate the PSNR\n",
        "# def calculate_psnr(img1_tensor, img2_tensor):\n",
        "#     img1 = tensor_to_numpy(img1_tensor)\n",
        "#     img2 = tensor_to_numpy(img2_tensor)\n",
        "#     psnr_value = psnr(img1, img2)\n",
        "#     return psnr_value\n",
        "# # -------------------------------------------------\n",
        "\n",
        "# lpips_loss = LPIPS(net='alex').to('cuda')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vpzE-J7Ft38b"
      },
      "outputs": [],
      "source": [
        "# def evaluate(model: nn.Module, val_loader) -> float:\n",
        "#     model.eval()  # Turn on evaluation mode\n",
        "\n",
        "#     # initialize all losses\n",
        "#     criterion = nn.L1Loss()\n",
        "#     # lpips_loss = LPIPS(net='alex').to('cuda')\n",
        "#     # -----------------------------------------------\n",
        "\n",
        "#     # initialize losses values\n",
        "#     total_loss = 0.0\n",
        "#     total_psnr = 0.0\n",
        "#     total_ssim = 0.0\n",
        "#     total_lpips = 0.0\n",
        "#   # -----------------------------------------\n",
        "\n",
        "# # list of low result\n",
        "#     output_l = []\n",
        "#     targets_l = []\n",
        "#     inputs_l = []\n",
        "# # ------------------------------------\n",
        "\n",
        "#     # list of top results\n",
        "#     tev_out = []\n",
        "#     tev_inputs = []\n",
        "#     tev_targets = []\n",
        "# # ---------------------------------------\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         # with tqdm(total=len(val_loader), desc=\"Validating\", unit=\"batch\") as val_pbar:\n",
        "#             for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
        "#                 inputs, targets = inputs.to('cuda'), targets.to('cuda')\n",
        "#                 output = model(inputs)\n",
        "\n",
        "#                 # L1 loss\n",
        "#                 loss = criterion(output, targets)\n",
        "#                 # ------------------------------------------\n",
        "\n",
        "#                 # ssim loss\n",
        "#                 ssim_L = ssim(output, targets, data_range=1.0)\n",
        "#                 # -------------------------------------------\n",
        "\n",
        "#                 # lpips loss\n",
        "#                 # batch_lpips = 0.0\n",
        "#                 img1 = (targets * 2) - 1\n",
        "#                 img2 = (output * 2) - 1\n",
        "#                 # for i in range(inputs.size(0)):\n",
        "#                 #   batch_lpips += lpips_loss(img1[i], img2[i]).item()\n",
        "#                 # batch_lpips /= inputs.size(0)\n",
        "#                 batch_lpips = lpips_loss(img1, img2).mean()\n",
        "#                 # -----------------------------------------------\n",
        "\n",
        "#                 # Calculate the PSNR for each batch\n",
        "#                 batch_psnr = 0\n",
        "#                 for i in range(inputs.size(0)):\n",
        "#                     batch_psnr += calculate_psnr(output[i].detach(), targets[i])\n",
        "#                 batch_psnr /= inputs.size(0)\n",
        "#                 # ------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "#                 # total loss for all losses\n",
        "#                 total_loss += loss.item() # total loss for L1\n",
        "#                 total_psnr += batch_psnr # total loss for psnr\n",
        "#                 total_ssim += ssim_L.item() # total loss for ssim\n",
        "#                 total_lpips += batch_lpips # total loss for lpips\n",
        "#                 # -----------------------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "#                 # top and low results\n",
        "#                 if batch_psnr > 10 and batch_psnr <= 16:\n",
        "#                   output_l.append(output.to('cpu'))\n",
        "#                   targets_l.append(targets.to('cpu'))\n",
        "#                   inputs_l.append(inputs.to('cpu'))\n",
        "#                 elif batch_psnr >= 29:\n",
        "#                   tev_out.append(output.to('cpu'))\n",
        "#                   tev_inputs.append(inputs.to('cpu'))\n",
        "#                   tev_targets.append(targets.to('cpu'))\n",
        "#                   # ------------------------------------------\n",
        "\n",
        "\n",
        "#                 # Update progress bar for the current batch\n",
        "#                 # val_pbar.set_postfix({\"loss\": loss.item(), \"batch_psnr\": batch_psnr, 'batch_ssim': ssim_L.item(), 'batch_lpips': batch_lpips})\n",
        "#                 # val_pbar.update(1)\n",
        "\n",
        "#     avg_loss = total_loss / len(val_loader)\n",
        "#     avg_psnr = total_psnr / len(val_loader)\n",
        "#     avg_ssim = total_ssim / len(val_loader)\n",
        "#     avg_lpips = total_lpips / len(val_loader)\n",
        "#     return  tev_out, tev_inputs, tev_targets, output_l, targets_l, inputs_l, avg_loss,  avg_psnr, avg_ssim, avg_lpips"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-S8fcHRPRGbi"
      },
      "outputs": [],
      "source": [
        "# def train(model: nn.Module, train_loader, epochs) -> None:\n",
        "#     model.train()  # Turn on train mode\n",
        "\n",
        "#     lr = 0.001\n",
        "#     criterion = nn.L1Loss()\n",
        "#     # lpips_loss = LPIPS(net='alex').to('cuda')\n",
        "\n",
        "#     optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.99), eps=1e-08)\n",
        "\n",
        "#     top_output = []\n",
        "#     top_inputs = []\n",
        "#     top_targets = []\n",
        "#     low_output = []\n",
        "#     low_inputs = []\n",
        "#     low_targets = []\n",
        "#     for epoch in range(1, epochs+1):\n",
        "#             total_loss = 0.0\n",
        "#             total_psnr = 0.0\n",
        "#             total_ssim = 0.0\n",
        "#             total_lpips = 0.0\n",
        "#             start_time = time.time()\n",
        "#             batch_num = 1\n",
        "#         # Initialize tqdm progress bar\n",
        "#         # with tqdm(total=len(train_loader), desc=f\"Epoch {epoch}/{epochs}\", unit=\"batch\") as pbar:\n",
        "#             for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
        "#                 inputs, targets = inputs.to('cuda'), targets.to('cuda')\n",
        "#                 optimizer.zero_grad()\n",
        "#                 output = model(inputs)\n",
        "\n",
        "#                 # L1 loss + backprop\n",
        "#                 loss = criterion(output, targets)\n",
        "#                 loss.backward()\n",
        "#                 torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "#                 optimizer.step()\n",
        "#                 # ------------------------------------------------\n",
        "\n",
        "#                 # ssim loss\n",
        "#                 ssim_L = ssim(output, targets, data_range=1.0)\n",
        "#                 # -----------------------------------------------\n",
        "\n",
        "#                 # lpips loss\n",
        "#                 batch_lpips = 0.0\n",
        "#                 img1 = (targets * 2) - 1\n",
        "#                 img2 = (output * 2) - 1\n",
        "#                 # for i in range(inputs.size(0)):\n",
        "#                 #   batch_lpips += lpips_loss(img1[i], img2[i]).item()\n",
        "#                 # batch_lpips /= inputs.size(0)\n",
        "#                 with torch.no_grad():\n",
        "#                   batch_lpips = lpips_loss(img1, img2).mean()\n",
        "#                 # -----------------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#                 # Calculate the PSNR for each batch\n",
        "#                 batch_psnr = 0\n",
        "#                 for i in range(inputs.size(0)):\n",
        "#                     batch_psnr += calculate_psnr(output[i].detach(), targets[i])\n",
        "#                 batch_psnr /= inputs.size(0)\n",
        "\n",
        "#                 # ------------------------------------------------------\n",
        "\n",
        "#                 # total loss for all losses\n",
        "#                 total_loss += loss.item() # total loss for L1\n",
        "#                 total_psnr += batch_psnr # total loss for psnr\n",
        "#                 total_ssim += ssim_L.item() # total loss for ssim\n",
        "#                 total_lpips += batch_lpips # total loss for lpips\n",
        "#                 # -----------------------------------------------------\n",
        "\n",
        "#                 print(f' batch_num: {batch_num} | loss: {loss.item():.5f} | psnr_btch: {batch_psnr:.5f} | ssim: {ssim_L.item():.5f}| lpips: {batch_lpips:.5f}')\n",
        "#                 batch_num += 1\n",
        "#                 # Update the best enhanced images\n",
        "#                 if batch_psnr > 10 and batch_psnr < 16:\n",
        "#                     low_output.append(output.to('cpu'))\n",
        "#                     low_inputs.append(inputs.to('cpu'))\n",
        "#                     low_targets.append(targets.to('cpu'))\n",
        "#                 elif batch_psnr > 29:\n",
        "#                     top_output.append(output.to('cpu'))\n",
        "#                     top_inputs.append(inputs.to('cpu'))\n",
        "#                     top_targets.append(targets.to('cpu'))\n",
        "#                 # -----------------------------------------------------------\n",
        "\n",
        "#                 # Update progress bar\n",
        "#                 # pbar.set_postfix({\"loss\": loss.item(), \"batch_psnr\": batch_psnr})\n",
        "#                 # pbar.update(1)\n",
        "\n",
        "#             # Log end of epoch details\n",
        "#             cur_loss = total_loss / len(train_loader)\n",
        "#             cur_psnr = total_psnr / len(train_loader)\n",
        "#             cur_ssim = total_ssim / len(train_loader)\n",
        "#             cur_lpips = total_lpips / len(train_loader)\n",
        "#             # val_loss, e_out, total_psnr = evaluate(model, val_loader)\n",
        "#             print(f'End of Epoch {epoch}  | LOSS {cur_loss:5.4f} | PSNR {cur_psnr:.4f} | SSIM: {ssim_L.item():.5f} | LPIPS: {batch_lpips:.5f}')\n",
        "#             tev_out, tev_inputs, tev_targets, output_l, targets_l, inputs_l, avg_loss,  avg_psnr, avg_ssim, avg_lpips = evaluate(model, val_loader)\n",
        "#             print(f'\\n End of valadation {epoch} | LOSS: {avg_loss:5.4f} | PSNR: {avg_psnr:.4f} | SSIM: {avg_ssim:.5f} | LPIPS: {avg_lpips:.5f}')\n",
        "\n",
        "#     print('Training completed.')\n",
        "#     return top_output, top_inputs, top_targets, low_output, low_inputs, low_targets, tev_out, tev_inputs, tev_targets, output_l, targets_l, inputs_l"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eaQ_clr1GKRr"
      },
      "source": [
        "## MD_Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_g3jWYSLOVHD"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import DataLoader\n",
        "# from torchvision.utils import save_image\n",
        "# from tqdm import tqdm\n",
        "# from skimage.metrics import structural_similarity as ssim\n",
        "# from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "# from lpips import LPIPS\n",
        "# import torchvision.models as models\n",
        "\n",
        "# # Constants\n",
        "# LEARNING_RATE = 0.001\n",
        "# NUM_EPOCHS = 100\n",
        "# SAVE_INTERVAL = 5\n",
        "# START_EPOCH = 0  # Start from epoch 80\n",
        "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# #G1.load_state_dict(torch.load(r\"C:\\Users\\nasef\\Desktop\\SBSR\\Models\\SBSR32\\SBSR30.pth\"))\n",
        "\n",
        "\n",
        "# optimizer_G = optim.Adam(G1.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999))\n",
        "\n",
        "# # Add the cosine annealing scheduler\n",
        "# scheduler_G = optim.lr_scheduler.CosineAnnealingLR(optimizer_G, T_max=NUM_EPOCHS - START_EPOCH, eta_min=0.000001)\n",
        "\n",
        "# # Loss functions\n",
        "# l1_loss = nn.L1Loss()\n",
        "# mse_loss = nn.MSELoss()\n",
        "# lpips_loss_fn = LPIPS(net='alex').to(DEVICE)  # Using AlexNet for LPIPS loss\n",
        "\n",
        "# # Define the VGG loss\n",
        "# class VGGLoss(nn.Module):\n",
        "#     def __init__(self, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
        "#         super(VGGLoss, self).__init__()\n",
        "#         vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features.to(device).eval()  # Move VGG layers to device and set to eval mode\n",
        "#         self.vgg_layers = nn.Sequential(*list(vgg)[:36]).to(device)\n",
        "#         self.device = device\n",
        "#         for param in self.vgg_layers.parameters():\n",
        "#             param.requires_grad = False\n",
        "\n",
        "#     def forward(self, x, y):\n",
        "#         x = x.to(self.device)\n",
        "#         y = y.to(self.device)\n",
        "#         x_vgg = self.vgg_layers(x)\n",
        "#         y_vgg = self.vgg_layers(y)\n",
        "#         loss = nn.functional.l1_loss(x_vgg, y_vgg)\n",
        "#         return loss\n",
        "\n",
        "# # Define Focal Frequency Loss\n",
        "# class FocalFrequencyLoss(nn.Module):\n",
        "#     def __init__(self, alpha=1.0, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
        "#         super(FocalFrequencyLoss, self).__init__()\n",
        "#         self.alpha = alpha\n",
        "#         self.device = device\n",
        "\n",
        "#     def forward(self, input, target):\n",
        "#         input = input.to(self.device)\n",
        "#         target = target.to(self.device)\n",
        "#         input_fft = torch.fft.fft2(input)\n",
        "#         target_fft = torch.fft.fft2(target)\n",
        "#         diff = input_fft - target_fft\n",
        "#         abs_diff = torch.abs(diff)\n",
        "#         loss = torch.pow(abs_diff, self.alpha)\n",
        "#         return torch.mean(loss)\n",
        "\n",
        "# # Loss weights\n",
        "# lambda_vgg = 0.01\n",
        "# lambda_ff = 0.1\n",
        "\n",
        "# # Combined loss\n",
        "# class CombinedLoss(nn.Module):\n",
        "#     def __init__(self, lambda_vgg, lambda_ff, device=DEVICE):\n",
        "#         super(CombinedLoss, self).__init__()\n",
        "#         self.lambda_vgg = lambda_vgg\n",
        "#         self.lambda_ff = lambda_ff\n",
        "#         self.vgg_loss = VGGLoss(device=device)\n",
        "#         self.ff_loss = FocalFrequencyLoss(device=device)\n",
        "\n",
        "#     def forward(self, input, target):\n",
        "#         l1 = l1_loss(input, target)\n",
        "#         vgg = self.vgg_loss(input, target)\n",
        "#         ff = self.ff_loss(input, target)\n",
        "#         return l1 + self.lambda_vgg * vgg + self.lambda_ff * ff\n",
        "\n",
        "# # Example usage\n",
        "# combined_loss = CombinedLoss(lambda_vgg=lambda_vgg, lambda_ff=lambda_ff, device=DEVICE)\n",
        "\n",
        "# # Directories for checkpoints and samples\n",
        "# checkpoint_dir = r'C:\\Users\\Admin\\Desktop\\AI_RAMiT\\RAMiT'\n",
        "# sample_dir = r'C:\\Users\\Admin\\Desktop\\AI_RAMiT\\RAMiT'\n",
        "# os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "# os.makedirs(sample_dir, exist_ok=True)\n",
        "# save_interval = SAVE_INTERVAL  # Save checkpoint and samples every 5 epochs\n",
        "\n",
        "# for epoch in range(START_EPOCH, NUM_EPOCHS):\n",
        "#     # Training\n",
        "#     G1.train()\n",
        "#     total_g_loss = 0.0\n",
        "#     total_l1_loss = 0.0\n",
        "#     with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{NUM_EPOCHS}', unit='batch') as pbar:\n",
        "#         for i, (real_shadow, real_free) in enumerate(train_loader):\n",
        "#             real_shadow = real_shadow.to(DEVICE)\n",
        "#             real_free = real_free.to(DEVICE)\n",
        "#             optimizer_G.zero_grad()\n",
        "\n",
        "#             pred_free = G1(real_shadow)\n",
        "#             # Generator loss\n",
        "#             g1_loss = l1_loss(pred_free, real_free)\n",
        "#             g_loss = g1_loss\n",
        "#             g_loss.backward()\n",
        "#             torch.nn.utils.clip_grad_norm_(G1.parameters(), max_norm=1.0)\n",
        "#             optimizer_G.step()\n",
        "\n",
        "#             # Accumulate losses\n",
        "#             total_g_loss += g_loss.item()\n",
        "#             total_l1_loss += g1_loss.item()\n",
        "\n",
        "#             pbar.set_postfix(G_loss=g_loss.item(), G1_loss=g1_loss.item())\n",
        "#             pbar.update(1)\n",
        "\n",
        "#     # Calculate average losses\n",
        "#     avg_g_loss = total_g_loss / len(train_loader)\n",
        "#     avg_l1_loss = total_l1_loss / len(train_loader)\n",
        "\n",
        "#     print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}] - Average G Loss: {avg_g_loss:.4f}\")\n",
        "#     scheduler_G.step()\n",
        "\n",
        "#     m = 0\n",
        "#     # Validation and Checkpoints\n",
        "#     if (epoch + 1+70) % save_interval == 0:\n",
        "#         torch.cuda.empty_cache()\n",
        "#         G1.eval()\n",
        "#         with torch.no_grad():\n",
        "#             val_g_loss = 0.0\n",
        "#             val_l1_loss = 0.0\n",
        "#             val_ssim_loss = 0.0\n",
        "#             val_lpips_loss = 0.0\n",
        "#             val_psnr_mask = 0.0\n",
        "\n",
        "#             with tqdm(total=len(val_loader), desc=f'Validation Epoch {epoch + 1}/{NUM_EPOCHS}', unit='batch') as pbar_val:\n",
        "#                 for i, (real_shadow, real_free) in enumerate(val_loader):\n",
        "#                     real_shadow = real_shadow.to(DEVICE)\n",
        "#                     real_free = real_free.to(DEVICE)\n",
        "\n",
        "#                     pred_free = G1(real_shadow)\n",
        "#                     # Generator loss\n",
        "#                     g1_loss = l1_loss(pred_free, real_free)\n",
        "#                     g_loss = g1_loss\n",
        "\n",
        "#                     val_g_loss += g_loss.item()\n",
        "#                     val_l1_loss += g1_loss.item()\n",
        "\n",
        "#                     # Calculate SSIM and LPIPS\n",
        "#                     pred_free_np = pred_free.cpu().numpy().transpose(0, 2, 3, 1)\n",
        "#                     real_free_np = real_free.cpu().numpy().transpose(0, 2, 3, 1)\n",
        "\n",
        "#                     #ssim_val = ssim(pred_free_np, real_free_np, data_range=1, channel_axis=-1)\n",
        "#                     lpips_val = lpips_loss_fn(pred_free, real_free).mean().item()  # Average LPIPS over batch\n",
        "#                     #val_ssim_loss += ssim_val\n",
        "#                     val_lpips_loss += lpips_val\n",
        "\n",
        "#                     # Calculate PSNR for final shadow-free images\n",
        "#                     for j in range(real_free_np.shape[0]):\n",
        "#                         val_psnr_mask += psnr(real_free_np[j], pred_free_np[j], data_range=1)\n",
        "#                         val_ssim_loss += ssim(real_free_np[j], pred_free_np[j], data_range=1, channel_axis=-1)\n",
        "\n",
        "\n",
        "#                     pbar_val.update(1)\n",
        "#                     save_image(pred_free, os.path.join(sample_dir, f'pred_free_epoch_{epoch + 1}_{m}.png'))\n",
        "#                     m += 1\n",
        "\n",
        "#             # Averaging over the entire validation set\n",
        "#             val_g_loss /= len(val_loader)\n",
        "#             val_l1_loss /= len(val_loader)\n",
        "#             val_ssim_loss /= len(val_loader.dataset)\n",
        "#             val_lpips_loss /= len(val_loader)\n",
        "#             val_psnr_mask /= len(val_loader.dataset)\n",
        "\n",
        "#             print(f\"Validation after Epoch [{epoch + 1}/{NUM_EPOCHS}] \"\n",
        "#                   f\"Val G Loss: {val_g_loss:.4f}, Val L1 Loss: {val_l1_loss:.4f}, \"\n",
        "#                   f\"PSNR_MASK: {val_psnr_mask:.4f}, SSIM: {val_ssim_loss:.4f}, LPIPS: {val_lpips_loss:.4f}\")\n",
        "\n",
        "#             # Save model checkpoints\n",
        "#             torch.save(G1.state_dict(), os.path.join(checkpoint_dir, f'SBSR{epoch + 1}.pth'))\n",
        "\n",
        "# print(\"Training completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CFQNEBuds1j0"
      },
      "source": [
        "## RMAiT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oLsFaUnBs1j1"
      },
      "outputs": [],
      "source": [
        "# import os\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "# import torch.optim as optim\n",
        "# from torch.utils.data import DataLoader\n",
        "# from torchvision.utils import save_image\n",
        "# from tqdm import tqdm\n",
        "# from skimage.metrics import structural_similarity as ssim\n",
        "# from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "# from lpips import LPIPS\n",
        "# import torchvision.models as models\n",
        "\n",
        "# # Constants\n",
        "# LEARNING_RATE = 0.001\n",
        "# NUM_EPOCHS = 500\n",
        "# SAVE_INTERVAL = 5\n",
        "# START_EPOCH = 0  # Start from epoch 80\n",
        "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# #G1.load_state_dict(torch.load(r\"C:\\Users\\nasef\\Desktop\\SBSR\\Models\\SBSR32\\SBSR30.pth\"))\n",
        "\n",
        "\n",
        "# optimizer_G = optim.Adam(G1.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999))\n",
        "\n",
        "# # Add the cosine annealing scheduler\n",
        "# scheduler_G = optim.lr_scheduler.CosineAnnealingLR(optimizer_G, T_max=NUM_EPOCHS - START_EPOCH, eta_min=0.000001)\n",
        "\n",
        "# # Loss functions\n",
        "# l1_loss = nn.L1Loss()\n",
        "# mse_loss = nn.MSELoss()\n",
        "# lpips_loss_fn = LPIPS(net='alex').to(DEVICE)  # Using AlexNet for LPIPS loss\n",
        "\n",
        "# # Define the VGG loss\n",
        "# class VGGLoss(nn.Module):\n",
        "#     def __init__(self, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
        "#         super(VGGLoss, self).__init__()\n",
        "#         vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features.to(device).eval()  # Move VGG layers to device and set to eval mode\n",
        "#         self.vgg_layers = nn.Sequential(*list(vgg)[:36]).to(device)\n",
        "#         self.device = device\n",
        "#         for param in self.vgg_layers.parameters():\n",
        "#             param.requires_grad = False\n",
        "\n",
        "#     def forward(self, x, y):\n",
        "#         x = x.to(self.device)\n",
        "#         y = y.to(self.device)\n",
        "#         x_vgg = self.vgg_layers(x)\n",
        "#         y_vgg = self.vgg_layers(y)\n",
        "#         loss = nn.functional.l1_loss(x_vgg, y_vgg)\n",
        "#         return loss\n",
        "\n",
        "# # Define Focal Frequency Loss\n",
        "# class FocalFrequencyLoss(nn.Module):\n",
        "#     def __init__(self, alpha=1.0, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
        "#         super(FocalFrequencyLoss, self).__init__()\n",
        "#         self.alpha = alpha\n",
        "#         self.device = device\n",
        "\n",
        "#     def forward(self, input, target):\n",
        "#         input = input.to(self.device)\n",
        "#         target = target.to(self.device)\n",
        "#         input_fft = torch.fft.fft2(input)\n",
        "#         target_fft = torch.fft.fft2(target)\n",
        "#         diff = input_fft - target_fft\n",
        "#         abs_diff = torch.abs(diff)\n",
        "#         loss = torch.pow(abs_diff, self.alpha)\n",
        "#         return torch.mean(loss)\n",
        "\n",
        "# # Loss weights\n",
        "# lambda_vgg = 0.01\n",
        "# lambda_ff = 0.1\n",
        "\n",
        "# # Combined loss\n",
        "# class CombinedLoss(nn.Module):\n",
        "#     def __init__(self, lambda_vgg, lambda_ff, device=DEVICE):\n",
        "#         super(CombinedLoss, self).__init__()\n",
        "#         self.lambda_vgg = lambda_vgg\n",
        "#         self.lambda_ff = lambda_ff\n",
        "#         self.vgg_loss = VGGLoss(device=device)\n",
        "#         self.ff_loss = FocalFrequencyLoss(device=device)\n",
        "\n",
        "#     def forward(self, input, target):\n",
        "#         l1 = l1_loss(input, target)\n",
        "#         vgg = self.vgg_loss(input, target)\n",
        "#         ff = self.ff_loss(input, target)\n",
        "#         return l1 + self.lambda_vgg * vgg + self.lambda_ff * ff\n",
        "\n",
        "# # Example usage\n",
        "# combined_loss = CombinedLoss(lambda_vgg=lambda_vgg, lambda_ff=lambda_ff, device=DEVICE)\n",
        "\n",
        "# # Directories for checkpoints and samples\n",
        "# checkpoint_dir = r'C:\\Users\\Admin\\Desktop\\AI_RAMiT\\RAMiT'\n",
        "# sample_dir = r'C:\\Users\\Admin\\Desktop\\AI_RAMiT\\RAMiT'\n",
        "# os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "# os.makedirs(sample_dir, exist_ok=True)\n",
        "# save_interval = SAVE_INTERVAL  # Save checkpoint and samples every 5 epochs\n",
        "\n",
        "# for epoch in range(START_EPOCH, NUM_EPOCHS):\n",
        "#     # Training\n",
        "#     G1.train()\n",
        "#     total_g_loss = 0.0\n",
        "#     total_l1_loss = 0.0\n",
        "#     with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{NUM_EPOCHS}', unit='batch') as pbar:\n",
        "#         for i, (real_shadow, real_free) in enumerate(train_loader):\n",
        "#             real_shadow = real_shadow.to(DEVICE)\n",
        "#             real_free = real_free.to(DEVICE)\n",
        "#             optimizer_G.zero_grad()\n",
        "\n",
        "#             pred_free = G1(real_shadow)\n",
        "#             # Generator loss\n",
        "#             g1_loss = l1_loss(pred_free, real_free)\n",
        "#             g_loss = g1_loss\n",
        "#             g_loss.backward()\n",
        "#             torch.nn.utils.clip_grad_norm_(G1.parameters(), max_norm=1.0)\n",
        "#             optimizer_G.step()\n",
        "\n",
        "#             # Accumulate losses\n",
        "#             total_g_loss += g_loss.item()\n",
        "#             total_l1_loss += g1_loss.item()\n",
        "\n",
        "#             pbar.set_postfix(G_loss=g_loss.item(), G1_loss=g1_loss.item())\n",
        "#             pbar.update(1)\n",
        "\n",
        "#     # Calculate average losses\n",
        "#     avg_g_loss = total_g_loss / len(train_loader)\n",
        "#     avg_l1_loss = total_l1_loss / len(train_loader)\n",
        "\n",
        "#     print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}] - Average G Loss: {avg_g_loss:.4f}\")\n",
        "#     scheduler_G.step()\n",
        "\n",
        "#     m = 0\n",
        "#     # Validation and Checkpoints\n",
        "#     if (epoch + 1+70) % save_interval == 0:\n",
        "#         torch.cuda.empty_cache()\n",
        "#         G1.eval()\n",
        "#         with torch.no_grad():\n",
        "#             val_g_loss = 0.0\n",
        "#             val_l1_loss = 0.0\n",
        "#             val_ssim_loss = 0.0\n",
        "#             val_lpips_loss = 0.0\n",
        "#             val_psnr_mask = 0.0\n",
        "\n",
        "#             with tqdm(total=len(val_loader), desc=f'Validation Epoch {epoch + 1}/{NUM_EPOCHS}', unit='batch') as pbar_val:\n",
        "#                 for i, (real_shadow, real_free) in enumerate(val_loader):\n",
        "#                     real_shadow = real_shadow.to(DEVICE)\n",
        "#                     real_free = real_free.to(DEVICE)\n",
        "\n",
        "#                     pred_free = G1(real_shadow)\n",
        "#                     # Generator loss\n",
        "#                     g1_loss = l1_loss(pred_free, real_free)\n",
        "#                     g_loss = g1_loss\n",
        "\n",
        "#                     val_g_loss += g_loss.item()\n",
        "#                     val_l1_loss += g1_loss.item()\n",
        "\n",
        "#                     # Calculate SSIM and LPIPS\n",
        "#                     pred_free_np = pred_free.cpu().numpy().transpose(0, 2, 3, 1)\n",
        "#                     real_free_np = real_free.cpu().numpy().transpose(0, 2, 3, 1)\n",
        "\n",
        "#                     #ssim_val = ssim(pred_free_np, real_free_np, data_range=1, channel_axis=-1)\n",
        "#                     lpips_val = lpips_loss_fn(pred_free, real_free).mean().item()  # Average LPIPS over batch\n",
        "#                     #val_ssim_loss += ssim_val\n",
        "#                     val_lpips_loss += lpips_val\n",
        "\n",
        "#                     # Calculate PSNR for final shadow-free images\n",
        "#                     for j in range(real_free_np.shape[0]):\n",
        "#                         val_psnr_mask += psnr(real_free_np[j], pred_free_np[j], data_range=1)\n",
        "#                         val_ssim_loss += ssim(real_free_np[j], pred_free_np[j], data_range=1, channel_axis=-1)\n",
        "\n",
        "\n",
        "#                     pbar_val.update(1)\n",
        "#                     save_image(pred_free, os.path.join(sample_dir, f'pred_free_epoch_{epoch + 1}_{m}.png'))\n",
        "#                     m += 1\n",
        "\n",
        "#             # Averaging over the entire validation set\n",
        "#             val_g_loss /= len(val_loader)\n",
        "#             val_l1_loss /= len(val_loader)\n",
        "#             val_ssim_loss /= len(val_loader.dataset)\n",
        "#             val_lpips_loss /= len(val_loader)\n",
        "#             val_psnr_mask /= len(val_loader.dataset)\n",
        "\n",
        "#             print(f\"Validation after Epoch [{epoch + 1}/{NUM_EPOCHS}] \"\n",
        "#                   f\"Val G Loss: {val_g_loss:.4f}, Val L1 Loss: {val_l1_loss:.4f}, \"\n",
        "#                   f\"PSNR_MASK: {val_psnr_mask:.4f}, SSIM: {val_ssim_loss:.4f}, LPIPS: {val_lpips_loss:.4f}\")\n",
        "\n",
        "#             # Save model checkpoints\n",
        "#             torch.save(G1.state_dict(), os.path.join(checkpoint_dir, f'SBSR{epoch + 1}.pth'))\n",
        "\n",
        "# print(\"Training completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV8BMo7ps1j2"
      },
      "source": [
        "## FFAT_RAMiT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EEZ9ZU1vs1j3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "from tqdm import tqdm\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from lpips import LPIPS\n",
        "import torchvision.models as models\n",
        "\n",
        "# Constants\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 500\n",
        "SAVE_INTERVAL = 5\n",
        "START_EPOCH = 0  # Start from epoch 80\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#G1.load_state_dict(torch.load(r\"C:\\Users\\nasef\\Desktop\\SBSR\\Models\\SBSR32\\SBSR30.pth\"))\n",
        "\n",
        "\n",
        "optimizer_G = optim.Adam(G1.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999))\n",
        "\n",
        "# Add the cosine annealing scheduler\n",
        "scheduler_G = optim.lr_scheduler.CosineAnnealingLR(optimizer_G, T_max=NUM_EPOCHS - START_EPOCH, eta_min=0.000001)\n",
        "\n",
        "# Loss functions\n",
        "l1_loss = nn.L1Loss()\n",
        "mse_loss = nn.MSELoss()\n",
        "lpips_loss_fn = LPIPS(net='alex').to(DEVICE)  # Using AlexNet for LPIPS loss\n",
        "\n",
        "# Define the VGG loss\n",
        "class VGGLoss(nn.Module):\n",
        "    def __init__(self, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
        "        super(VGGLoss, self).__init__()\n",
        "        vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features.to(device).eval()  # Move VGG layers to device and set to eval mode\n",
        "        self.vgg_layers = nn.Sequential(*list(vgg)[:36]).to(device)\n",
        "        self.device = device\n",
        "        for param in self.vgg_layers.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = x.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "        x_vgg = self.vgg_layers(x)\n",
        "        y_vgg = self.vgg_layers(y)\n",
        "        loss = nn.functional.l1_loss(x_vgg, y_vgg)\n",
        "        return loss\n",
        "\n",
        "# Define Focal Frequency Loss\n",
        "class FocalFrequencyLoss(nn.Module):\n",
        "    def __init__(self, alpha=1.0, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
        "        super(FocalFrequencyLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        input = input.to(self.device)\n",
        "        target = target.to(self.device)\n",
        "        input_fft = torch.fft.fft2(input)\n",
        "        target_fft = torch.fft.fft2(target)\n",
        "        diff = input_fft - target_fft\n",
        "        abs_diff = torch.abs(diff)\n",
        "        loss = torch.pow(abs_diff, self.alpha)\n",
        "        return torch.mean(loss)\n",
        "\n",
        "# Loss weights\n",
        "lambda_vgg = 0.01\n",
        "lambda_ff = 0.1\n",
        "\n",
        "# Combined loss\n",
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, lambda_vgg, lambda_ff, device=DEVICE):\n",
        "        super(CombinedLoss, self).__init__()\n",
        "        self.lambda_vgg = lambda_vgg\n",
        "        self.lambda_ff = lambda_ff\n",
        "        self.vgg_loss = VGGLoss(device=device)\n",
        "        self.ff_loss = FocalFrequencyLoss(device=device)\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        l1 = l1_loss(input, target)\n",
        "        vgg = self.vgg_loss(input, target)\n",
        "        ff = self.ff_loss(input, target)\n",
        "        return l1 + self.lambda_vgg * vgg + self.lambda_ff * ff\n",
        "\n",
        "# Example usage\n",
        "combined_loss = CombinedLoss(lambda_vgg=lambda_vgg, lambda_ff=lambda_ff, device=DEVICE)\n",
        "\n",
        "# Directories for checkpoints and samples\n",
        "checkpoint_dir = r'C:\\Users\\Admin\\Desktop\\AI_RAMiT\\FFAT_RAMiT'\n",
        "sample_dir = r'C:\\Users\\Admin\\Desktop\\AI_RAMiT\\FFAT_RAMiT_s'\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "os.makedirs(sample_dir, exist_ok=True)\n",
        "save_interval = SAVE_INTERVAL  # Save checkpoint and samples every 5 epochs\n",
        "\n",
        "for epoch in range(START_EPOCH, NUM_EPOCHS):\n",
        "    # Training\n",
        "    G1.train()\n",
        "    total_g_loss = 0.0\n",
        "    total_l1_loss = 0.0\n",
        "    with tqdm(total=len(train_loader), desc=f'Epoch {epoch + 1}/{NUM_EPOCHS}', unit='batch') as pbar:\n",
        "        for i, (real_shadow, real_free) in enumerate(train_loader):\n",
        "            real_shadow = real_shadow.to(DEVICE)\n",
        "            real_free = real_free.to(DEVICE)\n",
        "            optimizer_G.zero_grad()\n",
        "\n",
        "            pred_free = G1(real_shadow)\n",
        "            # Generator loss\n",
        "            g1_loss = l1_loss(pred_free, real_free)\n",
        "            g_loss = g1_loss\n",
        "            g_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(G1.parameters(), max_norm=1.0)\n",
        "            optimizer_G.step()\n",
        "\n",
        "            # Accumulate losses\n",
        "            total_g_loss += g_loss.item()\n",
        "            total_l1_loss += g1_loss.item()\n",
        "\n",
        "            pbar.set_postfix(G_loss=g_loss.item(), G1_loss=g1_loss.item())\n",
        "            pbar.update(1)\n",
        "\n",
        "    # Calculate average losses\n",
        "    avg_g_loss = total_g_loss / len(train_loader)\n",
        "    avg_l1_loss = total_l1_loss / len(train_loader)\n",
        "\n",
        "    print(f\"Epoch [{epoch + 1}/{NUM_EPOCHS}] - Average G Loss: {avg_g_loss:.4f}\")\n",
        "    scheduler_G.step()\n",
        "\n",
        "    m = 0\n",
        "    # Validation and Checkpoints\n",
        "    if (epoch + 1) % save_interval == 0:\n",
        "        torch.cuda.empty_cache()\n",
        "        G1.eval()\n",
        "        with torch.no_grad():\n",
        "            val_g_loss = 0.0\n",
        "            val_l1_loss = 0.0\n",
        "            val_ssim_loss = 0.0\n",
        "            val_lpips_loss = 0.0\n",
        "            val_psnr_mask = 0.0\n",
        "\n",
        "            with tqdm(total=len(val_loader), desc=f'Validation Epoch {epoch + 1}/{NUM_EPOCHS}', unit='batch') as pbar_val:\n",
        "                for i, (real_shadow, real_free) in enumerate(val_loader):\n",
        "                    real_shadow = real_shadow.to(DEVICE)\n",
        "                    real_free = real_free.to(DEVICE)\n",
        "\n",
        "                    pred_free = G1(real_shadow)\n",
        "                    # Generator loss\n",
        "                    g1_loss = l1_loss(pred_free, real_free)\n",
        "                    g_loss = g1_loss\n",
        "\n",
        "                    val_g_loss += g_loss.item()\n",
        "                    val_l1_loss += g1_loss.item()\n",
        "\n",
        "                    # Calculate SSIM and LPIPS\n",
        "                    pred_free_np = pred_free.cpu().numpy().transpose(0, 2, 3, 1)\n",
        "                    real_free_np = real_free.cpu().numpy().transpose(0, 2, 3, 1)\n",
        "\n",
        "                    #ssim_val = ssim(pred_free_np, real_free_np, data_range=1, channel_axis=-1)\n",
        "                    lpips_val = lpips_loss_fn(pred_free, real_free).mean().item()  # Average LPIPS over batch\n",
        "                    #val_ssim_loss += ssim_val\n",
        "                    val_lpips_loss += lpips_val\n",
        "\n",
        "                    # Calculate PSNR for final shadow-free images\n",
        "                    for j in range(real_free_np.shape[0]):\n",
        "                        val_psnr_mask += psnr(real_free_np[j], pred_free_np[j], data_range=1)\n",
        "                        val_ssim_loss += ssim(real_free_np[j], pred_free_np[j], data_range=1, channel_axis=-1)\n",
        "\n",
        "\n",
        "                    pbar_val.update(1)\n",
        "                    save_image(pred_free, os.path.join(sample_dir, f'pred_free_epoch_{epoch + 1}_{m}.png'))\n",
        "                    m += 1\n",
        "\n",
        "            # Averaging over the entire validation set\n",
        "            val_g_loss /= len(val_loader)\n",
        "            val_l1_loss /= len(val_loader)\n",
        "            val_ssim_loss /= len(val_loader.dataset)\n",
        "            val_lpips_loss /= len(val_loader)\n",
        "            val_psnr_mask /= len(val_loader.dataset)\n",
        "\n",
        "            print(f\"Validation after Epoch [{epoch + 1}/{NUM_EPOCHS}] \"\n",
        "                  f\"Val G Loss: {val_g_loss:.4f}, Val L1 Loss: {val_l1_loss:.4f}, \"\n",
        "                  f\"PSNR_MASK: {val_psnr_mask:.4f}, SSIM: {val_ssim_loss:.4f}, LPIPS: {val_lpips_loss:.4f}\")\n",
        "\n",
        "            # Save model checkpoints\n",
        "            torch.save(G1.state_dict(), os.path.join(checkpoint_dir, f'SBSR{epoch + 1}.pth'))\n",
        "\n",
        "print(\"Training completed\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJe5BUwks1j4"
      },
      "source": [
        "## RAMiT eval"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3aA-siss1j5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "from tqdm import tqdm\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from lpips import LPIPS\n",
        "import torchvision.models as models\n",
        "e = 5\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "G1 = RAMiT_FFAT(target_mode='light_realdn',img_norm=False, mv_ver = 3)\n",
        "while e<=500:\n",
        "    from collections import OrderedDict\n",
        "\n",
        "    # Load checkpoint and create new state_dict with 'module.' prefix if needed\n",
        "    checkpoint = torch.load(f'C:/Users/Admin/Desktop/AI_RAMiT/FFAT_RAMiT/SBSR{e}.pth', map_location='cuda')\n",
        "    state_dict = checkpoint['state_dict'] if 'state_dict' in checkpoint else checkpoint\n",
        "    new_state_dict = OrderedDict()\n",
        "\n",
        "    # Add prefix 'module.' if needed\n",
        "    for k, v in state_dict.items():\n",
        "        name = k if k.startswith('module.') else 'module.' + k\n",
        "        new_state_dict[name] = v\n",
        "\n",
        "\n",
        "    # Apply DataParallel and move to GPU\n",
        "    G1 = G1.to('cuda')\n",
        "    G1 = torch.nn.DataParallel(G1, device_ids=[0, 1])\n",
        "\n",
        "    # G1.to('cuda')\n",
        "    epoch = 1\n",
        "    NUM_EPOCHS = 1\n",
        "# optimizer_G = optim.Adam(G1.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999))\n",
        "\n",
        "# # # Add the cosine annealing scheduler\n",
        "# scheduler_G = optim.lr_scheduler.CosineAnnealingLR(optimizer_G, T_max=NUM_EPOCHS - START_EPOCH, eta_min=0.000001)\n",
        "\n",
        "# Loss functions\n",
        "    l1_loss = nn.L1Loss()\n",
        "    mse_loss = nn.MSELoss()\n",
        "    lpips_loss_fn = LPIPS(net='alex').to(DEVICE)  # Using AlexNet for LPIPS loss\n",
        "    with torch.no_grad():\n",
        "                val_g_loss = 0.0\n",
        "                val_l1_loss = 0.0\n",
        "                val_ssim_loss = 0.0\n",
        "                val_ssim_crop4_loss = 0.0\n",
        "                val_lpips_loss = 0.0\n",
        "                val_psnr_mask = 0.0\n",
        "                val_psnr_y = 0.0\n",
        "\n",
        "                with tqdm(total=len(val_loader), desc=f'Validation Epoch {epoch + 1}/{NUM_EPOCHS}', unit='batch') as pbar_val:\n",
        "                    for i, (real_shadow, real_free) in enumerate(val_loader):\n",
        "                        real_shadow = real_shadow.to(DEVICE)\n",
        "                        real_free = real_free.to(DEVICE)\n",
        "\n",
        "                        pred_free = G1(real_shadow)\n",
        "                        # Generator loss\n",
        "                        g1_loss = l1_loss(pred_free, real_free)\n",
        "                        # g_loss = combined_loss(pred_free, real_free)\n",
        "\n",
        "                        # val_g_loss += g_loss.item()\n",
        "                        val_l1_loss += g1_loss.item()\n",
        "\n",
        "                        # Calculate SSIM, LPIPS, PSNR, PSNR Y, and SSIM crop 4\n",
        "                        pred_free_np = pred_free.cpu().numpy().transpose(0, 2, 3, 1)\n",
        "                        real_free_np = real_free.cpu().numpy().transpose(0, 2, 3, 1)\n",
        "\n",
        "                        lpips_val = lpips_loss_fn(pred_free, real_free).mean().item()  # Average LPIPS over batch\n",
        "                        val_lpips_loss += lpips_val\n",
        "\n",
        "                        for j in range(real_free_np.shape[0]):\n",
        "                            val_psnr_mask += psnr(real_free_np[j], pred_free_np[j], data_range=1)\n",
        "                            val_ssim_loss += ssim(real_free_np[j], pred_free_np[j], data_range=1, channel_axis=-1)\n",
        "                            # val_psnr_y += psnr_y(real_free_np[j], pred_free_np[j])\n",
        "                            # val_ssim_crop4_loss += ssim_crop4(real_free_np[j], pred_free_np[j])\n",
        "\n",
        "                        pbar_val.update(1)\n",
        "\n",
        "                        # m += 1\n",
        "\n",
        "                # Averaging over the entire validation set\n",
        "                # val_g_loss /= len(val_loader)\n",
        "                val_l1_loss /= len(val_loader)\n",
        "                val_ssim_loss /= len(val_loader.dataset)\n",
        "                # val_ssim_crop4_loss /= len(val_loader.dataset)\n",
        "                val_lpips_loss /= len(val_loader)\n",
        "                val_psnr_mask /= len(val_loader.dataset)\n",
        "                # val_psnr_y /= len(val_loader.dataset)\n",
        "\n",
        "                print(f\"Validation after Epoch [{epoch + 1}/{NUM_EPOCHS}] \"\n",
        "                    f\"Val G Loss: 0, Val L1 Loss: {val_l1_loss:.4f}, \"\n",
        "                    f\"PSNR_MASK: {val_psnr_mask:.4f}, PSNR_Y: {val_psnr_y:.4f}, \"\n",
        "                    f\"SSIM: {val_ssim_loss:.4f}, SSIM_crop4: {val_ssim_crop4_loss:.4f}, \"\n",
        "                    f\"LPIPS: {val_lpips_loss:.4f},\"\n",
        "                    f\"epoch{e}\")\n",
        "                e = e+5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-7kIJndts1j6"
      },
      "outputs": [],
      "source": [
        "# Validation code\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.utils import save_image\n",
        "from tqdm import tqdm\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from lpips import LPIPS\n",
        "import torchvision.models as models\n",
        "\n",
        "# Constants\n",
        "LEARNING_RATE = 0.001\n",
        "NUM_EPOCHS = 500\n",
        "SAVE_INTERVAL = 5\n",
        "START_EPOCH = 0  # Start from epoch 80\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "#G1.load_state_dict(torch.load(r\"C:\\Users\\nasef\\Desktop\\SBSR\\Models\\SBSR32\\SBSR30.pth\"))\n",
        "\n",
        "\n",
        "optimizer_G = optim.Adam(G1.parameters(), lr=LEARNING_RATE, betas=(0.9, 0.999))\n",
        "\n",
        "# Add the cosine annealing scheduler\n",
        "scheduler_G = optim.lr_scheduler.CosineAnnealingLR(optimizer_G, T_max=NUM_EPOCHS - START_EPOCH, eta_min=0.000001)\n",
        "\n",
        "# Loss functions\n",
        "l1_loss = nn.L1Loss()\n",
        "mse_loss = nn.MSELoss()\n",
        "lpips_loss_fn = LPIPS(net='alex').to(DEVICE)  # Using AlexNet for LPIPS loss\n",
        "\n",
        "# Define the VGG loss\n",
        "class VGGLoss(nn.Module):\n",
        "    def __init__(self, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
        "        super(VGGLoss, self).__init__()\n",
        "        vgg = models.vgg19(weights=models.VGG19_Weights.IMAGENET1K_V1).features.to(device).eval()  # Move VGG layers to device and set to eval mode\n",
        "        self.vgg_layers = nn.Sequential(*list(vgg)[:36]).to(device)\n",
        "        self.device = device\n",
        "        for param in self.vgg_layers.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = x.to(self.device)\n",
        "        y = y.to(self.device)\n",
        "        x_vgg = self.vgg_layers(x)\n",
        "        y_vgg = self.vgg_layers(y)\n",
        "        loss = nn.functional.l1_loss(x_vgg, y_vgg)\n",
        "        return loss\n",
        "\n",
        "# Define Focal Frequency Loss\n",
        "class FocalFrequencyLoss(nn.Module):\n",
        "    def __init__(self, alpha=1.0, device=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")):\n",
        "        super(FocalFrequencyLoss, self).__init__()\n",
        "        self.alpha = alpha\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        input = input.to(self.device)\n",
        "        target = target.to(self.device)\n",
        "        input_fft = torch.fft.fft2(input)\n",
        "        target_fft = torch.fft.fft2(target)\n",
        "        diff = input_fft - target_fft\n",
        "        abs_diff = torch.abs(diff)\n",
        "        loss = torch.pow(abs_diff, self.alpha)\n",
        "        return torch.mean(loss)\n",
        "\n",
        "# Loss weights\n",
        "lambda_vgg = 0.01\n",
        "lambda_ff = 0.1\n",
        "\n",
        "# Combined loss\n",
        "class CombinedLoss(nn.Module):\n",
        "    def __init__(self, lambda_vgg, lambda_ff, device=DEVICE):\n",
        "        super(CombinedLoss, self).__init__()\n",
        "        self.lambda_vgg = lambda_vgg\n",
        "        self.lambda_ff = lambda_ff\n",
        "        self.vgg_loss = VGGLoss(device=device)\n",
        "        self.ff_loss = FocalFrequencyLoss(device=device)\n",
        "\n",
        "    def forward(self, input, target):\n",
        "        l1 = l1_loss(input, target)\n",
        "        vgg = self.vgg_loss(input, target)\n",
        "        ff = self.ff_loss(input, target)\n",
        "        return l1 + self.lambda_vgg * vgg + self.lambda_ff * ff\n",
        "\n",
        "# Example usage\n",
        "combined_loss = CombinedLoss(lambda_vgg=lambda_vgg, lambda_ff=lambda_ff, device=DEVICE)\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "lpips_loss_fn = LPIPS(net='alex').to(DEVICE)  # Using AlexNet for LPIPS loss\n",
        "G1 = RAMiT_FFAT(target_mode='light_realdn',img_norm=False, mv_ver = 3)\n",
        "# Validation and Checkpoints\n",
        "G1.eval()\n",
        "e = 5\n",
        "while e <= 500:\n",
        "    G1 = RAMiT_FFAT(target_mode='light_realdn',img_norm=False, mv_ver = 3)\n",
        "    print(e)\n",
        "    from collections import OrderedDict\n",
        "    G1 = G1.to('cuda')\n",
        "    # Load checkpoint and create new state_dict with 'module.' prefix if needed\n",
        "    checkpoint = torch.load(f'C:/Users/Admin/Desktop/AI_RAMiT/FFAT_RAMiT/SBSR{e}.pth', map_location='cuda')\n",
        "    state_dict = checkpoint['state_dict'] if 'state_dict' in checkpoint else checkpoint\n",
        "    new_state_dict = OrderedDict()\n",
        "\n",
        "    # Add prefix 'module.' if needed\n",
        "    new_state_dict = {k.replace('module.', ''): v for k, v in state_dict.items()}\n",
        "\n",
        "    # Apply DataParallel and move to GPU\n",
        "    G1.load_state_dict(new_state_dict)\n",
        "    G1 = torch.nn.DataParallel(G1)\n",
        "    with torch.no_grad():\n",
        "        val_g_loss = 0.0\n",
        "        val_l1_loss = 0.0\n",
        "        val_ssim_loss = 0.0\n",
        "        val_lpips_loss = 0.0\n",
        "        val_psnr_mask = 0.0\n",
        "        m = 0\n",
        "\n",
        "        with tqdm(total=len(val_loader), desc=f'Validation', unit='batch') as pbar_val:\n",
        "            for i, (real_shadow, real_free) in enumerate(val_loader):\n",
        "                real_shadow = real_shadow.to(DEVICE)\n",
        "                real_free = real_free.to(DEVICE)\n",
        "\n",
        "                pred_free = G1(real_shadow)\n",
        "                # Generator loss\n",
        "                g1_loss = l1_loss(pred_free, real_free)\n",
        "                g_loss = g1_loss\n",
        "\n",
        "                val_g_loss += g_loss.item()\n",
        "                val_l1_loss += g1_loss.item()\n",
        "\n",
        "                # Calculate SSIM and LPIPS\n",
        "                pred_free_np = pred_free.cpu().numpy().transpose(0, 2, 3, 1)\n",
        "                real_free_np = real_free.cpu().numpy().transpose(0, 2, 3, 1)\n",
        "\n",
        "                lpips_val = lpips_loss_fn(pred_free, real_free).mean().item()  # Average LPIPS over batch\n",
        "                val_lpips_loss += lpips_val\n",
        "\n",
        "                # Calculate PSNR for final shadow-free images\n",
        "                for j in range(real_free_np.shape[0]):\n",
        "                    val_psnr_mask += psnr(real_free_np[j], pred_free_np[j], data_range=1)\n",
        "                    val_ssim_loss += ssim(real_free_np[j], pred_free_np[j], data_range=1, channel_axis=-1)\n",
        "\n",
        "                pbar_val.update(1)\n",
        "                # save_image(pred_free, os.path.join(sample_dir, f'pred_free_epoch_{m}.png'))\n",
        "                m += 1\n",
        "\n",
        "        # Averaging over the entire validation set\n",
        "        val_g_loss /= len(val_loader)\n",
        "        val_l1_loss /= len(val_loader)\n",
        "        val_ssim_loss /= len(val_loader.dataset)\n",
        "        val_lpips_loss /= len(val_loader)\n",
        "        val_psnr_mask /= len(val_loader.dataset)\n",
        "\n",
        "        print(f\"Validation \"\n",
        "            f\"Val G Loss: {val_g_loss:.4f}, Val L1 Loss: {val_l1_loss:.4f}, \"\n",
        "            f\"PSNR_MASK: {val_psnr_mask:.4f}, SSIM: {val_ssim_loss:.4f}, LPIPS: {val_lpips_loss:.4f}\")\n",
        "        e = e+5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZfDgQc07rAV"
      },
      "outputs": [],
      "source": [
        "# from google.colab import runtime\n",
        "# runtime.unassign()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dD_orSeWrrc0"
      },
      "outputs": [],
      "source": [
        "# top_output, top_inputs, top_targets, low_output, low_inputs, low_targets, tev_out, tev_inputs, tev_targets, output_l, targets_l, inputs_l = train(model, train_loader, 10)\n",
        "# # val_loss, e_out, total_psnr = evaluate(model, val_loader)\n",
        "# # val_ppl = math.exp(val_loss)\n",
        "# elapsed = time.time()\n",
        "# print('-' * 89)\n",
        "# print(f'time: {elapsed:5.2f}s ')#| psnr{total_psnr:5.2f} |'\n",
        "#             # f'valid loss {val_loss:5.2f}')\n",
        "# print('-' * 89)\n",
        "\n",
        "# import numpy as np\n",
        "\n",
        "# # scheduler.step()\n",
        "#  # load best model states"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PMm8jS-e0hGb"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# from torchvision import transforms\n",
        "# import matplotlib.pyplot as plt\n",
        "# from PIL import Image\n",
        "\n",
        "# # Assuming 'tensor' is your input tensor with shape [2, 3, 256, 256]\n",
        "# tensor_list = top_output  # Example tensor, replace with your actual tensor\n",
        "# ind = 0\n",
        "# # Denormalize the tensor\n",
        "# for tensor in tensor_list:\n",
        "#   tensor = tensor * 255  # Scale the tensor values to [0, 255]\n",
        "#   tensor = tensor.clamp(0, 255)  # Ensure the values are within [0, 255]\n",
        "#   tensor = tensor.byte()  # Convert to byte type\n",
        "#   tensor_list[ind] = tensor\n",
        "#   ind += 1\n",
        "\n",
        "# # Convert the tensor to a PIL image and display both images\n",
        "\n",
        "# to_pil_image = transforms.ToPILImage()\n",
        "\n",
        "# for tensor in tensor_list:\n",
        "#   for i in range(tensor.shape[0]):\n",
        "#     image = to_pil_image(tensor[0])\n",
        "#     plt.imshow(image)\n",
        "#     plt.axis('off')  # Hide the axes\n",
        "#     plt.show()\n",
        "# # for i in range(tensor.shape[0]):\n",
        "# #     image = to_pil_image(tensor[i])\n",
        "# #     plt.imshow(image)\n",
        "# #     plt.axis('off')  # Hide the axes\n",
        "# #     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N3WkKuUJh_TL"
      },
      "outputs": [],
      "source": [
        "# tensor = top_inputs  # Example tensor, replace with your actual tensor\n",
        "\n",
        "# # Denormalize the tensor\n",
        "# tensor = tensor * 255  # Scale the tensor values to [0, 255]\n",
        "# tensor = tensor.clamp(0, 255)  # Ensure the values are within [0, 255]\n",
        "# tensor = tensor.byte()  # Convert to byte type\n",
        "\n",
        "# # Convert the tensor to a PIL image and display both images\n",
        "# to_pil_image = transforms.ToPILImage()\n",
        "\n",
        "# for i in range(tensor.shape[0]):\n",
        "#     image = to_pil_image(tensor[i])\n",
        "#     plt.imshow(image)\n",
        "#     plt.axis('off')  # Hide the axes\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zEaAb6w0iJnY"
      },
      "outputs": [],
      "source": [
        "# tensor = top_targets  # Example tensor, replace with your actual tensor\n",
        "\n",
        "# # Denormalize the tensor\n",
        "# tensor = tensor * 255  # Scale the tensor values to [0, 255]\n",
        "# tensor = tensor.clamp(0, 255)  # Ensure the values are within [0, 255]\n",
        "# tensor = tensor.byte()  # Convert to byte type\n",
        "\n",
        "# # Convert the tensor to a PIL image and display both images\n",
        "# to_pil_image = transforms.ToPILImage()\n",
        "\n",
        "# for i in range(tensor.shape[0]):\n",
        "#     image = to_pil_image(tensor[i])\n",
        "#     plt.imshow(image)\n",
        "#     plt.axis('off')  # Hide the axes\n",
        "#     plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1tGKPM2Qmu-U"
      },
      "outputs": [],
      "source": [
        "# # import torch\n",
        "# # from torchvision import transforms\n",
        "# # import matplotlib.pyplot as plt\n",
        "# # from PIL import Image\n",
        "\n",
        "# # Assuming 'tensor' is your input tensor with shape [1, 3, 256, 256]\n",
        "# # tensor = e_out  # Example tensor, replace with your actual tensor\n",
        "\n",
        "# # # Remove the batch dimension\n",
        "# # tensor = tensor.squeeze(0)  # Now the shape is [3, 256, 256]\n",
        "\n",
        "# # # Convert the tensor to a PIL image\n",
        "# # to_pil_image = transforms.ToPILImage()\n",
        "# # image = to_pil_image(tensor)\n",
        "\n",
        "# # # Display the image\n",
        "# # plt.imshow(image)\n",
        "# # plt.axis('off')  # Hide the axes\n",
        "# # plt.show()\n",
        "\n",
        "\n",
        "# import torch\n",
        "# from torchvision import transforms\n",
        "# import matplotlib.pyplot as plt\n",
        "# from PIL import Image\n",
        "\n",
        "# # Assuming 'tensor' is your input tensor with shape [2, 3, 256, 256]\n",
        "# tensor = e_out  # Example tensor, replace with your actual tensor\n",
        "\n",
        "# # Denormalize the tensor\n",
        "# tensor = tensor * 255  # Scale the tensor values to [0, 255]\n",
        "# tensor = tensor.clamp(0, 255)  # Ensure the values are within [0, 255]\n",
        "# tensor = tensor.byte()  # Convert to byte type\n",
        "\n",
        "# # Convert the tensor to a PIL image and display both images\n",
        "# to_pil_image = transforms.ToPILImage()\n",
        "\n",
        "# for i in range(tensor.shape[0]):\n",
        "#     image = to_pil_image(tensor[0])\n",
        "#     plt.imshow(image)\n",
        "#     plt.axis('off')  # Hide the axes\n",
        "#     plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "FKTfTBRTTz4T",
        "DYsCKqC6bg5c",
        "typm68nYiiGa",
        "yItTYYBeUIfS",
        "iyo1v8YvoqO1",
        "kJ5hGAuX9Lr3"
      ],
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "ramit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}